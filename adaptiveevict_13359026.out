Job started at Fri Apr 18 12:49:52 CDT 2025
Running on g067.grace.hprc.tamu.edu
Fri Apr 18 12:49:52 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-PCIE-40GB          On  | 00000000:3B:00.0 Off |                  Off |
| N/A   18C    P0              29W / 250W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
INFO:get_data:Using data from ./data/data.txt



Started at 2025-04-18 14:12:15
('batch_size', 64)
('data_file', 'data.txt')
('data_path', './data/')
('dropout', 0.0)
('embed_dim', 48)
('epochs', 25)
('forward_mul', 2)
('gen_tokens_len', 256)
('input_text', 'You')
('load_model', False)
('load_tokenizer', False)
('lr', 0.001)
('max_merged_tokens', 5000)
('model_path', './saved_models')
('n_heads', 4)
('n_layers', 6)
('n_workers', 2)
('network_type', 'llama')
('temperature', 1.0)
('test_only', False)
('top_k', 10)
('top_p', 0.6)
('train_tokens_len', 64)
('warmup_epochs', 5)

16915548 tokens created from the file. Each epoch will have 264305 batches.
Number of trainable parameters in the model: 1015032
Number of tokens per parameters: 16.6650.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 1/25	It: 1/4130	batch_loss: 8.9178	batch_accuracy: 0.00%	lr:0.000010
Ep: 1/25	It: 51/4130	batch_loss: 8.9097	batch_accuracy: 0.00%	lr:0.000012
Ep: 1/25	It: 101/4130	batch_loss: 8.8563	batch_accuracy: 0.02%	lr:0.000015
Ep: 1/25	It: 151/4130	batch_loss: 8.8553	batch_accuracy: 0.02%	lr:0.000017
Ep: 1/25	It: 201/4130	batch_loss: 8.8201	batch_accuracy: 0.07%	lr:0.000020
Ep: 1/25	It: 251/4130	batch_loss: 8.7745	batch_accuracy: 0.00%	lr:0.000022
Ep: 1/25	It: 301/4130	batch_loss: 8.7217	batch_accuracy: 0.05%	lr:0.000024
Ep: 1/25	It: 351/4130	batch_loss: 8.6248	batch_accuracy: 0.22%	lr:0.000027
Ep: 1/25	It: 401/4130	batch_loss: 8.5434	batch_accuracy: 0.61%	lr:0.000029
Ep: 1/25	It: 451/4130	batch_loss: 8.4624	batch_accuracy: 1.71%	lr:0.000032
Ep: 1/25	It: 501/4130	batch_loss: 8.3748	batch_accuracy: 1.93%	lr:0.000034
Ep: 1/25	It: 551/4130	batch_loss: 8.2598	batch_accuracy: 2.66%	lr:0.000036
Ep: 1/25	It: 601/4130	batch_loss: 8.1945	batch_accuracy: 3.00%	lr:0.000039
Ep: 1/25	It: 651/4130	batch_loss: 8.0984	batch_accuracy: 3.42%	lr:0.000041
Ep: 1/25	It: 701/4130	batch_loss: 8.0031	batch_accuracy: 4.37%	lr:0.000044
Ep: 1/25	It: 751/4130	batch_loss: 7.9424	batch_accuracy: 4.13%	lr:0.000046
Ep: 1/25	It: 801/4130	batch_loss: 7.8208	batch_accuracy: 4.81%	lr:0.000048
Ep: 1/25	It: 851/4130	batch_loss: 7.7514	batch_accuracy: 4.71%	lr:0.000051
Ep: 1/25	It: 901/4130	batch_loss: 7.6548	batch_accuracy: 5.44%	lr:0.000053
Ep: 1/25	It: 951/4130	batch_loss: 7.6289	batch_accuracy: 3.96%	lr:0.000056
Ep: 1/25	It: 1001/4130	batch_loss: 7.5103	batch_accuracy: 5.96%	lr:0.000058
Ep: 1/25	It: 1051/4130	batch_loss: 7.4536	batch_accuracy: 6.01%	lr:0.000060
Ep: 1/25	It: 1101/4130	batch_loss: 7.3629	batch_accuracy: 5.83%	lr:0.000063
Ep: 1/25	It: 1151/4130	batch_loss: 7.3550	batch_accuracy: 4.54%	lr:0.000065
Ep: 1/25	It: 1201/4130	batch_loss: 7.3294	batch_accuracy: 5.15%	lr:0.000068
Ep: 1/25	It: 1251/4130	batch_loss: 7.2844	batch_accuracy: 5.49%	lr:0.000070
Ep: 1/25	It: 1301/4130	batch_loss: 7.1897	batch_accuracy: 6.91%	lr:0.000072
Ep: 1/25	It: 1351/4130	batch_loss: 7.1571	batch_accuracy: 6.79%	lr:0.000075
Ep: 1/25	It: 1401/4130	batch_loss: 7.1845	batch_accuracy: 5.96%	lr:0.000077
Ep: 1/25	It: 1451/4130	batch_loss: 7.0982	batch_accuracy: 7.03%	lr:0.000080
Ep: 1/25	It: 1501/4130	batch_loss: 7.1250	batch_accuracy: 6.30%	lr:0.000082
Ep: 1/25	It: 1551/4130	batch_loss: 7.0622	batch_accuracy: 7.01%	lr:0.000084
Ep: 1/25	It: 1601/4130	batch_loss: 7.0016	batch_accuracy: 8.06%	lr:0.000087
Ep: 1/25	It: 1651/4130	batch_loss: 6.9578	batch_accuracy: 8.11%	lr:0.000089
Ep: 1/25	It: 1701/4130	batch_loss: 6.9813	batch_accuracy: 7.08%	lr:0.000092
Ep: 1/25	It: 1751/4130	batch_loss: 7.0325	batch_accuracy: 6.88%	lr:0.000094
Ep: 1/25	It: 1801/4130	batch_loss: 7.0198	batch_accuracy: 6.81%	lr:0.000096
Ep: 1/25	It: 1851/4130	batch_loss: 7.0684	batch_accuracy: 6.81%	lr:0.000099
Ep: 1/25	It: 1901/4130	batch_loss: 6.9070	batch_accuracy: 8.11%	lr:0.000101
Ep: 1/25	It: 1951/4130	batch_loss: 6.8382	batch_accuracy: 9.23%	lr:0.000104
Ep: 1/25	It: 2001/4130	batch_loss: 6.9194	batch_accuracy: 7.64%	lr:0.000106
Ep: 1/25	It: 2051/4130	batch_loss: 6.8487	batch_accuracy: 8.64%	lr:0.000108
Ep: 1/25	It: 2101/4130	batch_loss: 6.8863	batch_accuracy: 8.74%	lr:0.000111
Ep: 1/25	It: 2151/4130	batch_loss: 6.8348	batch_accuracy: 8.47%	lr:0.000113
Ep: 1/25	It: 2201/4130	batch_loss: 6.8731	batch_accuracy: 6.96%	lr:0.000116
Ep: 1/25	It: 2251/4130	batch_loss: 6.7743	batch_accuracy: 8.76%	lr:0.000118
Ep: 1/25	It: 2301/4130	batch_loss: 6.6219	batch_accuracy: 10.77%	lr:0.000120
Ep: 1/25	It: 2351/4130	batch_loss: 6.6705	batch_accuracy: 9.69%	lr:0.000123
Ep: 1/25	It: 2401/4130	batch_loss: 6.6772	batch_accuracy: 9.25%	lr:0.000125
Ep: 1/25	It: 2451/4130	batch_loss: 6.6119	batch_accuracy: 9.62%	lr:0.000128
Ep: 1/25	It: 2501/4130	batch_loss: 6.6060	batch_accuracy: 9.55%	lr:0.000130
Ep: 1/25	It: 2551/4130	batch_loss: 6.4384	batch_accuracy: 11.60%	lr:0.000132
Ep: 1/25	It: 2601/4130	batch_loss: 6.5631	batch_accuracy: 10.18%	lr:0.000135
Ep: 1/25	It: 2651/4130	batch_loss: 6.5303	batch_accuracy: 10.42%	lr:0.000137
Ep: 1/25	It: 2701/4130	batch_loss: 6.3588	batch_accuracy: 12.16%	lr:0.000139
Ep: 1/25	It: 2751/4130	batch_loss: 6.4537	batch_accuracy: 10.11%	lr:0.000142
Ep: 1/25	It: 2801/4130	batch_loss: 6.4060	batch_accuracy: 11.74%	lr:0.000144
Ep: 1/25	It: 2851/4130	batch_loss: 6.2800	batch_accuracy: 12.43%	lr:0.000147
Ep: 1/25	It: 2901/4130	batch_loss: 6.3075	batch_accuracy: 11.72%	lr:0.000149
Ep: 1/25	It: 2951/4130	batch_loss: 6.4148	batch_accuracy: 10.89%	lr:0.000151
Ep: 1/25	It: 3001/4130	batch_loss: 6.4352	batch_accuracy: 9.77%	lr:0.000154
Ep: 1/25	It: 3051/4130	batch_loss: 6.2692	batch_accuracy: 11.69%	lr:0.000156
Ep: 1/25	It: 3101/4130	batch_loss: 6.2933	batch_accuracy: 10.99%	lr:0.000159
Ep: 1/25	It: 3151/4130	batch_loss: 6.1941	batch_accuracy: 12.38%	lr:0.000161
Ep: 1/25	It: 3201/4130	batch_loss: 6.1357	batch_accuracy: 13.04%	lr:0.000163
Ep: 1/25	It: 3251/4130	batch_loss: 6.2206	batch_accuracy: 12.48%	lr:0.000166
Ep: 1/25	It: 3301/4130	batch_loss: 6.1045	batch_accuracy: 12.40%	lr:0.000168
Ep: 1/25	It: 3351/4130	batch_loss: 6.1829	batch_accuracy: 11.91%	lr:0.000171
Ep: 1/25	It: 3401/4130	batch_loss: 6.1736	batch_accuracy: 13.01%	lr:0.000173
Ep: 1/25	It: 3451/4130	batch_loss: 6.1800	batch_accuracy: 10.79%	lr:0.000175
Ep: 1/25	It: 3501/4130	batch_loss: 6.1433	batch_accuracy: 11.38%	lr:0.000178
Ep: 1/25	It: 3551/4130	batch_loss: 6.0474	batch_accuracy: 13.13%	lr:0.000180
Ep: 1/25	It: 3601/4130	batch_loss: 6.1333	batch_accuracy: 11.91%	lr:0.000183
Ep: 1/25	It: 3651/4130	batch_loss: 5.9149	batch_accuracy: 13.67%	lr:0.000185
Ep: 1/25	It: 3701/4130	batch_loss: 6.1128	batch_accuracy: 12.26%	lr:0.000187
Ep: 1/25	It: 3751/4130	batch_loss: 6.0085	batch_accuracy: 12.16%	lr:0.000190
Ep: 1/25	It: 3801/4130	batch_loss: 6.0100	batch_accuracy: 13.45%	lr:0.000192
Ep: 1/25	It: 3851/4130	batch_loss: 5.9707	batch_accuracy: 13.01%	lr:0.000195
Ep: 1/25	It: 3901/4130	batch_loss: 5.9105	batch_accuracy: 13.33%	lr:0.000197
Ep: 1/25	It: 3951/4130	batch_loss: 5.9190	batch_accuracy: 13.72%	lr:0.000199
Ep: 1/25	It: 4001/4130	batch_loss: 5.9930	batch_accuracy: 13.28%	lr:0.000202
Ep: 1/25	It: 4051/4130	batch_loss: 6.0827	batch_accuracy: 11.67%	lr:0.000204
Ep: 1/25	It: 4101/4130	batch_loss: 5.8901	batch_accuracy: 14.38%	lr:0.000207
Ep: 1/25	It: 4130/4130	batch_loss: 5.9852	batch_accuracy: 12.82%	lr:0.000208


Generated text for input text "You" is:
Yous. The results show that the results have been not the results of the new of the presence of the first of the number of the first-coles (F) is also be not be used by a significant, and a study were significantly more important.
<eot>
<sot>
ADs of the EPBE

The study is an important of the AHF-cant and the CSR-ciling.

In this paper, we present a new study of the most a the MDA

The study of the PHii, which is not only that a study is a novel of the study of the CPPH2, a T-S-pier (F) was observed in the SPH) and the PMA, and the MAA and the LMR. In this research, the MTR.
<eot>
<sot>
B-Ds (LF) is a the SMP. The results show that of a CS

A-pan, and BD. AH2 and SFAO) and SNA

The paper presents a significant


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 2/25	It: 1/4130	batch_loss: 5.8531	batch_accuracy: 14.87%	lr:0.000208
Ep: 2/25	It: 51/4130	batch_loss: 5.9387	batch_accuracy: 13.77%	lr:0.000210
Ep: 2/25	It: 101/4130	batch_loss: 5.9634	batch_accuracy: 13.09%	lr:0.000213
Ep: 2/25	It: 151/4130	batch_loss: 5.9406	batch_accuracy: 11.94%	lr:0.000215
Ep: 2/25	It: 201/4130	batch_loss: 5.7802	batch_accuracy: 14.50%	lr:0.000218
Ep: 2/25	It: 251/4130	batch_loss: 5.8273	batch_accuracy: 14.72%	lr:0.000220
Ep: 2/25	It: 301/4130	batch_loss: 5.8206	batch_accuracy: 14.36%	lr:0.000222
Ep: 2/25	It: 351/4130	batch_loss: 5.7338	batch_accuracy: 14.09%	lr:0.000225
Ep: 2/25	It: 401/4130	batch_loss: 5.7766	batch_accuracy: 13.06%	lr:0.000227
Ep: 2/25	It: 451/4130	batch_loss: 5.7642	batch_accuracy: 14.06%	lr:0.000230
Ep: 2/25	It: 501/4130	batch_loss: 5.6933	batch_accuracy: 14.65%	lr:0.000232
Ep: 2/25	It: 551/4130	batch_loss: 5.7878	batch_accuracy: 13.57%	lr:0.000234
Ep: 2/25	It: 601/4130	batch_loss: 5.6914	batch_accuracy: 15.06%	lr:0.000237
Ep: 2/25	It: 651/4130	batch_loss: 5.7217	batch_accuracy: 14.36%	lr:0.000239
Ep: 2/25	It: 701/4130	batch_loss: 5.5940	batch_accuracy: 16.58%	lr:0.000242
Ep: 2/25	It: 751/4130	batch_loss: 5.8227	batch_accuracy: 13.67%	lr:0.000244
Ep: 2/25	It: 801/4130	batch_loss: 5.6909	batch_accuracy: 15.19%	lr:0.000246
Ep: 2/25	It: 851/4130	batch_loss: 5.7102	batch_accuracy: 13.84%	lr:0.000249
Ep: 2/25	It: 901/4130	batch_loss: 5.6886	batch_accuracy: 13.65%	lr:0.000251
Ep: 2/25	It: 951/4130	batch_loss: 5.6750	batch_accuracy: 14.40%	lr:0.000254
Ep: 2/25	It: 1001/4130	batch_loss: 5.5729	batch_accuracy: 15.48%	lr:0.000256
Ep: 2/25	It: 1051/4130	batch_loss: 5.5177	batch_accuracy: 15.89%	lr:0.000258
Ep: 2/25	It: 1101/4130	batch_loss: 5.6183	batch_accuracy: 14.79%	lr:0.000261
Ep: 2/25	It: 1151/4130	batch_loss: 5.7989	batch_accuracy: 14.04%	lr:0.000263
Ep: 2/25	It: 1201/4130	batch_loss: 5.5762	batch_accuracy: 15.62%	lr:0.000266
Ep: 2/25	It: 1251/4130	batch_loss: 5.5080	batch_accuracy: 16.06%	lr:0.000268
Ep: 2/25	It: 1301/4130	batch_loss: 5.5504	batch_accuracy: 15.80%	lr:0.000270
Ep: 2/25	It: 1351/4130	batch_loss: 5.6321	batch_accuracy: 14.97%	lr:0.000273
Ep: 2/25	It: 1401/4130	batch_loss: 5.4944	batch_accuracy: 16.16%	lr:0.000275
Ep: 2/25	It: 1451/4130	batch_loss: 5.4798	batch_accuracy: 16.09%	lr:0.000278
Ep: 2/25	It: 1501/4130	batch_loss: 5.4184	batch_accuracy: 16.70%	lr:0.000280
Ep: 2/25	It: 1551/4130	batch_loss: 5.5975	batch_accuracy: 14.72%	lr:0.000282
Ep: 2/25	It: 1601/4130	batch_loss: 5.6214	batch_accuracy: 14.04%	lr:0.000285
Ep: 2/25	It: 1651/4130	batch_loss: 5.4748	batch_accuracy: 16.63%	lr:0.000287
Ep: 2/25	It: 1701/4130	batch_loss: 5.4642	batch_accuracy: 17.14%	lr:0.000290
Ep: 2/25	It: 1751/4130	batch_loss: 5.5317	batch_accuracy: 15.06%	lr:0.000292
Ep: 2/25	It: 1801/4130	batch_loss: 5.3499	batch_accuracy: 17.11%	lr:0.000294
Ep: 2/25	It: 1851/4130	batch_loss: 5.3657	batch_accuracy: 16.63%	lr:0.000297
Ep: 2/25	It: 1901/4130	batch_loss: 5.4271	batch_accuracy: 16.28%	lr:0.000299
Ep: 2/25	It: 1951/4130	batch_loss: 5.5411	batch_accuracy: 14.36%	lr:0.000302
Ep: 2/25	It: 2001/4130	batch_loss: 5.4476	batch_accuracy: 16.38%	lr:0.000304
Ep: 2/25	It: 2051/4130	batch_loss: 5.3967	batch_accuracy: 17.41%	lr:0.000306
Ep: 2/25	It: 2101/4130	batch_loss: 5.3802	batch_accuracy: 16.92%	lr:0.000309
Ep: 2/25	It: 2151/4130	batch_loss: 5.4491	batch_accuracy: 16.55%	lr:0.000311
Ep: 2/25	It: 2201/4130	batch_loss: 5.3136	batch_accuracy: 17.94%	lr:0.000314
Ep: 2/25	It: 2251/4130	batch_loss: 5.3913	batch_accuracy: 16.28%	lr:0.000316
Ep: 2/25	It: 2301/4130	batch_loss: 5.4088	batch_accuracy: 16.94%	lr:0.000318
Ep: 2/25	It: 2351/4130	batch_loss: 5.3454	batch_accuracy: 16.94%	lr:0.000321
Ep: 2/25	It: 2401/4130	batch_loss: 5.3829	batch_accuracy: 15.67%	lr:0.000323
Ep: 2/25	It: 2451/4130	batch_loss: 5.3350	batch_accuracy: 17.65%	lr:0.000326
Ep: 2/25	It: 2501/4130	batch_loss: 5.3917	batch_accuracy: 16.48%	lr:0.000328
Ep: 2/25	It: 2551/4130	batch_loss: 5.3960	batch_accuracy: 16.82%	lr:0.000330
Ep: 2/25	It: 2601/4130	batch_loss: 5.3562	batch_accuracy: 16.67%	lr:0.000333
Ep: 2/25	It: 2651/4130	batch_loss: 5.3860	batch_accuracy: 17.24%	lr:0.000335
Ep: 2/25	It: 2701/4130	batch_loss: 5.3439	batch_accuracy: 18.09%	lr:0.000338
Ep: 2/25	It: 2751/4130	batch_loss: 5.2571	batch_accuracy: 17.04%	lr:0.000340
Ep: 2/25	It: 2801/4130	batch_loss: 5.3690	batch_accuracy: 17.11%	lr:0.000342
Ep: 2/25	It: 2851/4130	batch_loss: 5.3199	batch_accuracy: 17.09%	lr:0.000345
Ep: 2/25	It: 2901/4130	batch_loss: 5.2617	batch_accuracy: 16.85%	lr:0.000347
Ep: 2/25	It: 2951/4130	batch_loss: 5.2198	batch_accuracy: 18.36%	lr:0.000349
Ep: 2/25	It: 3001/4130	batch_loss: 5.3093	batch_accuracy: 17.21%	lr:0.000352
Ep: 2/25	It: 3051/4130	batch_loss: 5.2102	batch_accuracy: 17.29%	lr:0.000354
Ep: 2/25	It: 3101/4130	batch_loss: 5.3535	batch_accuracy: 16.26%	lr:0.000357
Ep: 2/25	It: 3151/4130	batch_loss: 5.3365	batch_accuracy: 16.46%	lr:0.000359
Ep: 2/25	It: 3201/4130	batch_loss: 5.3354	batch_accuracy: 17.29%	lr:0.000361
Ep: 2/25	It: 3251/4130	batch_loss: 5.1814	batch_accuracy: 18.07%	lr:0.000364
Ep: 2/25	It: 3301/4130	batch_loss: 5.2264	batch_accuracy: 17.04%	lr:0.000366
Ep: 2/25	It: 3351/4130	batch_loss: 5.2546	batch_accuracy: 17.82%	lr:0.000369
Ep: 2/25	It: 3401/4130	batch_loss: 5.3458	batch_accuracy: 16.94%	lr:0.000371
Ep: 2/25	It: 3451/4130	batch_loss: 5.2036	batch_accuracy: 17.43%	lr:0.000373
Ep: 2/25	It: 3501/4130	batch_loss: 5.2539	batch_accuracy: 17.82%	lr:0.000376
Ep: 2/25	It: 3551/4130	batch_loss: 5.2840	batch_accuracy: 16.65%	lr:0.000378
Ep: 2/25	It: 3601/4130	batch_loss: 5.2317	batch_accuracy: 18.33%	lr:0.000381
Ep: 2/25	It: 3651/4130	batch_loss: 5.2768	batch_accuracy: 16.92%	lr:0.000383
Ep: 2/25	It: 3701/4130	batch_loss: 5.1886	batch_accuracy: 17.04%	lr:0.000385
Ep: 2/25	It: 3751/4130	batch_loss: 5.1602	batch_accuracy: 18.41%	lr:0.000388
Ep: 2/25	It: 3801/4130	batch_loss: 5.1619	batch_accuracy: 18.58%	lr:0.000390
Ep: 2/25	It: 3851/4130	batch_loss: 5.2500	batch_accuracy: 17.07%	lr:0.000393
Ep: 2/25	It: 3901/4130	batch_loss: 5.0864	batch_accuracy: 18.95%	lr:0.000395
Ep: 2/25	It: 3951/4130	batch_loss: 5.2250	batch_accuracy: 17.82%	lr:0.000397
Ep: 2/25	It: 4001/4130	batch_loss: 5.2168	batch_accuracy: 17.87%	lr:0.000400
Ep: 2/25	It: 4051/4130	batch_loss: 5.3121	batch_accuracy: 16.36%	lr:0.000402
Ep: 2/25	It: 4101/4130	batch_loss: 5.1898	batch_accuracy: 17.43%	lr:0.000405
Ep: 2/25	It: 4130/4130	batch_loss: 5.1749	batch_accuracy: 17.57%	lr:0.000406


Generated text for input text "You" is:
You and the development of the cyster and a new data of the presence of the PP in the CEs. We report the most common, the presence of a high-drug, the presence of the N-2+-carbon (TE) in a high‐based, and the first time of the POI (D) and the first of the first-captic cell line in terms of the MB-1, and the PLF was also observed for the BFIs. The aim of the two-solding, there is not differ to the development of the MTM and the PBD is the proposed. The paper presents the most important role of the same information for all the performance of the proposeds of the first and its relationship.
<eot>
<sot>
Po-Ds: Authom

This paper we investigate the impact of the proposed method for the performance of the SRC and the Earth and Single-1O3. The results demonstrate that the C-2-T-Si-Turker-based system is the proposed in this problem. The authors provide an overview of the two cases, which


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 3/25	It: 1/4130	batch_loss: 5.1630	batch_accuracy: 17.92%	lr:0.000406
Ep: 3/25	It: 51/4130	batch_loss: 5.1350	batch_accuracy: 18.75%	lr:0.000408
Ep: 3/25	It: 101/4130	batch_loss: 5.1258	batch_accuracy: 18.58%	lr:0.000411
Ep: 3/25	It: 151/4130	batch_loss: 5.1986	batch_accuracy: 17.87%	lr:0.000413
Ep: 3/25	It: 201/4130	batch_loss: 5.2242	batch_accuracy: 17.68%	lr:0.000416
Ep: 3/25	It: 251/4130	batch_loss: 5.1374	batch_accuracy: 17.82%	lr:0.000418
Ep: 3/25	It: 301/4130	batch_loss: 5.2598	batch_accuracy: 16.70%	lr:0.000420
Ep: 3/25	It: 351/4130	batch_loss: 5.1907	batch_accuracy: 18.02%	lr:0.000423
Ep: 3/25	It: 401/4130	batch_loss: 5.2541	batch_accuracy: 17.11%	lr:0.000425
Ep: 3/25	It: 451/4130	batch_loss: 5.0353	batch_accuracy: 18.63%	lr:0.000428
Ep: 3/25	It: 501/4130	batch_loss: 5.1904	batch_accuracy: 17.07%	lr:0.000430
Ep: 3/25	It: 551/4130	batch_loss: 5.1366	batch_accuracy: 18.58%	lr:0.000432
Ep: 3/25	It: 601/4130	batch_loss: 5.1857	batch_accuracy: 17.87%	lr:0.000435
Ep: 3/25	It: 651/4130	batch_loss: 5.0938	batch_accuracy: 18.09%	lr:0.000437
Ep: 3/25	It: 701/4130	batch_loss: 4.9769	batch_accuracy: 19.34%	lr:0.000440
Ep: 3/25	It: 751/4130	batch_loss: 5.1204	batch_accuracy: 18.43%	lr:0.000442
Ep: 3/25	It: 801/4130	batch_loss: 4.9629	batch_accuracy: 19.43%	lr:0.000444
Ep: 3/25	It: 851/4130	batch_loss: 5.1714	batch_accuracy: 17.19%	lr:0.000447
Ep: 3/25	It: 901/4130	batch_loss: 5.0654	batch_accuracy: 19.26%	lr:0.000449
Ep: 3/25	It: 951/4130	batch_loss: 5.2052	batch_accuracy: 17.14%	lr:0.000452
Ep: 3/25	It: 1001/4130	batch_loss: 5.0305	batch_accuracy: 18.80%	lr:0.000454
Ep: 3/25	It: 1051/4130	batch_loss: 5.0536	batch_accuracy: 18.09%	lr:0.000456
Ep: 3/25	It: 1101/4130	batch_loss: 5.0582	batch_accuracy: 18.60%	lr:0.000459
Ep: 3/25	It: 1151/4130	batch_loss: 5.0697	batch_accuracy: 17.33%	lr:0.000461
Ep: 3/25	It: 1201/4130	batch_loss: 4.9520	batch_accuracy: 20.02%	lr:0.000464
Ep: 3/25	It: 1251/4130	batch_loss: 5.1175	batch_accuracy: 18.92%	lr:0.000466
Ep: 3/25	It: 1301/4130	batch_loss: 5.0704	batch_accuracy: 17.50%	lr:0.000468
Ep: 3/25	It: 1351/4130	batch_loss: 5.1243	batch_accuracy: 18.09%	lr:0.000471
Ep: 3/25	It: 1401/4130	batch_loss: 5.1545	batch_accuracy: 17.21%	lr:0.000473
Ep: 3/25	It: 1451/4130	batch_loss: 5.1289	batch_accuracy: 17.24%	lr:0.000476
Ep: 3/25	It: 1501/4130	batch_loss: 5.0822	batch_accuracy: 18.19%	lr:0.000478
Ep: 3/25	It: 1551/4130	batch_loss: 4.9101	batch_accuracy: 20.43%	lr:0.000480
Ep: 3/25	It: 1601/4130	batch_loss: 4.9908	batch_accuracy: 19.75%	lr:0.000483
Ep: 3/25	It: 1651/4130	batch_loss: 4.8925	batch_accuracy: 18.77%	lr:0.000485
Ep: 3/25	It: 1701/4130	batch_loss: 5.1024	batch_accuracy: 18.46%	lr:0.000488
Ep: 3/25	It: 1751/4130	batch_loss: 4.9669	batch_accuracy: 19.09%	lr:0.000490
Ep: 3/25	It: 1801/4130	batch_loss: 5.0628	batch_accuracy: 17.77%	lr:0.000492
Ep: 3/25	It: 1851/4130	batch_loss: 4.9895	batch_accuracy: 18.51%	lr:0.000495
Ep: 3/25	It: 1901/4130	batch_loss: 4.9202	batch_accuracy: 20.36%	lr:0.000497
Ep: 3/25	It: 1951/4130	batch_loss: 5.0561	batch_accuracy: 18.58%	lr:0.000500
Ep: 3/25	It: 2001/4130	batch_loss: 4.9308	batch_accuracy: 19.19%	lr:0.000502
Ep: 3/25	It: 2051/4130	batch_loss: 5.0549	batch_accuracy: 18.29%	lr:0.000504
Ep: 3/25	It: 2101/4130	batch_loss: 4.9636	batch_accuracy: 19.31%	lr:0.000507
Ep: 3/25	It: 2151/4130	batch_loss: 5.0556	batch_accuracy: 17.87%	lr:0.000509
Ep: 3/25	It: 2201/4130	batch_loss: 5.0163	batch_accuracy: 18.31%	lr:0.000512
Ep: 3/25	It: 2251/4130	batch_loss: 5.0307	batch_accuracy: 18.92%	lr:0.000514
Ep: 3/25	It: 2301/4130	batch_loss: 4.9712	batch_accuracy: 19.21%	lr:0.000516
Ep: 3/25	It: 2351/4130	batch_loss: 4.9583	batch_accuracy: 19.26%	lr:0.000519
Ep: 3/25	It: 2401/4130	batch_loss: 4.9826	batch_accuracy: 19.73%	lr:0.000521
Ep: 3/25	It: 2451/4130	batch_loss: 5.0643	batch_accuracy: 18.16%	lr:0.000524
Ep: 3/25	It: 2501/4130	batch_loss: 5.0757	batch_accuracy: 17.36%	lr:0.000526
Ep: 3/25	It: 2551/4130	batch_loss: 4.9556	batch_accuracy: 19.12%	lr:0.000528
Ep: 3/25	It: 2601/4130	batch_loss: 5.0111	batch_accuracy: 19.12%	lr:0.000531
Ep: 3/25	It: 2651/4130	batch_loss: 4.8419	batch_accuracy: 20.53%	lr:0.000533
Ep: 3/25	It: 2701/4130	batch_loss: 5.0299	batch_accuracy: 18.85%	lr:0.000536
Ep: 3/25	It: 2751/4130	batch_loss: 4.9272	batch_accuracy: 18.85%	lr:0.000538
Ep: 3/25	It: 2801/4130	batch_loss: 4.9545	batch_accuracy: 18.99%	lr:0.000540
Ep: 3/25	It: 2851/4130	batch_loss: 4.9589	batch_accuracy: 19.51%	lr:0.000543
Ep: 3/25	It: 2901/4130	batch_loss: 4.9125	batch_accuracy: 19.90%	lr:0.000545
Ep: 3/25	It: 2951/4130	batch_loss: 5.0347	batch_accuracy: 19.36%	lr:0.000548
Ep: 3/25	It: 3001/4130	batch_loss: 5.0257	batch_accuracy: 19.38%	lr:0.000550
Ep: 3/25	It: 3051/4130	batch_loss: 4.8522	batch_accuracy: 20.07%	lr:0.000552
Ep: 3/25	It: 3101/4130	batch_loss: 4.9444	batch_accuracy: 20.19%	lr:0.000555
Ep: 3/25	It: 3151/4130	batch_loss: 4.8733	batch_accuracy: 19.97%	lr:0.000557
Ep: 3/25	It: 3201/4130	batch_loss: 5.0166	batch_accuracy: 18.21%	lr:0.000559
Ep: 3/25	It: 3251/4130	batch_loss: 5.0174	batch_accuracy: 18.07%	lr:0.000562
Ep: 3/25	It: 3301/4130	batch_loss: 4.8109	batch_accuracy: 21.22%	lr:0.000564
Ep: 3/25	It: 3351/4130	batch_loss: 4.9773	batch_accuracy: 18.21%	lr:0.000567
Ep: 3/25	It: 3401/4130	batch_loss: 5.0004	batch_accuracy: 19.73%	lr:0.000569
Ep: 3/25	It: 3451/4130	batch_loss: 5.0257	batch_accuracy: 19.02%	lr:0.000571
Ep: 3/25	It: 3501/4130	batch_loss: 5.0673	batch_accuracy: 17.92%	lr:0.000574
Ep: 3/25	It: 3551/4130	batch_loss: 4.9462	batch_accuracy: 19.68%	lr:0.000576
Ep: 3/25	It: 3601/4130	batch_loss: 4.9896	batch_accuracy: 19.02%	lr:0.000579
Ep: 3/25	It: 3651/4130	batch_loss: 4.9773	batch_accuracy: 18.97%	lr:0.000581
Ep: 3/25	It: 3701/4130	batch_loss: 4.9108	batch_accuracy: 19.58%	lr:0.000583
Ep: 3/25	It: 3751/4130	batch_loss: 4.9997	batch_accuracy: 18.46%	lr:0.000586
Ep: 3/25	It: 3801/4130	batch_loss: 4.9007	batch_accuracy: 19.85%	lr:0.000588
Ep: 3/25	It: 3851/4130	batch_loss: 5.0075	batch_accuracy: 18.04%	lr:0.000591
Ep: 3/25	It: 3901/4130	batch_loss: 5.0949	batch_accuracy: 16.85%	lr:0.000593
Ep: 3/25	It: 3951/4130	batch_loss: 4.8816	batch_accuracy: 19.43%	lr:0.000595
Ep: 3/25	It: 4001/4130	batch_loss: 4.9886	batch_accuracy: 18.99%	lr:0.000598
Ep: 3/25	It: 4051/4130	batch_loss: 4.9170	batch_accuracy: 20.41%	lr:0.000600
Ep: 3/25	It: 4101/4130	batch_loss: 4.9096	batch_accuracy: 19.41%	lr:0.000603
Ep: 3/25	It: 4130/4130	batch_loss: 4.9955	batch_accuracy: 19.64%	lr:0.000604


Generated text for input text "You" is:
Youa, and benzene is shown to the effect of the MP, and the other, and the development of these two groups. This article reviews the performance, the first step in terms of the proposed in order to reduce the problem of the designs. This paper introduces that the most important properties of a single-fortunately, which is the main objective of a new technique for a new algorithm for the network and the performance. The model was conducted in the paper is presented. To determine the proposed scheme is to identify the use of the study of an important step of the proposed method for designing the design of the proposed approach.
<eot>
<sot>
Emmunal-Si-Fehicles and a new method of antitumor in a large-scale network.

This paper describes a novel approach, we investigate the problem of high-powering method. This paper presents the use of the proposed framework that a method for the proposed framework. To address the role of the proposed model. The algorithm has also the results that our approach is that they are discussed in which the performance of the proposed approach, and the proposed algorithm to provide a method to solve the proposed method. We discuss the problem


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 4/25	It: 1/4130	batch_loss: 4.8359	batch_accuracy: 20.19%	lr:0.000604
Ep: 4/25	It: 51/4130	batch_loss: 4.9820	batch_accuracy: 19.31%	lr:0.000606
Ep: 4/25	It: 101/4130	batch_loss: 4.9852	batch_accuracy: 19.02%	lr:0.000609
Ep: 4/25	It: 151/4130	batch_loss: 4.9367	batch_accuracy: 19.68%	lr:0.000611
Ep: 4/25	It: 201/4130	batch_loss: 4.9001	batch_accuracy: 19.85%	lr:0.000614
Ep: 4/25	It: 251/4130	batch_loss: 4.7913	batch_accuracy: 21.83%	lr:0.000616
Ep: 4/25	It: 301/4130	batch_loss: 4.9048	batch_accuracy: 18.92%	lr:0.000618
Ep: 4/25	It: 351/4130	batch_loss: 4.8294	batch_accuracy: 21.09%	lr:0.000621
Ep: 4/25	It: 401/4130	batch_loss: 4.8793	batch_accuracy: 19.90%	lr:0.000623
Ep: 4/25	It: 451/4130	batch_loss: 4.8806	batch_accuracy: 20.85%	lr:0.000626
Ep: 4/25	It: 501/4130	batch_loss: 4.8169	batch_accuracy: 20.21%	lr:0.000628
Ep: 4/25	It: 551/4130	batch_loss: 4.9975	batch_accuracy: 19.68%	lr:0.000630
Ep: 4/25	It: 601/4130	batch_loss: 4.9670	batch_accuracy: 19.14%	lr:0.000633
Ep: 4/25	It: 651/4130	batch_loss: 4.9229	batch_accuracy: 20.65%	lr:0.000635
Ep: 4/25	It: 701/4130	batch_loss: 4.8829	batch_accuracy: 19.14%	lr:0.000638
Ep: 4/25	It: 751/4130	batch_loss: 4.8863	batch_accuracy: 19.14%	lr:0.000640
Ep: 4/25	It: 801/4130	batch_loss: 4.8617	batch_accuracy: 20.92%	lr:0.000642
Ep: 4/25	It: 851/4130	batch_loss: 4.9016	batch_accuracy: 19.70%	lr:0.000645
Ep: 4/25	It: 901/4130	batch_loss: 4.8579	batch_accuracy: 21.41%	lr:0.000647
Ep: 4/25	It: 951/4130	batch_loss: 4.9366	batch_accuracy: 19.75%	lr:0.000650
Ep: 4/25	It: 1001/4130	batch_loss: 4.7858	batch_accuracy: 19.97%	lr:0.000652
Ep: 4/25	It: 1051/4130	batch_loss: 4.8937	batch_accuracy: 17.77%	lr:0.000654
Ep: 4/25	It: 1101/4130	batch_loss: 4.8686	batch_accuracy: 19.80%	lr:0.000657
Ep: 4/25	It: 1151/4130	batch_loss: 4.8255	batch_accuracy: 19.95%	lr:0.000659
Ep: 4/25	It: 1201/4130	batch_loss: 4.9561	batch_accuracy: 18.38%	lr:0.000662
Ep: 4/25	It: 1251/4130	batch_loss: 4.8529	batch_accuracy: 20.41%	lr:0.000664
Ep: 4/25	It: 1301/4130	batch_loss: 4.9280	batch_accuracy: 19.17%	lr:0.000666
Ep: 4/25	It: 1351/4130	batch_loss: 4.8285	batch_accuracy: 19.70%	lr:0.000669
Ep: 4/25	It: 1401/4130	batch_loss: 4.8921	batch_accuracy: 19.51%	lr:0.000671
Ep: 4/25	It: 1451/4130	batch_loss: 4.9142	batch_accuracy: 19.43%	lr:0.000674
Ep: 4/25	It: 1501/4130	batch_loss: 4.8258	batch_accuracy: 21.14%	lr:0.000676
Ep: 4/25	It: 1551/4130	batch_loss: 4.9259	batch_accuracy: 19.02%	lr:0.000678
Ep: 4/25	It: 1601/4130	batch_loss: 5.1063	batch_accuracy: 17.90%	lr:0.000681
Ep: 4/25	It: 1651/4130	batch_loss: 4.8616	batch_accuracy: 19.56%	lr:0.000683
Ep: 4/25	It: 1701/4130	batch_loss: 4.9526	batch_accuracy: 19.12%	lr:0.000686
Ep: 4/25	It: 1751/4130	batch_loss: 4.8555	batch_accuracy: 19.58%	lr:0.000688
Ep: 4/25	It: 1801/4130	batch_loss: 4.8503	batch_accuracy: 21.51%	lr:0.000690
Ep: 4/25	It: 1851/4130	batch_loss: 4.8261	batch_accuracy: 20.70%	lr:0.000693
Ep: 4/25	It: 1901/4130	batch_loss: 4.8059	batch_accuracy: 20.46%	lr:0.000695
Ep: 4/25	It: 1951/4130	batch_loss: 4.9115	batch_accuracy: 18.68%	lr:0.000698
Ep: 4/25	It: 2001/4130	batch_loss: 4.9188	batch_accuracy: 19.48%	lr:0.000700
Ep: 4/25	It: 2051/4130	batch_loss: 4.8439	batch_accuracy: 20.34%	lr:0.000702
Ep: 4/25	It: 2101/4130	batch_loss: 4.8191	batch_accuracy: 20.31%	lr:0.000705
Ep: 4/25	It: 2151/4130	batch_loss: 4.8463	batch_accuracy: 19.34%	lr:0.000707
Ep: 4/25	It: 2201/4130	batch_loss: 4.8531	batch_accuracy: 19.63%	lr:0.000710
Ep: 4/25	It: 2251/4130	batch_loss: 4.8462	batch_accuracy: 20.29%	lr:0.000712
Ep: 4/25	It: 2301/4130	batch_loss: 4.9565	batch_accuracy: 18.29%	lr:0.000714
Ep: 4/25	It: 2351/4130	batch_loss: 4.8203	batch_accuracy: 19.51%	lr:0.000717
Ep: 4/25	It: 2401/4130	batch_loss: 4.7601	batch_accuracy: 20.39%	lr:0.000719
Ep: 4/25	It: 2451/4130	batch_loss: 4.8738	batch_accuracy: 19.73%	lr:0.000722
Ep: 4/25	It: 2501/4130	batch_loss: 4.8375	batch_accuracy: 19.04%	lr:0.000724
Ep: 4/25	It: 2551/4130	batch_loss: 4.8874	batch_accuracy: 19.70%	lr:0.000726
Ep: 4/25	It: 2601/4130	batch_loss: 4.6989	batch_accuracy: 22.85%	lr:0.000729
Ep: 4/25	It: 2651/4130	batch_loss: 4.7277	batch_accuracy: 21.22%	lr:0.000731
Ep: 4/25	It: 2701/4130	batch_loss: 4.8537	batch_accuracy: 19.26%	lr:0.000734
Ep: 4/25	It: 2751/4130	batch_loss: 4.7666	batch_accuracy: 20.80%	lr:0.000736
Ep: 4/25	It: 2801/4130	batch_loss: 4.8489	batch_accuracy: 20.29%	lr:0.000738
Ep: 4/25	It: 2851/4130	batch_loss: 4.9326	batch_accuracy: 19.19%	lr:0.000741
Ep: 4/25	It: 2901/4130	batch_loss: 4.8880	batch_accuracy: 19.24%	lr:0.000743
Ep: 4/25	It: 2951/4130	batch_loss: 4.7320	batch_accuracy: 20.61%	lr:0.000746
Ep: 4/25	It: 3001/4130	batch_loss: 4.8577	batch_accuracy: 20.53%	lr:0.000748
Ep: 4/25	It: 3051/4130	batch_loss: 5.0039	batch_accuracy: 18.68%	lr:0.000750
Ep: 4/25	It: 3101/4130	batch_loss: 4.8782	batch_accuracy: 20.04%	lr:0.000753
Ep: 4/25	It: 3151/4130	batch_loss: 4.8096	batch_accuracy: 19.80%	lr:0.000755
Ep: 4/25	It: 3201/4130	batch_loss: 4.7587	batch_accuracy: 21.24%	lr:0.000757
Ep: 4/25	It: 3251/4130	batch_loss: 4.7419	batch_accuracy: 21.75%	lr:0.000760
Ep: 4/25	It: 3301/4130	batch_loss: 4.8410	batch_accuracy: 20.09%	lr:0.000762
Ep: 4/25	It: 3351/4130	batch_loss: 4.8385	batch_accuracy: 20.09%	lr:0.000765
Ep: 4/25	It: 3401/4130	batch_loss: 4.8886	batch_accuracy: 19.48%	lr:0.000767
Ep: 4/25	It: 3451/4130	batch_loss: 4.8031	batch_accuracy: 20.97%	lr:0.000769
Ep: 4/25	It: 3501/4130	batch_loss: 4.7651	batch_accuracy: 21.41%	lr:0.000772
Ep: 4/25	It: 3551/4130	batch_loss: 4.8247	batch_accuracy: 20.65%	lr:0.000774
Ep: 4/25	It: 3601/4130	batch_loss: 4.8335	batch_accuracy: 18.90%	lr:0.000777
Ep: 4/25	It: 3651/4130	batch_loss: 4.8677	batch_accuracy: 18.82%	lr:0.000779
Ep: 4/25	It: 3701/4130	batch_loss: 4.8716	batch_accuracy: 19.43%	lr:0.000781
Ep: 4/25	It: 3751/4130	batch_loss: 4.7686	batch_accuracy: 20.51%	lr:0.000784
Ep: 4/25	It: 3801/4130	batch_loss: 4.9374	batch_accuracy: 19.19%	lr:0.000786
Ep: 4/25	It: 3851/4130	batch_loss: 4.7576	batch_accuracy: 21.58%	lr:0.000789
Ep: 4/25	It: 3901/4130	batch_loss: 4.7837	batch_accuracy: 20.46%	lr:0.000791
Ep: 4/25	It: 3951/4130	batch_loss: 4.8409	batch_accuracy: 19.31%	lr:0.000793
Ep: 4/25	It: 4001/4130	batch_loss: 4.7954	batch_accuracy: 21.19%	lr:0.000796
Ep: 4/25	It: 4051/4130	batch_loss: 4.8448	batch_accuracy: 19.95%	lr:0.000798
Ep: 4/25	It: 4101/4130	batch_loss: 4.8761	batch_accuracy: 19.97%	lr:0.000801
Ep: 4/25	It: 4130/4130	batch_loss: 4.9360	batch_accuracy: 19.80%	lr:0.000802


Generated text for input text "You" is:
Youtlement. The article has been shown to improve-s.
The use of these findings in vivo, this study was conducted with this approach to explore the performance of the proposed approach. The proposed scheme is presented to obtaining the application of this technique is designed. It is proposed that these results show that the results are used to solve a novel approach. This paper proposes a new approach that uses a simple approach to the simulation design. Italytical approach is a new method for an application of proposed approach for the proposed method. The simulation of the model are presented based on the simulation, and a novel algorithm, which uses a model of a system and a real-time, and the proposed approach for the algorithm of the system-selfare and a model for design and design.
<eot>
<sot>
Meived Mechanical Product on Family Employment of Capltology (MRI)

In this paper, we investigate the effectiveness of the proposed system and the proposed model that can be implemented in a real-time framework to a multifrons to achieve the first of the first time. This study aims to be used for the simulation and simulation. It can be used for


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 5/25	It: 1/4130	batch_loss: 4.8321	batch_accuracy: 20.41%	lr:0.000802
Ep: 5/25	It: 51/4130	batch_loss: 4.7625	batch_accuracy: 20.21%	lr:0.000804
Ep: 5/25	It: 101/4130	batch_loss: 4.7740	batch_accuracy: 20.17%	lr:0.000807
Ep: 5/25	It: 151/4130	batch_loss: 4.8769	batch_accuracy: 20.17%	lr:0.000809
Ep: 5/25	It: 201/4130	batch_loss: 4.8438	batch_accuracy: 19.46%	lr:0.000812
Ep: 5/25	It: 251/4130	batch_loss: 4.8509	batch_accuracy: 20.04%	lr:0.000814
Ep: 5/25	It: 301/4130	batch_loss: 4.6558	batch_accuracy: 22.17%	lr:0.000816
Ep: 5/25	It: 351/4130	batch_loss: 4.8783	batch_accuracy: 19.90%	lr:0.000819
Ep: 5/25	It: 401/4130	batch_loss: 4.8617	batch_accuracy: 19.26%	lr:0.000821
Ep: 5/25	It: 451/4130	batch_loss: 4.7296	batch_accuracy: 20.92%	lr:0.000824
Ep: 5/25	It: 501/4130	batch_loss: 4.7943	batch_accuracy: 20.31%	lr:0.000826
Ep: 5/25	It: 551/4130	batch_loss: 4.7894	batch_accuracy: 20.58%	lr:0.000828
Ep: 5/25	It: 601/4130	batch_loss: 4.8003	batch_accuracy: 21.04%	lr:0.000831
Ep: 5/25	It: 651/4130	batch_loss: 4.8879	batch_accuracy: 19.07%	lr:0.000833
Ep: 5/25	It: 701/4130	batch_loss: 4.7624	batch_accuracy: 21.00%	lr:0.000836
Ep: 5/25	It: 751/4130	batch_loss: 4.6803	batch_accuracy: 22.34%	lr:0.000838
Ep: 5/25	It: 801/4130	batch_loss: 4.8129	batch_accuracy: 20.70%	lr:0.000840
Ep: 5/25	It: 851/4130	batch_loss: 4.7521	batch_accuracy: 21.66%	lr:0.000843
Ep: 5/25	It: 901/4130	batch_loss: 4.7047	batch_accuracy: 21.36%	lr:0.000845
Ep: 5/25	It: 951/4130	batch_loss: 4.8429	batch_accuracy: 20.09%	lr:0.000848
Ep: 5/25	It: 1001/4130	batch_loss: 4.7674	batch_accuracy: 20.70%	lr:0.000850
Ep: 5/25	It: 1051/4130	batch_loss: 4.8208	batch_accuracy: 19.53%	lr:0.000852
Ep: 5/25	It: 1101/4130	batch_loss: 4.7744	batch_accuracy: 20.19%	lr:0.000855
Ep: 5/25	It: 1151/4130	batch_loss: 4.8173	batch_accuracy: 19.73%	lr:0.000857
Ep: 5/25	It: 1201/4130	batch_loss: 4.7995	batch_accuracy: 20.83%	lr:0.000860
Ep: 5/25	It: 1251/4130	batch_loss: 4.7287	batch_accuracy: 22.02%	lr:0.000862
Ep: 5/25	It: 1301/4130	batch_loss: 4.7346	batch_accuracy: 20.26%	lr:0.000864
Ep: 5/25	It: 1351/4130	batch_loss: 4.6853	batch_accuracy: 21.39%	lr:0.000867
Ep: 5/25	It: 1401/4130	batch_loss: 4.8099	batch_accuracy: 18.90%	lr:0.000869
Ep: 5/25	It: 1451/4130	batch_loss: 4.6975	batch_accuracy: 22.14%	lr:0.000872
Ep: 5/25	It: 1501/4130	batch_loss: 4.8174	batch_accuracy: 20.07%	lr:0.000874
Ep: 5/25	It: 1551/4130	batch_loss: 4.8800	batch_accuracy: 18.68%	lr:0.000876
Ep: 5/25	It: 1601/4130	batch_loss: 4.7901	batch_accuracy: 20.34%	lr:0.000879
Ep: 5/25	It: 1651/4130	batch_loss: 4.8149	batch_accuracy: 20.70%	lr:0.000881
Ep: 5/25	It: 1701/4130	batch_loss: 4.7504	batch_accuracy: 21.73%	lr:0.000884
Ep: 5/25	It: 1751/4130	batch_loss: 4.6312	batch_accuracy: 21.92%	lr:0.000886
Ep: 5/25	It: 1801/4130	batch_loss: 4.7708	batch_accuracy: 20.56%	lr:0.000888
Ep: 5/25	It: 1851/4130	batch_loss: 4.6796	batch_accuracy: 21.70%	lr:0.000891
Ep: 5/25	It: 1901/4130	batch_loss: 4.8161	batch_accuracy: 20.68%	lr:0.000893
Ep: 5/25	It: 1951/4130	batch_loss: 4.6749	batch_accuracy: 20.85%	lr:0.000896
Ep: 5/25	It: 2001/4130	batch_loss: 4.8798	batch_accuracy: 20.21%	lr:0.000898
Ep: 5/25	It: 2051/4130	batch_loss: 4.7290	batch_accuracy: 21.61%	lr:0.000900
Ep: 5/25	It: 2101/4130	batch_loss: 4.7750	batch_accuracy: 21.80%	lr:0.000903
Ep: 5/25	It: 2151/4130	batch_loss: 4.8292	batch_accuracy: 20.39%	lr:0.000905
Ep: 5/25	It: 2201/4130	batch_loss: 4.6479	batch_accuracy: 22.71%	lr:0.000908
Ep: 5/25	It: 2251/4130	batch_loss: 4.7679	batch_accuracy: 20.75%	lr:0.000910
Ep: 5/25	It: 2301/4130	batch_loss: 4.7970	batch_accuracy: 21.17%	lr:0.000912
Ep: 5/25	It: 2351/4130	batch_loss: 4.6511	batch_accuracy: 23.14%	lr:0.000915
Ep: 5/25	It: 2401/4130	batch_loss: 4.7849	batch_accuracy: 21.36%	lr:0.000917
Ep: 5/25	It: 2451/4130	batch_loss: 4.7889	batch_accuracy: 20.51%	lr:0.000920
Ep: 5/25	It: 2501/4130	batch_loss: 4.7415	batch_accuracy: 21.90%	lr:0.000922
Ep: 5/25	It: 2551/4130	batch_loss: 4.8034	batch_accuracy: 20.61%	lr:0.000924
Ep: 5/25	It: 2601/4130	batch_loss: 4.6965	batch_accuracy: 21.92%	lr:0.000927
Ep: 5/25	It: 2651/4130	batch_loss: 4.6800	batch_accuracy: 20.97%	lr:0.000929
Ep: 5/25	It: 2701/4130	batch_loss: 4.6469	batch_accuracy: 22.27%	lr:0.000932
Ep: 5/25	It: 2751/4130	batch_loss: 4.8260	batch_accuracy: 21.31%	lr:0.000934
Ep: 5/25	It: 2801/4130	batch_loss: 4.7106	batch_accuracy: 21.90%	lr:0.000936
Ep: 5/25	It: 2851/4130	batch_loss: 4.6809	batch_accuracy: 21.63%	lr:0.000939
Ep: 5/25	It: 2901/4130	batch_loss: 4.7966	batch_accuracy: 20.58%	lr:0.000941
Ep: 5/25	It: 2951/4130	batch_loss: 4.8226	batch_accuracy: 20.65%	lr:0.000944
Ep: 5/25	It: 3001/4130	batch_loss: 4.7555	batch_accuracy: 20.90%	lr:0.000946
Ep: 5/25	It: 3051/4130	batch_loss: 4.7196	batch_accuracy: 21.44%	lr:0.000948
Ep: 5/25	It: 3101/4130	batch_loss: 4.6534	batch_accuracy: 21.66%	lr:0.000951
Ep: 5/25	It: 3151/4130	batch_loss: 4.6644	batch_accuracy: 21.39%	lr:0.000953
Ep: 5/25	It: 3201/4130	batch_loss: 4.7734	batch_accuracy: 21.85%	lr:0.000956
Ep: 5/25	It: 3251/4130	batch_loss: 4.8390	batch_accuracy: 20.61%	lr:0.000958
Ep: 5/25	It: 3301/4130	batch_loss: 4.6631	batch_accuracy: 21.75%	lr:0.000960
Ep: 5/25	It: 3351/4130	batch_loss: 4.7472	batch_accuracy: 21.26%	lr:0.000963
Ep: 5/25	It: 3401/4130	batch_loss: 4.7169	batch_accuracy: 21.14%	lr:0.000965
Ep: 5/25	It: 3451/4130	batch_loss: 4.6806	batch_accuracy: 20.90%	lr:0.000967
Ep: 5/25	It: 3501/4130	batch_loss: 4.7555	batch_accuracy: 22.09%	lr:0.000970
Ep: 5/25	It: 3551/4130	batch_loss: 4.7357	batch_accuracy: 20.80%	lr:0.000972
Ep: 5/25	It: 3601/4130	batch_loss: 4.5813	batch_accuracy: 22.92%	lr:0.000975
Ep: 5/25	It: 3651/4130	batch_loss: 4.8121	batch_accuracy: 19.46%	lr:0.000977
Ep: 5/25	It: 3701/4130	batch_loss: 4.7176	batch_accuracy: 21.51%	lr:0.000979
Ep: 5/25	It: 3751/4130	batch_loss: 4.7284	batch_accuracy: 22.07%	lr:0.000982
Ep: 5/25	It: 3801/4130	batch_loss: 4.5547	batch_accuracy: 23.36%	lr:0.000984
Ep: 5/25	It: 3851/4130	batch_loss: 4.6899	batch_accuracy: 21.90%	lr:0.000987
Ep: 5/25	It: 3901/4130	batch_loss: 4.7484	batch_accuracy: 22.14%	lr:0.000989
Ep: 5/25	It: 3951/4130	batch_loss: 4.6692	batch_accuracy: 21.92%	lr:0.000991
Ep: 5/25	It: 4001/4130	batch_loss: 4.7212	batch_accuracy: 21.58%	lr:0.000994
Ep: 5/25	It: 4051/4130	batch_loss: 4.7239	batch_accuracy: 21.90%	lr:0.000996
Ep: 5/25	It: 4101/4130	batch_loss: 4.7267	batch_accuracy: 20.92%	lr:0.000999
Ep: 5/25	It: 4130/4130	batch_loss: 4.5779	batch_accuracy: 21.05%	lr:0.001000


Generated text for input text "You" is:
Youar and the criminal and the druster is discussed. The present study revealed that the effect is also shown to determine whether the presence of the criminal nerve, is observed in averaged manner. This study also examined the effect of the HP in vitro. In this study, there is a positive effect on a new technique. The proposed method is based on a novel system based on the application of this method in terms of data processing, the first-order-space, we present a single-chemical-methods model. It is shown that the system is an efficient and the same time, which is based on the model of the proposed controller (GI) to improve the proposed system, and a new method for the proposed method. The proposed algorithm has been applied to the algorithm for detecting the performance. The proposed algorithm is based on a method that uses a numerical algorithm.
<eot>
<sot>
Systems and Experimental data and the analysis of the proposed method.

In this paper, we propose a hybrid model with the proposed method for real-time-sectivity of the algorithm to estimate the system in a new image. This paper investigates the method for a simple-


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 6/25	It: 1/4130	batch_loss: 4.6840	batch_accuracy: 22.66%	lr:0.001000
Ep: 6/25	It: 51/4130	batch_loss: 4.7134	batch_accuracy: 21.46%	lr:0.001000
Ep: 6/25	It: 101/4130	batch_loss: 4.6392	batch_accuracy: 22.56%	lr:0.001000
Ep: 6/25	It: 151/4130	batch_loss: 4.6740	batch_accuracy: 21.51%	lr:0.001000
Ep: 6/25	It: 201/4130	batch_loss: 4.6980	batch_accuracy: 21.73%	lr:0.001000
Ep: 6/25	It: 251/4130	batch_loss: 4.6796	batch_accuracy: 21.04%	lr:0.001000
Ep: 6/25	It: 301/4130	batch_loss: 4.7094	batch_accuracy: 22.53%	lr:0.001000
Ep: 6/25	It: 351/4130	batch_loss: 4.6315	batch_accuracy: 22.27%	lr:0.001000
Ep: 6/25	It: 401/4130	batch_loss: 4.5976	batch_accuracy: 22.95%	lr:0.001000
Ep: 6/25	It: 451/4130	batch_loss: 4.6677	batch_accuracy: 22.36%	lr:0.001000
Ep: 6/25	It: 501/4130	batch_loss: 4.7080	batch_accuracy: 21.04%	lr:0.001000
Ep: 6/25	It: 551/4130	batch_loss: 4.6948	batch_accuracy: 21.95%	lr:0.001000
Ep: 6/25	It: 601/4130	batch_loss: 4.5861	batch_accuracy: 23.58%	lr:0.001000
Ep: 6/25	It: 651/4130	batch_loss: 4.6040	batch_accuracy: 22.78%	lr:0.001000
Ep: 6/25	It: 701/4130	batch_loss: 4.6930	batch_accuracy: 22.09%	lr:0.001000
Ep: 6/25	It: 751/4130	batch_loss: 4.6393	batch_accuracy: 21.85%	lr:0.001000
Ep: 6/25	It: 801/4130	batch_loss: 4.6511	batch_accuracy: 22.12%	lr:0.001000
Ep: 6/25	It: 851/4130	batch_loss: 4.6322	batch_accuracy: 21.63%	lr:0.001000
Ep: 6/25	It: 901/4130	batch_loss: 4.8120	batch_accuracy: 20.02%	lr:0.001000
Ep: 6/25	It: 951/4130	batch_loss: 4.7428	batch_accuracy: 21.53%	lr:0.001000
Ep: 6/25	It: 1001/4130	batch_loss: 4.5388	batch_accuracy: 22.71%	lr:0.001000
Ep: 6/25	It: 1051/4130	batch_loss: 4.6483	batch_accuracy: 22.02%	lr:0.001000
Ep: 6/25	It: 1101/4130	batch_loss: 4.7048	batch_accuracy: 21.88%	lr:0.001000
Ep: 6/25	It: 1151/4130	batch_loss: 4.7829	batch_accuracy: 21.29%	lr:0.001000
Ep: 6/25	It: 1201/4130	batch_loss: 4.6200	batch_accuracy: 22.44%	lr:0.000999
Ep: 6/25	It: 1251/4130	batch_loss: 4.6253	batch_accuracy: 22.17%	lr:0.000999
Ep: 6/25	It: 1301/4130	batch_loss: 4.7468	batch_accuracy: 21.53%	lr:0.000999
Ep: 6/25	It: 1351/4130	batch_loss: 4.7223	batch_accuracy: 21.26%	lr:0.000999
Ep: 6/25	It: 1401/4130	batch_loss: 4.6174	batch_accuracy: 22.53%	lr:0.000999
Ep: 6/25	It: 1451/4130	batch_loss: 4.7979	batch_accuracy: 20.73%	lr:0.000999
Ep: 6/25	It: 1501/4130	batch_loss: 4.7016	batch_accuracy: 21.83%	lr:0.000999
Ep: 6/25	It: 1551/4130	batch_loss: 4.6649	batch_accuracy: 22.24%	lr:0.000999
Ep: 6/25	It: 1601/4130	batch_loss: 4.6703	batch_accuracy: 21.78%	lr:0.000999
Ep: 6/25	It: 1651/4130	batch_loss: 4.6163	batch_accuracy: 22.02%	lr:0.000999
Ep: 6/25	It: 1701/4130	batch_loss: 4.6403	batch_accuracy: 22.14%	lr:0.000999
Ep: 6/25	It: 1751/4130	batch_loss: 4.6654	batch_accuracy: 22.29%	lr:0.000999
Ep: 6/25	It: 1801/4130	batch_loss: 4.6612	batch_accuracy: 22.17%	lr:0.000999
Ep: 6/25	It: 1851/4130	batch_loss: 4.7468	batch_accuracy: 21.85%	lr:0.000999
Ep: 6/25	It: 1901/4130	batch_loss: 4.6398	batch_accuracy: 22.53%	lr:0.000999
Ep: 6/25	It: 1951/4130	batch_loss: 4.7561	batch_accuracy: 20.51%	lr:0.000999
Ep: 6/25	It: 2001/4130	batch_loss: 4.7066	batch_accuracy: 21.34%	lr:0.000999
Ep: 6/25	It: 2051/4130	batch_loss: 4.7101	batch_accuracy: 21.39%	lr:0.000998
Ep: 6/25	It: 2101/4130	batch_loss: 4.6983	batch_accuracy: 21.04%	lr:0.000998
Ep: 6/25	It: 2151/4130	batch_loss: 4.7939	batch_accuracy: 20.80%	lr:0.000998
Ep: 6/25	It: 2201/4130	batch_loss: 4.6191	batch_accuracy: 22.75%	lr:0.000998
Ep: 6/25	It: 2251/4130	batch_loss: 4.6409	batch_accuracy: 21.88%	lr:0.000998
Ep: 6/25	It: 2301/4130	batch_loss: 4.6759	batch_accuracy: 21.83%	lr:0.000998
Ep: 6/25	It: 2351/4130	batch_loss: 4.7156	batch_accuracy: 20.61%	lr:0.000998
Ep: 6/25	It: 2401/4130	batch_loss: 4.7194	batch_accuracy: 22.22%	lr:0.000998
Ep: 6/25	It: 2451/4130	batch_loss: 4.7058	batch_accuracy: 21.04%	lr:0.000998
Ep: 6/25	It: 2501/4130	batch_loss: 4.6464	batch_accuracy: 22.49%	lr:0.000998
Ep: 6/25	It: 2551/4130	batch_loss: 4.7598	batch_accuracy: 20.56%	lr:0.000998
Ep: 6/25	It: 2601/4130	batch_loss: 4.6872	batch_accuracy: 20.92%	lr:0.000998
Ep: 6/25	It: 2651/4130	batch_loss: 4.6370	batch_accuracy: 21.46%	lr:0.000997
Ep: 6/25	It: 2701/4130	batch_loss: 4.6330	batch_accuracy: 22.51%	lr:0.000997
Ep: 6/25	It: 2751/4130	batch_loss: 4.5814	batch_accuracy: 23.29%	lr:0.000997
Ep: 6/25	It: 2801/4130	batch_loss: 4.6408	batch_accuracy: 21.90%	lr:0.000997
Ep: 6/25	It: 2851/4130	batch_loss: 4.6184	batch_accuracy: 23.10%	lr:0.000997
Ep: 6/25	It: 2901/4130	batch_loss: 4.7039	batch_accuracy: 21.73%	lr:0.000997
Ep: 6/25	It: 2951/4130	batch_loss: 4.6426	batch_accuracy: 21.90%	lr:0.000997
Ep: 6/25	It: 3001/4130	batch_loss: 4.7373	batch_accuracy: 21.14%	lr:0.000997
Ep: 6/25	It: 3051/4130	batch_loss: 4.7735	batch_accuracy: 20.80%	lr:0.000997
Ep: 6/25	It: 3101/4130	batch_loss: 4.5745	batch_accuracy: 23.39%	lr:0.000997
Ep: 6/25	It: 3151/4130	batch_loss: 4.6205	batch_accuracy: 23.12%	lr:0.000996
Ep: 6/25	It: 3201/4130	batch_loss: 4.6573	batch_accuracy: 21.63%	lr:0.000996
Ep: 6/25	It: 3251/4130	batch_loss: 4.8299	batch_accuracy: 20.95%	lr:0.000996
Ep: 6/25	It: 3301/4130	batch_loss: 4.7050	batch_accuracy: 20.80%	lr:0.000996
Ep: 6/25	It: 3351/4130	batch_loss: 4.6485	batch_accuracy: 21.90%	lr:0.000996
Ep: 6/25	It: 3401/4130	batch_loss: 4.6884	batch_accuracy: 20.83%	lr:0.000996
Ep: 6/25	It: 3451/4130	batch_loss: 4.6245	batch_accuracy: 23.63%	lr:0.000996
Ep: 6/25	It: 3501/4130	batch_loss: 4.6758	batch_accuracy: 21.53%	lr:0.000996
Ep: 6/25	It: 3551/4130	batch_loss: 4.7880	batch_accuracy: 20.58%	lr:0.000995
Ep: 6/25	It: 3601/4130	batch_loss: 4.7227	batch_accuracy: 21.09%	lr:0.000995
Ep: 6/25	It: 3651/4130	batch_loss: 4.5906	batch_accuracy: 22.00%	lr:0.000995
Ep: 6/25	It: 3701/4130	batch_loss: 4.7123	batch_accuracy: 20.95%	lr:0.000995
Ep: 6/25	It: 3751/4130	batch_loss: 4.6523	batch_accuracy: 22.17%	lr:0.000995
Ep: 6/25	It: 3801/4130	batch_loss: 4.6542	batch_accuracy: 22.36%	lr:0.000995
Ep: 6/25	It: 3851/4130	batch_loss: 4.5880	batch_accuracy: 22.88%	lr:0.000995
Ep: 6/25	It: 3901/4130	batch_loss: 4.6020	batch_accuracy: 22.92%	lr:0.000995
Ep: 6/25	It: 3951/4130	batch_loss: 4.6249	batch_accuracy: 22.29%	lr:0.000994
Ep: 6/25	It: 4001/4130	batch_loss: 4.6603	batch_accuracy: 22.75%	lr:0.000994
Ep: 6/25	It: 4051/4130	batch_loss: 4.6368	batch_accuracy: 21.22%	lr:0.000994
Ep: 6/25	It: 4101/4130	batch_loss: 4.6749	batch_accuracy: 21.02%	lr:0.000994
Ep: 6/25	It: 4130/4130	batch_loss: 4.4779	batch_accuracy: 24.52%	lr:0.000994


Generated text for input text "You" is:
You are used to evaluate the quality of these algorithms, which was developed and implemented. This article describes a new algorithm to analyze the effectiveness of information for future applications. The simulation results are discussed. The results are presented to solve the design, we use of a single-cor-core. We describe the framework of an approach to identify the performance of the network and the quality of the proposed approach. This study aims to describe the performance of the proposed method and it is proposed to achieve the effectiveness of the proposed approach, which allows us the algorithm for the real-time. This paper proposes an alternative to the algorithm based on the data of the proposed method and a novel method of the two components of the proposed algorithm. The proposed method provides an overview of this paper, it can help better use for an application of a new approach. This paper presents a new model to the proposed algorithm for improving the performance of the method based on a linear method, which can be used for the data. The proposed method has been conducted to evaluate the performance of the proposed model of this system and a single-path to improve the performance. This paper introduces a novel data-based and an integrated model to obtain a new method, we propose a new framework to achieve the proposed


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 7/25	It: 1/4130	batch_loss: 4.7406	batch_accuracy: 22.24%	lr:0.000994
Ep: 7/25	It: 51/4130	batch_loss: 4.6152	batch_accuracy: 22.68%	lr:0.000994
Ep: 7/25	It: 101/4130	batch_loss: 4.6221	batch_accuracy: 23.29%	lr:0.000994
Ep: 7/25	It: 151/4130	batch_loss: 4.5724	batch_accuracy: 23.05%	lr:0.000993
Ep: 7/25	It: 201/4130	batch_loss: 4.7182	batch_accuracy: 22.17%	lr:0.000993
Ep: 7/25	It: 251/4130	batch_loss: 4.6924	batch_accuracy: 21.66%	lr:0.000993
Ep: 7/25	It: 301/4130	batch_loss: 4.6677	batch_accuracy: 21.63%	lr:0.000993
Ep: 7/25	It: 351/4130	batch_loss: 4.6812	batch_accuracy: 22.22%	lr:0.000993
Ep: 7/25	It: 401/4130	batch_loss: 4.5591	batch_accuracy: 22.78%	lr:0.000993
Ep: 7/25	It: 451/4130	batch_loss: 4.5495	batch_accuracy: 22.90%	lr:0.000993
Ep: 7/25	It: 501/4130	batch_loss: 4.6130	batch_accuracy: 22.17%	lr:0.000992
Ep: 7/25	It: 551/4130	batch_loss: 4.5552	batch_accuracy: 22.90%	lr:0.000992
Ep: 7/25	It: 601/4130	batch_loss: 4.6189	batch_accuracy: 23.14%	lr:0.000992
Ep: 7/25	It: 651/4130	batch_loss: 4.7094	batch_accuracy: 21.00%	lr:0.000992
Ep: 7/25	It: 701/4130	batch_loss: 4.6243	batch_accuracy: 21.48%	lr:0.000992
Ep: 7/25	It: 751/4130	batch_loss: 4.7774	batch_accuracy: 21.04%	lr:0.000991
Ep: 7/25	It: 801/4130	batch_loss: 4.6647	batch_accuracy: 22.44%	lr:0.000991
Ep: 7/25	It: 851/4130	batch_loss: 4.4798	batch_accuracy: 23.49%	lr:0.000991
Ep: 7/25	It: 901/4130	batch_loss: 4.5828	batch_accuracy: 22.05%	lr:0.000991
Ep: 7/25	It: 951/4130	batch_loss: 4.5841	batch_accuracy: 22.68%	lr:0.000991
Ep: 7/25	It: 1001/4130	batch_loss: 4.6168	batch_accuracy: 22.78%	lr:0.000991
Ep: 7/25	It: 1051/4130	batch_loss: 4.6346	batch_accuracy: 21.58%	lr:0.000990
Ep: 7/25	It: 1101/4130	batch_loss: 4.6147	batch_accuracy: 22.07%	lr:0.000990
Ep: 7/25	It: 1151/4130	batch_loss: 4.6149	batch_accuracy: 23.10%	lr:0.000990
Ep: 7/25	It: 1201/4130	batch_loss: 4.6614	batch_accuracy: 21.48%	lr:0.000990
Ep: 7/25	It: 1251/4130	batch_loss: 4.5694	batch_accuracy: 23.36%	lr:0.000990
Ep: 7/25	It: 1301/4130	batch_loss: 4.6017	batch_accuracy: 22.90%	lr:0.000989
Ep: 7/25	It: 1351/4130	batch_loss: 4.5616	batch_accuracy: 23.22%	lr:0.000989
Ep: 7/25	It: 1401/4130	batch_loss: 4.6856	batch_accuracy: 22.27%	lr:0.000989
Ep: 7/25	It: 1451/4130	batch_loss: 4.6095	batch_accuracy: 22.56%	lr:0.000989
Ep: 7/25	It: 1501/4130	batch_loss: 4.5364	batch_accuracy: 23.22%	lr:0.000989
Ep: 7/25	It: 1551/4130	batch_loss: 4.6551	batch_accuracy: 22.73%	lr:0.000988
Ep: 7/25	It: 1601/4130	batch_loss: 4.5679	batch_accuracy: 23.56%	lr:0.000988
Ep: 7/25	It: 1651/4130	batch_loss: 4.6851	batch_accuracy: 21.80%	lr:0.000988
Ep: 7/25	It: 1701/4130	batch_loss: 4.5056	batch_accuracy: 23.27%	lr:0.000988
Ep: 7/25	It: 1751/4130	batch_loss: 4.6781	batch_accuracy: 21.97%	lr:0.000988
Ep: 7/25	It: 1801/4130	batch_loss: 4.7323	batch_accuracy: 22.05%	lr:0.000987
Ep: 7/25	It: 1851/4130	batch_loss: 4.6176	batch_accuracy: 21.78%	lr:0.000987
Ep: 7/25	It: 1901/4130	batch_loss: 4.6793	batch_accuracy: 21.78%	lr:0.000987
Ep: 7/25	It: 1951/4130	batch_loss: 4.5364	batch_accuracy: 24.10%	lr:0.000987
Ep: 7/25	It: 2001/4130	batch_loss: 4.5160	batch_accuracy: 24.46%	lr:0.000987
Ep: 7/25	It: 2051/4130	batch_loss: 4.5312	batch_accuracy: 23.44%	lr:0.000986
Ep: 7/25	It: 2101/4130	batch_loss: 4.6322	batch_accuracy: 23.32%	lr:0.000986
Ep: 7/25	It: 2151/4130	batch_loss: 4.6069	batch_accuracy: 22.14%	lr:0.000986
Ep: 7/25	It: 2201/4130	batch_loss: 4.6347	batch_accuracy: 22.73%	lr:0.000986
Ep: 7/25	It: 2251/4130	batch_loss: 4.5625	batch_accuracy: 23.32%	lr:0.000985
Ep: 7/25	It: 2301/4130	batch_loss: 4.6846	batch_accuracy: 21.61%	lr:0.000985
Ep: 7/25	It: 2351/4130	batch_loss: 4.5558	batch_accuracy: 23.80%	lr:0.000985
Ep: 7/25	It: 2401/4130	batch_loss: 4.6085	batch_accuracy: 22.68%	lr:0.000985
Ep: 7/25	It: 2451/4130	batch_loss: 4.6227	batch_accuracy: 23.12%	lr:0.000985
Ep: 7/25	It: 2501/4130	batch_loss: 4.5705	batch_accuracy: 23.46%	lr:0.000984
Ep: 7/25	It: 2551/4130	batch_loss: 4.5177	batch_accuracy: 23.95%	lr:0.000984
Ep: 7/25	It: 2601/4130	batch_loss: 4.7218	batch_accuracy: 20.90%	lr:0.000984
Ep: 7/25	It: 2651/4130	batch_loss: 4.5707	batch_accuracy: 22.34%	lr:0.000984
Ep: 7/25	It: 2701/4130	batch_loss: 4.5466	batch_accuracy: 23.22%	lr:0.000983
Ep: 7/25	It: 2751/4130	batch_loss: 4.5561	batch_accuracy: 23.27%	lr:0.000983
Ep: 7/25	It: 2801/4130	batch_loss: 4.6493	batch_accuracy: 22.05%	lr:0.000983
Ep: 7/25	It: 2851/4130	batch_loss: 4.5929	batch_accuracy: 23.14%	lr:0.000983
Ep: 7/25	It: 2901/4130	batch_loss: 4.5851	batch_accuracy: 23.61%	lr:0.000982
Ep: 7/25	It: 2951/4130	batch_loss: 4.6178	batch_accuracy: 22.31%	lr:0.000982
Ep: 7/25	It: 3001/4130	batch_loss: 4.6525	batch_accuracy: 22.02%	lr:0.000982
Ep: 7/25	It: 3051/4130	batch_loss: 4.6212	batch_accuracy: 23.34%	lr:0.000982
Ep: 7/25	It: 3101/4130	batch_loss: 4.5673	batch_accuracy: 23.51%	lr:0.000981
Ep: 7/25	It: 3151/4130	batch_loss: 4.6211	batch_accuracy: 22.58%	lr:0.000981
Ep: 7/25	It: 3201/4130	batch_loss: 4.6919	batch_accuracy: 20.17%	lr:0.000981
Ep: 7/25	It: 3251/4130	batch_loss: 4.5991	batch_accuracy: 22.75%	lr:0.000981
Ep: 7/25	It: 3301/4130	batch_loss: 4.5222	batch_accuracy: 23.19%	lr:0.000980
Ep: 7/25	It: 3351/4130	batch_loss: 4.5634	batch_accuracy: 22.58%	lr:0.000980
Ep: 7/25	It: 3401/4130	batch_loss: 4.5809	batch_accuracy: 23.41%	lr:0.000980
Ep: 7/25	It: 3451/4130	batch_loss: 4.6004	batch_accuracy: 23.29%	lr:0.000980
Ep: 7/25	It: 3501/4130	batch_loss: 4.6901	batch_accuracy: 22.49%	lr:0.000979
Ep: 7/25	It: 3551/4130	batch_loss: 4.5610	batch_accuracy: 23.02%	lr:0.000979
Ep: 7/25	It: 3601/4130	batch_loss: 4.5433	batch_accuracy: 23.05%	lr:0.000979
Ep: 7/25	It: 3651/4130	batch_loss: 4.5401	batch_accuracy: 23.90%	lr:0.000978
Ep: 7/25	It: 3701/4130	batch_loss: 4.7157	batch_accuracy: 21.80%	lr:0.000978
Ep: 7/25	It: 3751/4130	batch_loss: 4.5860	batch_accuracy: 22.49%	lr:0.000978
Ep: 7/25	It: 3801/4130	batch_loss: 4.5987	batch_accuracy: 23.05%	lr:0.000978
Ep: 7/25	It: 3851/4130	batch_loss: 4.6168	batch_accuracy: 22.63%	lr:0.000977
Ep: 7/25	It: 3901/4130	batch_loss: 4.5883	batch_accuracy: 22.34%	lr:0.000977
Ep: 7/25	It: 3951/4130	batch_loss: 4.7695	batch_accuracy: 21.02%	lr:0.000977
Ep: 7/25	It: 4001/4130	batch_loss: 4.5678	batch_accuracy: 23.05%	lr:0.000977
Ep: 7/25	It: 4051/4130	batch_loss: 4.6670	batch_accuracy: 22.49%	lr:0.000976
Ep: 7/25	It: 4101/4130	batch_loss: 4.5639	batch_accuracy: 23.49%	lr:0.000976
Ep: 7/25	It: 4130/4130	batch_loss: 4.6427	batch_accuracy: 22.19%	lr:0.000976


Generated text for input text "You" is:
You’s in the United States (the C. 2014). The authors have been published in order to evaluate and compare the main aspects of these methods and the main goals of our study.

MICAL AND METHODS
The study was conducted by the Elder officery inference. The authors identified a retrospective study, in the study of the literature of the study of the two groups. The results indicate that the use of a case of three different approaches and the analysis of the analysis was conducted in the UK. We performed a questionnaire with respect to the use of a total of the surveyed questionnaires. Results showed that the data indicate that this method is not used to identify these methods. A case study is also presented in terms of the impact of the analysis of the students. We also show that the results of the study will lead to the quality of the proposed approach, while in order to be obtained in the first experiment and the use of the method, and is presented to show that the method of a new model of a high accuracy. The method is also presented and the best accuracy of the method based on the data, based on a new method of the method is carried out for the method.
<eot>
<sot>


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 8/25	It: 1/4130	batch_loss: 4.6051	batch_accuracy: 23.29%	lr:0.000976
Ep: 8/25	It: 51/4130	batch_loss: 4.6086	batch_accuracy: 22.02%	lr:0.000975
Ep: 8/25	It: 101/4130	batch_loss: 4.5733	batch_accuracy: 22.83%	lr:0.000975
Ep: 8/25	It: 151/4130	batch_loss: 4.4683	batch_accuracy: 23.83%	lr:0.000975
Ep: 8/25	It: 201/4130	batch_loss: 4.5578	batch_accuracy: 22.44%	lr:0.000975
Ep: 8/25	It: 251/4130	batch_loss: 4.7289	batch_accuracy: 20.43%	lr:0.000974
Ep: 8/25	It: 301/4130	batch_loss: 4.5709	batch_accuracy: 23.02%	lr:0.000974
Ep: 8/25	It: 351/4130	batch_loss: 4.6587	batch_accuracy: 22.17%	lr:0.000974
Ep: 8/25	It: 401/4130	batch_loss: 4.4787	batch_accuracy: 23.90%	lr:0.000973
Ep: 8/25	It: 451/4130	batch_loss: 4.5707	batch_accuracy: 22.78%	lr:0.000973
Ep: 8/25	It: 501/4130	batch_loss: 4.5796	batch_accuracy: 23.51%	lr:0.000973
Ep: 8/25	It: 551/4130	batch_loss: 4.4852	batch_accuracy: 23.41%	lr:0.000972
Ep: 8/25	It: 601/4130	batch_loss: 4.5631	batch_accuracy: 22.83%	lr:0.000972
Ep: 8/25	It: 651/4130	batch_loss: 4.6493	batch_accuracy: 22.39%	lr:0.000972
Ep: 8/25	It: 701/4130	batch_loss: 4.5700	batch_accuracy: 22.88%	lr:0.000972
Ep: 8/25	It: 751/4130	batch_loss: 4.6005	batch_accuracy: 23.71%	lr:0.000971
Ep: 8/25	It: 801/4130	batch_loss: 4.5825	batch_accuracy: 22.88%	lr:0.000971
Ep: 8/25	It: 851/4130	batch_loss: 4.6443	batch_accuracy: 22.34%	lr:0.000971
Ep: 8/25	It: 901/4130	batch_loss: 4.5641	batch_accuracy: 22.88%	lr:0.000970
Ep: 8/25	It: 951/4130	batch_loss: 4.5348	batch_accuracy: 24.02%	lr:0.000970
Ep: 8/25	It: 1001/4130	batch_loss: 4.5598	batch_accuracy: 23.05%	lr:0.000970
Ep: 8/25	It: 1051/4130	batch_loss: 4.6445	batch_accuracy: 21.66%	lr:0.000969
Ep: 8/25	It: 1101/4130	batch_loss: 4.6355	batch_accuracy: 22.92%	lr:0.000969
Ep: 8/25	It: 1151/4130	batch_loss: 4.6138	batch_accuracy: 22.53%	lr:0.000969
Ep: 8/25	It: 1201/4130	batch_loss: 4.4830	batch_accuracy: 23.90%	lr:0.000968
Ep: 8/25	It: 1251/4130	batch_loss: 4.6609	batch_accuracy: 22.24%	lr:0.000968
Ep: 8/25	It: 1301/4130	batch_loss: 4.5619	batch_accuracy: 22.97%	lr:0.000968
Ep: 8/25	It: 1351/4130	batch_loss: 4.6428	batch_accuracy: 22.71%	lr:0.000967
Ep: 8/25	It: 1401/4130	batch_loss: 4.4768	batch_accuracy: 24.24%	lr:0.000967
Ep: 8/25	It: 1451/4130	batch_loss: 4.5007	batch_accuracy: 23.51%	lr:0.000967
Ep: 8/25	It: 1501/4130	batch_loss: 4.6331	batch_accuracy: 21.44%	lr:0.000966
Ep: 8/25	It: 1551/4130	batch_loss: 4.4886	batch_accuracy: 24.24%	lr:0.000966
Ep: 8/25	It: 1601/4130	batch_loss: 4.5945	batch_accuracy: 22.34%	lr:0.000966
Ep: 8/25	It: 1651/4130	batch_loss: 4.5880	batch_accuracy: 21.66%	lr:0.000965
Ep: 8/25	It: 1701/4130	batch_loss: 4.4851	batch_accuracy: 24.88%	lr:0.000965
Ep: 8/25	It: 1751/4130	batch_loss: 4.4977	batch_accuracy: 23.58%	lr:0.000965
Ep: 8/25	It: 1801/4130	batch_loss: 4.6586	batch_accuracy: 22.63%	lr:0.000964
Ep: 8/25	It: 1851/4130	batch_loss: 4.5727	batch_accuracy: 22.88%	lr:0.000964
Ep: 8/25	It: 1901/4130	batch_loss: 4.3820	batch_accuracy: 25.83%	lr:0.000963
Ep: 8/25	It: 1951/4130	batch_loss: 4.6559	batch_accuracy: 21.95%	lr:0.000963
Ep: 8/25	It: 2001/4130	batch_loss: 4.4912	batch_accuracy: 24.51%	lr:0.000963
Ep: 8/25	It: 2051/4130	batch_loss: 4.4938	batch_accuracy: 23.78%	lr:0.000962
Ep: 8/25	It: 2101/4130	batch_loss: 4.5206	batch_accuracy: 23.63%	lr:0.000962
Ep: 8/25	It: 2151/4130	batch_loss: 4.4365	batch_accuracy: 23.39%	lr:0.000962
Ep: 8/25	It: 2201/4130	batch_loss: 4.6203	batch_accuracy: 21.73%	lr:0.000961
Ep: 8/25	It: 2251/4130	batch_loss: 4.5554	batch_accuracy: 23.32%	lr:0.000961
Ep: 8/25	It: 2301/4130	batch_loss: 4.6351	batch_accuracy: 21.95%	lr:0.000961
Ep: 8/25	It: 2351/4130	batch_loss: 4.4635	batch_accuracy: 23.75%	lr:0.000960
Ep: 8/25	It: 2401/4130	batch_loss: 4.4981	batch_accuracy: 23.39%	lr:0.000960
Ep: 8/25	It: 2451/4130	batch_loss: 4.5482	batch_accuracy: 23.46%	lr:0.000959
Ep: 8/25	It: 2501/4130	batch_loss: 4.5677	batch_accuracy: 22.49%	lr:0.000959
Ep: 8/25	It: 2551/4130	batch_loss: 4.4334	batch_accuracy: 24.05%	lr:0.000959
Ep: 8/25	It: 2601/4130	batch_loss: 4.5361	batch_accuracy: 23.78%	lr:0.000958
Ep: 8/25	It: 2651/4130	batch_loss: 4.5048	batch_accuracy: 23.68%	lr:0.000958
Ep: 8/25	It: 2701/4130	batch_loss: 4.6113	batch_accuracy: 21.63%	lr:0.000958
Ep: 8/25	It: 2751/4130	batch_loss: 4.4622	batch_accuracy: 24.17%	lr:0.000957
Ep: 8/25	It: 2801/4130	batch_loss: 4.7074	batch_accuracy: 21.75%	lr:0.000957
Ep: 8/25	It: 2851/4130	batch_loss: 4.5006	batch_accuracy: 23.80%	lr:0.000956
Ep: 8/25	It: 2901/4130	batch_loss: 4.5309	batch_accuracy: 23.58%	lr:0.000956
Ep: 8/25	It: 2951/4130	batch_loss: 4.7091	batch_accuracy: 21.83%	lr:0.000956
Ep: 8/25	It: 3001/4130	batch_loss: 4.6584	batch_accuracy: 22.92%	lr:0.000955
Ep: 8/25	It: 3051/4130	batch_loss: 4.6220	batch_accuracy: 22.22%	lr:0.000955
Ep: 8/25	It: 3101/4130	batch_loss: 4.5149	batch_accuracy: 23.68%	lr:0.000954
Ep: 8/25	It: 3151/4130	batch_loss: 4.6559	batch_accuracy: 22.31%	lr:0.000954
Ep: 8/25	It: 3201/4130	batch_loss: 4.5407	batch_accuracy: 21.97%	lr:0.000954
Ep: 8/25	It: 3251/4130	batch_loss: 4.4638	batch_accuracy: 24.07%	lr:0.000953
Ep: 8/25	It: 3301/4130	batch_loss: 4.4249	batch_accuracy: 24.68%	lr:0.000953
Ep: 8/25	It: 3351/4130	batch_loss: 4.6884	batch_accuracy: 22.14%	lr:0.000953
Ep: 8/25	It: 3401/4130	batch_loss: 4.6242	batch_accuracy: 23.39%	lr:0.000952
Ep: 8/25	It: 3451/4130	batch_loss: 4.6076	batch_accuracy: 23.56%	lr:0.000952
Ep: 8/25	It: 3501/4130	batch_loss: 4.5119	batch_accuracy: 23.51%	lr:0.000951
Ep: 8/25	It: 3551/4130	batch_loss: 4.6049	batch_accuracy: 22.17%	lr:0.000951
Ep: 8/25	It: 3601/4130	batch_loss: 4.6163	batch_accuracy: 21.88%	lr:0.000950
Ep: 8/25	It: 3651/4130	batch_loss: 4.5394	batch_accuracy: 22.97%	lr:0.000950
Ep: 8/25	It: 3701/4130	batch_loss: 4.5594	batch_accuracy: 21.26%	lr:0.000950
Ep: 8/25	It: 3751/4130	batch_loss: 4.6045	batch_accuracy: 23.12%	lr:0.000949
Ep: 8/25	It: 3801/4130	batch_loss: 4.7151	batch_accuracy: 22.46%	lr:0.000949
Ep: 8/25	It: 3851/4130	batch_loss: 4.4769	batch_accuracy: 23.90%	lr:0.000948
Ep: 8/25	It: 3901/4130	batch_loss: 4.5881	batch_accuracy: 22.83%	lr:0.000948
Ep: 8/25	It: 3951/4130	batch_loss: 4.6519	batch_accuracy: 23.54%	lr:0.000948
Ep: 8/25	It: 4001/4130	batch_loss: 4.3875	batch_accuracy: 26.00%	lr:0.000947
Ep: 8/25	It: 4051/4130	batch_loss: 4.5446	batch_accuracy: 23.39%	lr:0.000947
Ep: 8/25	It: 4101/4130	batch_loss: 4.4281	batch_accuracy: 24.44%	lr:0.000946
Ep: 8/25	It: 4130/4130	batch_loss: 4.5694	batch_accuracy: 22.03%	lr:0.000946


Generated text for input text "You" is:
Youvie and have been identified in the first case of the BDs of the CLPO3 (M) of the MMP. The PGA is a promising to be used for the EEG-FI-F-F and the LM.

RESULTS
The results were found to be a positive for both the CPF. The results show that the PEA and NPK-A, with no significant differences in the number of SNR2 and C. papillin. The results indicate that the results of the proposed approach is more accurate than the other handlings in the same time.
<eot>
<sot>
Prevalence of Apperature-Mulnet in Chemodedia: An Improvement of the Elderly

Background: The aim of this study was to assess the effect of Tama-Pr. The Symptomatic Association in Brazil, a study of Southern Prials

In this paper, we present a comparative study of this study, to study the role of a review of the clinical practice, and that the authors are the first of the study of the use of


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 9/25	It: 1/4130	batch_loss: 4.5258	batch_accuracy: 22.88%	lr:0.000946
Ep: 9/25	It: 51/4130	batch_loss: 4.4768	batch_accuracy: 24.22%	lr:0.000946
Ep: 9/25	It: 101/4130	batch_loss: 4.5255	batch_accuracy: 23.49%	lr:0.000945
Ep: 9/25	It: 151/4130	batch_loss: 4.4550	batch_accuracy: 24.46%	lr:0.000945
Ep: 9/25	It: 201/4130	batch_loss: 4.5993	batch_accuracy: 22.49%	lr:0.000944
Ep: 9/25	It: 251/4130	batch_loss: 4.6082	batch_accuracy: 23.12%	lr:0.000944
Ep: 9/25	It: 301/4130	batch_loss: 4.5595	batch_accuracy: 22.63%	lr:0.000943
Ep: 9/25	It: 351/4130	batch_loss: 4.5353	batch_accuracy: 23.19%	lr:0.000943
Ep: 9/25	It: 401/4130	batch_loss: 4.4897	batch_accuracy: 22.75%	lr:0.000943
Ep: 9/25	It: 451/4130	batch_loss: 4.5454	batch_accuracy: 23.97%	lr:0.000942
Ep: 9/25	It: 501/4130	batch_loss: 4.5373	batch_accuracy: 23.05%	lr:0.000942
Ep: 9/25	It: 551/4130	batch_loss: 4.5699	batch_accuracy: 22.83%	lr:0.000941
Ep: 9/25	It: 601/4130	batch_loss: 4.5503	batch_accuracy: 22.09%	lr:0.000941
Ep: 9/25	It: 651/4130	batch_loss: 4.6362	batch_accuracy: 23.22%	lr:0.000940
Ep: 9/25	It: 701/4130	batch_loss: 4.6044	batch_accuracy: 22.71%	lr:0.000940
Ep: 9/25	It: 751/4130	batch_loss: 4.6207	batch_accuracy: 22.61%	lr:0.000939
Ep: 9/25	It: 801/4130	batch_loss: 4.5648	batch_accuracy: 22.63%	lr:0.000939
Ep: 9/25	It: 851/4130	batch_loss: 4.4966	batch_accuracy: 23.73%	lr:0.000939
Ep: 9/25	It: 901/4130	batch_loss: 4.4868	batch_accuracy: 24.73%	lr:0.000938
Ep: 9/25	It: 951/4130	batch_loss: 4.4359	batch_accuracy: 24.95%	lr:0.000938
Ep: 9/25	It: 1001/4130	batch_loss: 4.5181	batch_accuracy: 24.34%	lr:0.000937
Ep: 9/25	It: 1051/4130	batch_loss: 4.5704	batch_accuracy: 23.32%	lr:0.000937
Ep: 9/25	It: 1101/4130	batch_loss: 4.7442	batch_accuracy: 21.53%	lr:0.000936
Ep: 9/25	It: 1151/4130	batch_loss: 4.4974	batch_accuracy: 23.07%	lr:0.000936
Ep: 9/25	It: 1201/4130	batch_loss: 4.5150	batch_accuracy: 23.19%	lr:0.000935
Ep: 9/25	It: 1251/4130	batch_loss: 4.5456	batch_accuracy: 24.07%	lr:0.000935
Ep: 9/25	It: 1301/4130	batch_loss: 4.5685	batch_accuracy: 22.83%	lr:0.000934
Ep: 9/25	It: 1351/4130	batch_loss: 4.4676	batch_accuracy: 23.93%	lr:0.000934
Ep: 9/25	It: 1401/4130	batch_loss: 4.5430	batch_accuracy: 22.58%	lr:0.000933
Ep: 9/25	It: 1451/4130	batch_loss: 4.5378	batch_accuracy: 23.66%	lr:0.000933
Ep: 9/25	It: 1501/4130	batch_loss: 4.5336	batch_accuracy: 23.63%	lr:0.000932
Ep: 9/25	It: 1551/4130	batch_loss: 4.5743	batch_accuracy: 23.73%	lr:0.000932
Ep: 9/25	It: 1601/4130	batch_loss: 4.4955	batch_accuracy: 23.41%	lr:0.000932
Ep: 9/25	It: 1651/4130	batch_loss: 4.4507	batch_accuracy: 24.83%	lr:0.000931
Ep: 9/25	It: 1701/4130	batch_loss: 4.4971	batch_accuracy: 24.88%	lr:0.000931
Ep: 9/25	It: 1751/4130	batch_loss: 4.6767	batch_accuracy: 21.95%	lr:0.000930
Ep: 9/25	It: 1801/4130	batch_loss: 4.4555	batch_accuracy: 24.02%	lr:0.000930
Ep: 9/25	It: 1851/4130	batch_loss: 4.5533	batch_accuracy: 23.44%	lr:0.000929
Ep: 9/25	It: 1901/4130	batch_loss: 4.6269	batch_accuracy: 22.09%	lr:0.000929
Ep: 9/25	It: 1951/4130	batch_loss: 4.5690	batch_accuracy: 22.90%	lr:0.000928
Ep: 9/25	It: 2001/4130	batch_loss: 4.6446	batch_accuracy: 21.75%	lr:0.000928
Ep: 9/25	It: 2051/4130	batch_loss: 4.5618	batch_accuracy: 22.51%	lr:0.000927
Ep: 9/25	It: 2101/4130	batch_loss: 4.5025	batch_accuracy: 24.00%	lr:0.000927
Ep: 9/25	It: 2151/4130	batch_loss: 4.5766	batch_accuracy: 22.75%	lr:0.000926
Ep: 9/25	It: 2201/4130	batch_loss: 4.5545	batch_accuracy: 23.19%	lr:0.000926
Ep: 9/25	It: 2251/4130	batch_loss: 4.5380	batch_accuracy: 23.71%	lr:0.000925
Ep: 9/25	It: 2301/4130	batch_loss: 4.4745	batch_accuracy: 23.90%	lr:0.000925
Ep: 9/25	It: 2351/4130	batch_loss: 4.5117	batch_accuracy: 24.73%	lr:0.000924
Ep: 9/25	It: 2401/4130	batch_loss: 4.4879	batch_accuracy: 23.51%	lr:0.000924
Ep: 9/25	It: 2451/4130	batch_loss: 4.5159	batch_accuracy: 24.27%	lr:0.000923
Ep: 9/25	It: 2501/4130	batch_loss: 4.4170	batch_accuracy: 24.61%	lr:0.000923
Ep: 9/25	It: 2551/4130	batch_loss: 4.5328	batch_accuracy: 24.27%	lr:0.000922
Ep: 9/25	It: 2601/4130	batch_loss: 4.6109	batch_accuracy: 22.44%	lr:0.000922
Ep: 9/25	It: 2651/4130	batch_loss: 4.5858	batch_accuracy: 23.34%	lr:0.000921
Ep: 9/25	It: 2701/4130	batch_loss: 4.5690	batch_accuracy: 22.90%	lr:0.000921
Ep: 9/25	It: 2751/4130	batch_loss: 4.4763	batch_accuracy: 23.95%	lr:0.000920
Ep: 9/25	It: 2801/4130	batch_loss: 4.4924	batch_accuracy: 24.22%	lr:0.000920
Ep: 9/25	It: 2851/4130	batch_loss: 4.5460	batch_accuracy: 23.54%	lr:0.000919
Ep: 9/25	It: 2901/4130	batch_loss: 4.6236	batch_accuracy: 22.95%	lr:0.000919
Ep: 9/25	It: 2951/4130	batch_loss: 4.4831	batch_accuracy: 23.83%	lr:0.000918
Ep: 9/25	It: 3001/4130	batch_loss: 4.4848	batch_accuracy: 23.36%	lr:0.000918
Ep: 9/25	It: 3051/4130	batch_loss: 4.6292	batch_accuracy: 22.63%	lr:0.000917
Ep: 9/25	It: 3101/4130	batch_loss: 4.6230	batch_accuracy: 22.19%	lr:0.000917
Ep: 9/25	It: 3151/4130	batch_loss: 4.5170	batch_accuracy: 23.66%	lr:0.000916
Ep: 9/25	It: 3201/4130	batch_loss: 4.4576	batch_accuracy: 23.97%	lr:0.000915
Ep: 9/25	It: 3251/4130	batch_loss: 4.4896	batch_accuracy: 23.93%	lr:0.000915
Ep: 9/25	It: 3301/4130	batch_loss: 4.4857	batch_accuracy: 24.15%	lr:0.000914
Ep: 9/25	It: 3351/4130	batch_loss: 4.4476	batch_accuracy: 24.02%	lr:0.000914
Ep: 9/25	It: 3401/4130	batch_loss: 4.4184	batch_accuracy: 25.12%	lr:0.000913
Ep: 9/25	It: 3451/4130	batch_loss: 4.4085	batch_accuracy: 24.41%	lr:0.000913
Ep: 9/25	It: 3501/4130	batch_loss: 4.5268	batch_accuracy: 23.27%	lr:0.000912
Ep: 9/25	It: 3551/4130	batch_loss: 4.4730	batch_accuracy: 24.54%	lr:0.000912
Ep: 9/25	It: 3601/4130	batch_loss: 4.7012	batch_accuracy: 21.26%	lr:0.000911
Ep: 9/25	It: 3651/4130	batch_loss: 4.5087	batch_accuracy: 24.07%	lr:0.000911
Ep: 9/25	It: 3701/4130	batch_loss: 4.5792	batch_accuracy: 22.36%	lr:0.000910
Ep: 9/25	It: 3751/4130	batch_loss: 4.4937	batch_accuracy: 24.39%	lr:0.000910
Ep: 9/25	It: 3801/4130	batch_loss: 4.6088	batch_accuracy: 22.83%	lr:0.000909
Ep: 9/25	It: 3851/4130	batch_loss: 4.5699	batch_accuracy: 22.75%	lr:0.000909
Ep: 9/25	It: 3901/4130	batch_loss: 4.6134	batch_accuracy: 22.00%	lr:0.000908
Ep: 9/25	It: 3951/4130	batch_loss: 4.5695	batch_accuracy: 22.78%	lr:0.000907
Ep: 9/25	It: 4001/4130	batch_loss: 4.5473	batch_accuracy: 23.95%	lr:0.000907
Ep: 9/25	It: 4051/4130	batch_loss: 4.5549	batch_accuracy: 22.34%	lr:0.000906
Ep: 9/25	It: 4101/4130	batch_loss: 4.5333	batch_accuracy: 23.12%	lr:0.000906
Ep: 9/25	It: 4130/4130	batch_loss: 4.5238	batch_accuracy: 23.63%	lr:0.000905


Generated text for input text "You" is:
Youi, and the Fuis, were not only as a more frequent in the world. AA has notably, and is associated withdr.

CONCLUSION
There was thesis of these patients (9%) had no difference between two groups. There was a significant difference between the patient who had note and the patient with the disease (P < 0.001). The median of age in the age was significantly greater in all patients, with less severe adverse morbidity. The mean duration of mortality was increased in group 2 (range, 0.5 mm (95% CI 0.88 and 5.0%) in all patients (1.3%) patients, (1.8%) and non-sided patients were significantly associated with HV-1, whereas patients with HBV-1 had high sensitivity (P<0.001). In the study, the mean age of treatment, 51% of the patients had no longer than 54% of patients with CVR-treated with AMV-induced mortality. Conclusions: Our results indicate that the effect of a higher incidence of the total number of patients with PP1 and S-cell lymphatic diseases, and the treatment of the lungs were treated


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 10/25	It: 1/4130	batch_loss: 4.3961	batch_accuracy: 25.56%	lr:0.000905
Ep: 10/25	It: 51/4130	batch_loss: 4.3199	batch_accuracy: 25.32%	lr:0.000905
Ep: 10/25	It: 101/4130	batch_loss: 4.4170	batch_accuracy: 23.78%	lr:0.000904
Ep: 10/25	It: 151/4130	batch_loss: 4.4324	batch_accuracy: 25.68%	lr:0.000904
Ep: 10/25	It: 201/4130	batch_loss: 4.6261	batch_accuracy: 21.22%	lr:0.000903
Ep: 10/25	It: 251/4130	batch_loss: 4.5633	batch_accuracy: 22.09%	lr:0.000903
Ep: 10/25	It: 301/4130	batch_loss: 4.7068	batch_accuracy: 21.44%	lr:0.000902
Ep: 10/25	It: 351/4130	batch_loss: 4.3848	batch_accuracy: 25.20%	lr:0.000902
Ep: 10/25	It: 401/4130	batch_loss: 4.5918	batch_accuracy: 23.61%	lr:0.000901
Ep: 10/25	It: 451/4130	batch_loss: 4.5385	batch_accuracy: 22.78%	lr:0.000900
Ep: 10/25	It: 501/4130	batch_loss: 4.4860	batch_accuracy: 22.53%	lr:0.000900
Ep: 10/25	It: 551/4130	batch_loss: 4.5655	batch_accuracy: 22.85%	lr:0.000899
Ep: 10/25	It: 601/4130	batch_loss: 4.5657	batch_accuracy: 22.56%	lr:0.000899
Ep: 10/25	It: 651/4130	batch_loss: 4.4695	batch_accuracy: 23.34%	lr:0.000898
Ep: 10/25	It: 701/4130	batch_loss: 4.6075	batch_accuracy: 23.14%	lr:0.000898
Ep: 10/25	It: 751/4130	batch_loss: 4.5465	batch_accuracy: 23.32%	lr:0.000897
Ep: 10/25	It: 801/4130	batch_loss: 4.5902	batch_accuracy: 22.24%	lr:0.000896
Ep: 10/25	It: 851/4130	batch_loss: 4.5152	batch_accuracy: 24.10%	lr:0.000896
Ep: 10/25	It: 901/4130	batch_loss: 4.4588	batch_accuracy: 24.10%	lr:0.000895
Ep: 10/25	It: 951/4130	batch_loss: 4.4605	batch_accuracy: 23.95%	lr:0.000895
Ep: 10/25	It: 1001/4130	batch_loss: 4.5300	batch_accuracy: 22.97%	lr:0.000894
Ep: 10/25	It: 1051/4130	batch_loss: 4.4722	batch_accuracy: 23.88%	lr:0.000894
Ep: 10/25	It: 1101/4130	batch_loss: 4.4762	batch_accuracy: 24.17%	lr:0.000893
Ep: 10/25	It: 1151/4130	batch_loss: 4.4077	batch_accuracy: 24.78%	lr:0.000892
Ep: 10/25	It: 1201/4130	batch_loss: 4.5192	batch_accuracy: 22.78%	lr:0.000892
Ep: 10/25	It: 1251/4130	batch_loss: 4.5509	batch_accuracy: 23.22%	lr:0.000891
Ep: 10/25	It: 1301/4130	batch_loss: 4.5071	batch_accuracy: 23.44%	lr:0.000891
Ep: 10/25	It: 1351/4130	batch_loss: 4.4005	batch_accuracy: 25.22%	lr:0.000890
Ep: 10/25	It: 1401/4130	batch_loss: 4.3520	batch_accuracy: 25.76%	lr:0.000889
Ep: 10/25	It: 1451/4130	batch_loss: 4.4573	batch_accuracy: 23.90%	lr:0.000889
Ep: 10/25	It: 1501/4130	batch_loss: 4.4784	batch_accuracy: 24.80%	lr:0.000888
Ep: 10/25	It: 1551/4130	batch_loss: 4.5496	batch_accuracy: 23.34%	lr:0.000888
Ep: 10/25	It: 1601/4130	batch_loss: 4.5210	batch_accuracy: 24.22%	lr:0.000887
Ep: 10/25	It: 1651/4130	batch_loss: 4.3817	batch_accuracy: 25.78%	lr:0.000886
Ep: 10/25	It: 1701/4130	batch_loss: 4.4155	batch_accuracy: 25.51%	lr:0.000886
Ep: 10/25	It: 1751/4130	batch_loss: 4.4927	batch_accuracy: 23.12%	lr:0.000885
Ep: 10/25	It: 1801/4130	batch_loss: 4.5718	batch_accuracy: 23.07%	lr:0.000885
Ep: 10/25	It: 1851/4130	batch_loss: 4.4488	batch_accuracy: 24.34%	lr:0.000884
Ep: 10/25	It: 1901/4130	batch_loss: 4.4664	batch_accuracy: 24.39%	lr:0.000883
Ep: 10/25	It: 1951/4130	batch_loss: 4.5504	batch_accuracy: 23.22%	lr:0.000883
Ep: 10/25	It: 2001/4130	batch_loss: 4.5697	batch_accuracy: 22.17%	lr:0.000882
Ep: 10/25	It: 2051/4130	batch_loss: 4.5329	batch_accuracy: 23.10%	lr:0.000882
Ep: 10/25	It: 2101/4130	batch_loss: 4.5023	batch_accuracy: 23.54%	lr:0.000881
Ep: 10/25	It: 2151/4130	batch_loss: 4.4511	batch_accuracy: 24.46%	lr:0.000880
Ep: 10/25	It: 2201/4130	batch_loss: 4.5353	batch_accuracy: 23.51%	lr:0.000880
Ep: 10/25	It: 2251/4130	batch_loss: 4.5239	batch_accuracy: 23.41%	lr:0.000879
Ep: 10/25	It: 2301/4130	batch_loss: 4.5763	batch_accuracy: 22.75%	lr:0.000878
Ep: 10/25	It: 2351/4130	batch_loss: 4.5594	batch_accuracy: 23.80%	lr:0.000878
Ep: 10/25	It: 2401/4130	batch_loss: 4.4034	batch_accuracy: 24.12%	lr:0.000877
Ep: 10/25	It: 2451/4130	batch_loss: 4.5711	batch_accuracy: 22.90%	lr:0.000877
Ep: 10/25	It: 2501/4130	batch_loss: 4.5248	batch_accuracy: 22.73%	lr:0.000876
Ep: 10/25	It: 2551/4130	batch_loss: 4.5641	batch_accuracy: 22.92%	lr:0.000875
Ep: 10/25	It: 2601/4130	batch_loss: 4.4072	batch_accuracy: 24.83%	lr:0.000875
Ep: 10/25	It: 2651/4130	batch_loss: 4.4580	batch_accuracy: 22.71%	lr:0.000874
Ep: 10/25	It: 2701/4130	batch_loss: 4.4321	batch_accuracy: 24.29%	lr:0.000874
Ep: 10/25	It: 2751/4130	batch_loss: 4.6657	batch_accuracy: 21.73%	lr:0.000873
Ep: 10/25	It: 2801/4130	batch_loss: 4.5583	batch_accuracy: 22.92%	lr:0.000872
Ep: 10/25	It: 2851/4130	batch_loss: 4.4866	batch_accuracy: 23.95%	lr:0.000872
Ep: 10/25	It: 2901/4130	batch_loss: 4.5491	batch_accuracy: 23.36%	lr:0.000871
Ep: 10/25	It: 2951/4130	batch_loss: 4.3919	batch_accuracy: 25.17%	lr:0.000870
Ep: 10/25	It: 3001/4130	batch_loss: 4.4856	batch_accuracy: 25.15%	lr:0.000870
Ep: 10/25	It: 3051/4130	batch_loss: 4.5207	batch_accuracy: 22.61%	lr:0.000869
Ep: 10/25	It: 3101/4130	batch_loss: 4.4139	batch_accuracy: 25.10%	lr:0.000868
Ep: 10/25	It: 3151/4130	batch_loss: 4.5813	batch_accuracy: 22.09%	lr:0.000868
Ep: 10/25	It: 3201/4130	batch_loss: 4.4899	batch_accuracy: 22.88%	lr:0.000867
Ep: 10/25	It: 3251/4130	batch_loss: 4.4158	batch_accuracy: 25.10%	lr:0.000867
Ep: 10/25	It: 3301/4130	batch_loss: 4.4822	batch_accuracy: 23.27%	lr:0.000866
Ep: 10/25	It: 3351/4130	batch_loss: 4.5568	batch_accuracy: 22.24%	lr:0.000865
Ep: 10/25	It: 3401/4130	batch_loss: 4.5692	batch_accuracy: 23.24%	lr:0.000865
Ep: 10/25	It: 3451/4130	batch_loss: 4.5089	batch_accuracy: 23.29%	lr:0.000864
Ep: 10/25	It: 3501/4130	batch_loss: 4.4330	batch_accuracy: 24.85%	lr:0.000863
Ep: 10/25	It: 3551/4130	batch_loss: 4.4328	batch_accuracy: 25.44%	lr:0.000863
Ep: 10/25	It: 3601/4130	batch_loss: 4.4262	batch_accuracy: 24.37%	lr:0.000862
Ep: 10/25	It: 3651/4130	batch_loss: 4.5605	batch_accuracy: 22.44%	lr:0.000861
Ep: 10/25	It: 3701/4130	batch_loss: 4.4074	batch_accuracy: 25.07%	lr:0.000861
Ep: 10/25	It: 3751/4130	batch_loss: 4.5397	batch_accuracy: 24.02%	lr:0.000860
Ep: 10/25	It: 3801/4130	batch_loss: 4.5303	batch_accuracy: 24.56%	lr:0.000859
Ep: 10/25	It: 3851/4130	batch_loss: 4.4127	batch_accuracy: 24.46%	lr:0.000859
Ep: 10/25	It: 3901/4130	batch_loss: 4.4220	batch_accuracy: 24.80%	lr:0.000858
Ep: 10/25	It: 3951/4130	batch_loss: 4.5738	batch_accuracy: 23.00%	lr:0.000857
Ep: 10/25	It: 4001/4130	batch_loss: 4.4060	batch_accuracy: 24.78%	lr:0.000857
Ep: 10/25	It: 4051/4130	batch_loss: 4.4600	batch_accuracy: 24.02%	lr:0.000856
Ep: 10/25	It: 4101/4130	batch_loss: 4.5517	batch_accuracy: 23.22%	lr:0.000855
Ep: 10/25	It: 4130/4130	batch_loss: 4.4626	batch_accuracy: 24.49%	lr:0.000855


Generated text for input text "You" is:
Youe.
FIs were studied, and analyzed inver (p. 3) were treated with a median survival rate (3%) and 2 (95% CI: 5.9%) were found in the control group (5%) and 4.

RESULTS
CONCLUSION
A cohort was 8% for 1.7 years (95% CI, 0.321) (83%) (8.6%), the median survival of the patients with HV (1.4%) and 4.5% of cases (0.5%) had no significant improvement in the group. In addition, the incidence of PH-related and HCC patients was not statistically significant. A significant association with a mean age interval was 7.5 (1.0±3.3) (95% CI 0.7 to 2.94, P < 0.001) than that in the 2.5-due and 0.1, respectively. There was a significant difference in the total population of the patients with BPKA with the mean P = 0.01 and 0.58) of the PP was significantly lower than that of the 5-month age. Conclusions: There was no significant differences in survival of the


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 11/25	It: 1/4130	batch_loss: 4.4792	batch_accuracy: 24.71%	lr:0.000855
Ep: 11/25	It: 51/4130	batch_loss: 4.4476	batch_accuracy: 25.37%	lr:0.000854
Ep: 11/25	It: 101/4130	batch_loss: 4.5647	batch_accuracy: 22.75%	lr:0.000854
Ep: 11/25	It: 151/4130	batch_loss: 4.5009	batch_accuracy: 23.41%	lr:0.000853
Ep: 11/25	It: 201/4130	batch_loss: 4.4856	batch_accuracy: 23.44%	lr:0.000852
Ep: 11/25	It: 251/4130	batch_loss: 4.5207	batch_accuracy: 23.95%	lr:0.000852
Ep: 11/25	It: 301/4130	batch_loss: 4.3832	batch_accuracy: 25.34%	lr:0.000851
Ep: 11/25	It: 351/4130	batch_loss: 4.4441	batch_accuracy: 25.49%	lr:0.000850
Ep: 11/25	It: 401/4130	batch_loss: 4.4989	batch_accuracy: 22.61%	lr:0.000850
Ep: 11/25	It: 451/4130	batch_loss: 4.4124	batch_accuracy: 24.80%	lr:0.000849
Ep: 11/25	It: 501/4130	batch_loss: 4.3775	batch_accuracy: 24.61%	lr:0.000848
Ep: 11/25	It: 551/4130	batch_loss: 4.4549	batch_accuracy: 24.27%	lr:0.000848
Ep: 11/25	It: 601/4130	batch_loss: 4.4357	batch_accuracy: 24.80%	lr:0.000847
Ep: 11/25	It: 651/4130	batch_loss: 4.4544	batch_accuracy: 24.83%	lr:0.000846
Ep: 11/25	It: 701/4130	batch_loss: 4.4492	batch_accuracy: 22.83%	lr:0.000846
Ep: 11/25	It: 751/4130	batch_loss: 4.5470	batch_accuracy: 23.07%	lr:0.000845
Ep: 11/25	It: 801/4130	batch_loss: 4.4778	batch_accuracy: 24.34%	lr:0.000844
Ep: 11/25	It: 851/4130	batch_loss: 4.4559	batch_accuracy: 24.00%	lr:0.000843
Ep: 11/25	It: 901/4130	batch_loss: 4.5962	batch_accuracy: 22.63%	lr:0.000843
Ep: 11/25	It: 951/4130	batch_loss: 4.6478	batch_accuracy: 21.78%	lr:0.000842
Ep: 11/25	It: 1001/4130	batch_loss: 4.5695	batch_accuracy: 22.34%	lr:0.000841
Ep: 11/25	It: 1051/4130	batch_loss: 4.5593	batch_accuracy: 22.68%	lr:0.000841
Ep: 11/25	It: 1101/4130	batch_loss: 4.4551	batch_accuracy: 23.34%	lr:0.000840
Ep: 11/25	It: 1151/4130	batch_loss: 4.4520	batch_accuracy: 23.12%	lr:0.000839
Ep: 11/25	It: 1201/4130	batch_loss: 4.5059	batch_accuracy: 23.68%	lr:0.000839
Ep: 11/25	It: 1251/4130	batch_loss: 4.4767	batch_accuracy: 23.44%	lr:0.000838
Ep: 11/25	It: 1301/4130	batch_loss: 4.4182	batch_accuracy: 24.58%	lr:0.000837
Ep: 11/25	It: 1351/4130	batch_loss: 4.5694	batch_accuracy: 22.34%	lr:0.000837
Ep: 11/25	It: 1401/4130	batch_loss: 4.5344	batch_accuracy: 23.17%	lr:0.000836
Ep: 11/25	It: 1451/4130	batch_loss: 4.4402	batch_accuracy: 25.05%	lr:0.000835
Ep: 11/25	It: 1501/4130	batch_loss: 4.3011	batch_accuracy: 26.25%	lr:0.000834
Ep: 11/25	It: 1551/4130	batch_loss: 4.4019	batch_accuracy: 24.98%	lr:0.000834
Ep: 11/25	It: 1601/4130	batch_loss: 4.4246	batch_accuracy: 24.29%	lr:0.000833
Ep: 11/25	It: 1651/4130	batch_loss: 4.4634	batch_accuracy: 23.56%	lr:0.000832
Ep: 11/25	It: 1701/4130	batch_loss: 4.3843	batch_accuracy: 24.63%	lr:0.000832
Ep: 11/25	It: 1751/4130	batch_loss: 4.3988	batch_accuracy: 25.88%	lr:0.000831
Ep: 11/25	It: 1801/4130	batch_loss: 4.5057	batch_accuracy: 23.75%	lr:0.000830
Ep: 11/25	It: 1851/4130	batch_loss: 4.3594	batch_accuracy: 26.29%	lr:0.000830
Ep: 11/25	It: 1901/4130	batch_loss: 4.3894	batch_accuracy: 24.29%	lr:0.000829
Ep: 11/25	It: 1951/4130	batch_loss: 4.4573	batch_accuracy: 25.95%	lr:0.000828
Ep: 11/25	It: 2001/4130	batch_loss: 4.4942	batch_accuracy: 24.32%	lr:0.000827
Ep: 11/25	It: 2051/4130	batch_loss: 4.4366	batch_accuracy: 24.39%	lr:0.000827
Ep: 11/25	It: 2101/4130	batch_loss: 4.3650	batch_accuracy: 26.29%	lr:0.000826
Ep: 11/25	It: 2151/4130	batch_loss: 4.4401	batch_accuracy: 24.93%	lr:0.000825
Ep: 11/25	It: 2201/4130	batch_loss: 4.5140	batch_accuracy: 23.00%	lr:0.000825
Ep: 11/25	It: 2251/4130	batch_loss: 4.4604	batch_accuracy: 24.56%	lr:0.000824
Ep: 11/25	It: 2301/4130	batch_loss: 4.3076	batch_accuracy: 25.56%	lr:0.000823
Ep: 11/25	It: 2351/4130	batch_loss: 4.4545	batch_accuracy: 23.73%	lr:0.000822
Ep: 11/25	It: 2401/4130	batch_loss: 4.4860	batch_accuracy: 22.92%	lr:0.000822
Ep: 11/25	It: 2451/4130	batch_loss: 4.4583	batch_accuracy: 24.78%	lr:0.000821
Ep: 11/25	It: 2501/4130	batch_loss: 4.5408	batch_accuracy: 23.90%	lr:0.000820
Ep: 11/25	It: 2551/4130	batch_loss: 4.4558	batch_accuracy: 23.80%	lr:0.000819
Ep: 11/25	It: 2601/4130	batch_loss: 4.3626	batch_accuracy: 25.29%	lr:0.000819
Ep: 11/25	It: 2651/4130	batch_loss: 4.5345	batch_accuracy: 23.73%	lr:0.000818
Ep: 11/25	It: 2701/4130	batch_loss: 4.3695	batch_accuracy: 25.49%	lr:0.000817
Ep: 11/25	It: 2751/4130	batch_loss: 4.6115	batch_accuracy: 22.39%	lr:0.000817
Ep: 11/25	It: 2801/4130	batch_loss: 4.4573	batch_accuracy: 24.27%	lr:0.000816
Ep: 11/25	It: 2851/4130	batch_loss: 4.5167	batch_accuracy: 23.44%	lr:0.000815
Ep: 11/25	It: 2901/4130	batch_loss: 4.3463	batch_accuracy: 26.05%	lr:0.000814
Ep: 11/25	It: 2951/4130	batch_loss: 4.5181	batch_accuracy: 24.39%	lr:0.000814
Ep: 11/25	It: 3001/4130	batch_loss: 4.5680	batch_accuracy: 22.41%	lr:0.000813
Ep: 11/25	It: 3051/4130	batch_loss: 4.3235	batch_accuracy: 26.78%	lr:0.000812
Ep: 11/25	It: 3101/4130	batch_loss: 4.4125	batch_accuracy: 24.44%	lr:0.000811
Ep: 11/25	It: 3151/4130	batch_loss: 4.5486	batch_accuracy: 23.83%	lr:0.000811
Ep: 11/25	It: 3201/4130	batch_loss: 4.4031	batch_accuracy: 24.85%	lr:0.000810
Ep: 11/25	It: 3251/4130	batch_loss: 4.5049	batch_accuracy: 23.75%	lr:0.000809
Ep: 11/25	It: 3301/4130	batch_loss: 4.4590	batch_accuracy: 23.02%	lr:0.000808
Ep: 11/25	It: 3351/4130	batch_loss: 4.4191	batch_accuracy: 25.24%	lr:0.000808
Ep: 11/25	It: 3401/4130	batch_loss: 4.4578	batch_accuracy: 24.00%	lr:0.000807
Ep: 11/25	It: 3451/4130	batch_loss: 4.4852	batch_accuracy: 24.88%	lr:0.000806
Ep: 11/25	It: 3501/4130	batch_loss: 4.4463	batch_accuracy: 25.02%	lr:0.000805
Ep: 11/25	It: 3551/4130	batch_loss: 4.4544	batch_accuracy: 24.22%	lr:0.000805
Ep: 11/25	It: 3601/4130	batch_loss: 4.5269	batch_accuracy: 22.83%	lr:0.000804
Ep: 11/25	It: 3651/4130	batch_loss: 4.4148	batch_accuracy: 25.73%	lr:0.000803
Ep: 11/25	It: 3701/4130	batch_loss: 4.4448	batch_accuracy: 24.73%	lr:0.000802
Ep: 11/25	It: 3751/4130	batch_loss: 4.4020	batch_accuracy: 24.39%	lr:0.000802
Ep: 11/25	It: 3801/4130	batch_loss: 4.4722	batch_accuracy: 24.34%	lr:0.000801
Ep: 11/25	It: 3851/4130	batch_loss: 4.3945	batch_accuracy: 25.20%	lr:0.000800
Ep: 11/25	It: 3901/4130	batch_loss: 4.4610	batch_accuracy: 23.83%	lr:0.000799
Ep: 11/25	It: 3951/4130	batch_loss: 4.4494	batch_accuracy: 24.58%	lr:0.000799
Ep: 11/25	It: 4001/4130	batch_loss: 4.4506	batch_accuracy: 24.15%	lr:0.000798
Ep: 11/25	It: 4051/4130	batch_loss: 4.3845	batch_accuracy: 25.27%	lr:0.000797
Ep: 11/25	It: 4101/4130	batch_loss: 4.4298	batch_accuracy: 25.32%	lr:0.000796
Ep: 11/25	It: 4130/4130	batch_loss: 4.3465	batch_accuracy: 25.54%	lr:0.000796


Generated text for input text "You" is:
Youa’s and to the “bital”, is the main challenges that in turns, to be a worg. We also discuss the potential to improve the effects of this issue, inherent and more than the existing system, and can be made, for the fact that aver of theories to the environment of the process and the system. In this paper, we propose a novel method for determining the effectiveness of the proposed approach, which can be achieved by the proposed algorithm. We use a method that is based on the proposed system for the proposed method.
<eot>
<sot>
The role of the influence of different models.

The influence of a single‐quality of a wide spectrum of different materials and the current of a single-dimensional structure of the model is carried out. This method is based on a simple model that enables us to estimate the performance of the proposed algorithm. The proposed approach is implemented using the method, which allows to evaluate the performance of the proposed algorithm to estimate the effectiveness of the proposed algorithm, but we propose a new method to solve the effectiveness of the performance.
<eot>
<sot>
Atomic study of the Capital River

This paper examines the


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 12/25	It: 1/4130	batch_loss: 4.3314	batch_accuracy: 25.32%	lr:0.000796
Ep: 12/25	It: 51/4130	batch_loss: 4.4982	batch_accuracy: 22.68%	lr:0.000795
Ep: 12/25	It: 101/4130	batch_loss: 4.4371	batch_accuracy: 23.97%	lr:0.000794
Ep: 12/25	It: 151/4130	batch_loss: 4.5848	batch_accuracy: 23.36%	lr:0.000794
Ep: 12/25	It: 201/4130	batch_loss: 4.5520	batch_accuracy: 23.49%	lr:0.000793
Ep: 12/25	It: 251/4130	batch_loss: 4.4144	batch_accuracy: 24.95%	lr:0.000792
Ep: 12/25	It: 301/4130	batch_loss: 4.4001	batch_accuracy: 24.58%	lr:0.000791
Ep: 12/25	It: 351/4130	batch_loss: 4.4667	batch_accuracy: 24.39%	lr:0.000791
Ep: 12/25	It: 401/4130	batch_loss: 4.4922	batch_accuracy: 23.88%	lr:0.000790
Ep: 12/25	It: 451/4130	batch_loss: 4.4620	batch_accuracy: 25.10%	lr:0.000789
Ep: 12/25	It: 501/4130	batch_loss: 4.4317	batch_accuracy: 24.02%	lr:0.000788
Ep: 12/25	It: 551/4130	batch_loss: 4.4734	batch_accuracy: 23.80%	lr:0.000787
Ep: 12/25	It: 601/4130	batch_loss: 4.4511	batch_accuracy: 25.15%	lr:0.000787
Ep: 12/25	It: 651/4130	batch_loss: 4.5043	batch_accuracy: 24.39%	lr:0.000786
Ep: 12/25	It: 701/4130	batch_loss: 4.3864	batch_accuracy: 24.39%	lr:0.000785
Ep: 12/25	It: 751/4130	batch_loss: 4.5864	batch_accuracy: 22.14%	lr:0.000784
Ep: 12/25	It: 801/4130	batch_loss: 4.3469	batch_accuracy: 25.63%	lr:0.000784
Ep: 12/25	It: 851/4130	batch_loss: 4.3510	batch_accuracy: 26.10%	lr:0.000783
Ep: 12/25	It: 901/4130	batch_loss: 4.4737	batch_accuracy: 25.81%	lr:0.000782
Ep: 12/25	It: 951/4130	batch_loss: 4.4576	batch_accuracy: 24.34%	lr:0.000781
Ep: 12/25	It: 1001/4130	batch_loss: 4.4407	batch_accuracy: 24.44%	lr:0.000780
Ep: 12/25	It: 1051/4130	batch_loss: 4.4481	batch_accuracy: 24.63%	lr:0.000780
Ep: 12/25	It: 1101/4130	batch_loss: 4.5307	batch_accuracy: 23.24%	lr:0.000779
Ep: 12/25	It: 1151/4130	batch_loss: 4.3777	batch_accuracy: 25.39%	lr:0.000778
Ep: 12/25	It: 1201/4130	batch_loss: 4.5433	batch_accuracy: 24.00%	lr:0.000777
Ep: 12/25	It: 1251/4130	batch_loss: 4.4208	batch_accuracy: 25.27%	lr:0.000777
Ep: 12/25	It: 1301/4130	batch_loss: 4.3509	batch_accuracy: 26.10%	lr:0.000776
Ep: 12/25	It: 1351/4130	batch_loss: 4.3807	batch_accuracy: 24.83%	lr:0.000775
Ep: 12/25	It: 1401/4130	batch_loss: 4.4420	batch_accuracy: 23.83%	lr:0.000774
Ep: 12/25	It: 1451/4130	batch_loss: 4.4759	batch_accuracy: 24.32%	lr:0.000773
Ep: 12/25	It: 1501/4130	batch_loss: 4.4606	batch_accuracy: 24.54%	lr:0.000773
Ep: 12/25	It: 1551/4130	batch_loss: 4.5319	batch_accuracy: 23.85%	lr:0.000772
Ep: 12/25	It: 1601/4130	batch_loss: 4.5427	batch_accuracy: 22.24%	lr:0.000771
Ep: 12/25	It: 1651/4130	batch_loss: 4.4122	batch_accuracy: 24.22%	lr:0.000770
Ep: 12/25	It: 1701/4130	batch_loss: 4.5328	batch_accuracy: 23.85%	lr:0.000769
Ep: 12/25	It: 1751/4130	batch_loss: 4.4990	batch_accuracy: 23.44%	lr:0.000769
Ep: 12/25	It: 1801/4130	batch_loss: 4.4491	batch_accuracy: 23.97%	lr:0.000768
Ep: 12/25	It: 1851/4130	batch_loss: 4.3625	batch_accuracy: 25.46%	lr:0.000767
Ep: 12/25	It: 1901/4130	batch_loss: 4.5042	batch_accuracy: 22.90%	lr:0.000766
Ep: 12/25	It: 1951/4130	batch_loss: 4.4701	batch_accuracy: 23.17%	lr:0.000765
Ep: 12/25	It: 2001/4130	batch_loss: 4.5362	batch_accuracy: 23.07%	lr:0.000765
Ep: 12/25	It: 2051/4130	batch_loss: 4.4409	batch_accuracy: 23.34%	lr:0.000764
Ep: 12/25	It: 2101/4130	batch_loss: 4.4882	batch_accuracy: 23.05%	lr:0.000763
Ep: 12/25	It: 2151/4130	batch_loss: 4.4098	batch_accuracy: 24.49%	lr:0.000762
Ep: 12/25	It: 2201/4130	batch_loss: 4.3576	batch_accuracy: 24.34%	lr:0.000761
Ep: 12/25	It: 2251/4130	batch_loss: 4.3277	batch_accuracy: 25.24%	lr:0.000761
Ep: 12/25	It: 2301/4130	batch_loss: 4.3976	batch_accuracy: 25.32%	lr:0.000760
Ep: 12/25	It: 2351/4130	batch_loss: 4.5431	batch_accuracy: 22.90%	lr:0.000759
Ep: 12/25	It: 2401/4130	batch_loss: 4.4725	batch_accuracy: 23.85%	lr:0.000758
Ep: 12/25	It: 2451/4130	batch_loss: 4.4175	batch_accuracy: 25.05%	lr:0.000757
Ep: 12/25	It: 2501/4130	batch_loss: 4.4633	batch_accuracy: 23.85%	lr:0.000757
Ep: 12/25	It: 2551/4130	batch_loss: 4.5453	batch_accuracy: 23.32%	lr:0.000756
Ep: 12/25	It: 2601/4130	batch_loss: 4.5391	batch_accuracy: 23.05%	lr:0.000755
Ep: 12/25	It: 2651/4130	batch_loss: 4.4704	batch_accuracy: 24.68%	lr:0.000754
Ep: 12/25	It: 2701/4130	batch_loss: 4.5484	batch_accuracy: 22.92%	lr:0.000753
Ep: 12/25	It: 2751/4130	batch_loss: 4.5342	batch_accuracy: 23.66%	lr:0.000753
Ep: 12/25	It: 2801/4130	batch_loss: 4.3605	batch_accuracy: 25.24%	lr:0.000752
Ep: 12/25	It: 2851/4130	batch_loss: 4.4169	batch_accuracy: 25.00%	lr:0.000751
Ep: 12/25	It: 2901/4130	batch_loss: 4.4427	batch_accuracy: 24.32%	lr:0.000750
Ep: 12/25	It: 2951/4130	batch_loss: 4.4456	batch_accuracy: 24.12%	lr:0.000749
Ep: 12/25	It: 3001/4130	batch_loss: 4.4096	batch_accuracy: 25.59%	lr:0.000748
Ep: 12/25	It: 3051/4130	batch_loss: 4.5429	batch_accuracy: 22.80%	lr:0.000748
Ep: 12/25	It: 3101/4130	batch_loss: 4.3628	batch_accuracy: 24.68%	lr:0.000747
Ep: 12/25	It: 3151/4130	batch_loss: 4.5390	batch_accuracy: 23.12%	lr:0.000746
Ep: 12/25	It: 3201/4130	batch_loss: 4.4945	batch_accuracy: 23.10%	lr:0.000745
Ep: 12/25	It: 3251/4130	batch_loss: 4.5462	batch_accuracy: 22.71%	lr:0.000744
Ep: 12/25	It: 3301/4130	batch_loss: 4.4068	batch_accuracy: 24.95%	lr:0.000744
Ep: 12/25	It: 3351/4130	batch_loss: 4.4633	batch_accuracy: 23.12%	lr:0.000743
Ep: 12/25	It: 3401/4130	batch_loss: 4.4706	batch_accuracy: 25.02%	lr:0.000742
Ep: 12/25	It: 3451/4130	batch_loss: 4.5178	batch_accuracy: 24.00%	lr:0.000741
Ep: 12/25	It: 3501/4130	batch_loss: 4.5044	batch_accuracy: 22.46%	lr:0.000740
Ep: 12/25	It: 3551/4130	batch_loss: 4.3633	batch_accuracy: 26.88%	lr:0.000739
Ep: 12/25	It: 3601/4130	batch_loss: 4.3926	batch_accuracy: 24.73%	lr:0.000739
Ep: 12/25	It: 3651/4130	batch_loss: 4.4460	batch_accuracy: 24.63%	lr:0.000738
Ep: 12/25	It: 3701/4130	batch_loss: 4.4324	batch_accuracy: 24.05%	lr:0.000737
Ep: 12/25	It: 3751/4130	batch_loss: 4.4255	batch_accuracy: 24.66%	lr:0.000736
Ep: 12/25	It: 3801/4130	batch_loss: 4.3438	batch_accuracy: 25.83%	lr:0.000735
Ep: 12/25	It: 3851/4130	batch_loss: 4.4308	batch_accuracy: 23.46%	lr:0.000734
Ep: 12/25	It: 3901/4130	batch_loss: 4.5517	batch_accuracy: 23.93%	lr:0.000734
Ep: 12/25	It: 3951/4130	batch_loss: 4.3208	batch_accuracy: 26.25%	lr:0.000733
Ep: 12/25	It: 4001/4130	batch_loss: 4.5088	batch_accuracy: 24.34%	lr:0.000732
Ep: 12/25	It: 4051/4130	batch_loss: 4.4847	batch_accuracy: 25.22%	lr:0.000731
Ep: 12/25	It: 4101/4130	batch_loss: 4.4683	batch_accuracy: 24.17%	lr:0.000730
Ep: 12/25	It: 4130/4130	batch_loss: 4.3887	batch_accuracy: 25.22%	lr:0.000730


Generated text for input text "You" is:
Youi. The authors suggest that this hypothesis that inherently, however, tobaccoise is not only azle-litant took aver to achieve the use of the use of these patients with cancer. In thesis, thesis is associated with clinical and non-figmentation.
<eot>
<sot>
Medical and clinical outcomes in a patient with chronic disease of acute liver transplant.

The role of chronic pain and treatment of the liver metastatic leukemia of the liver cancer was investigated in the study of the patient with the diagnosis of acute lymphoid tumors and cancer treatment in cancer.


RESULTS
A total of 106 patients with HCVA.


CONCLUSIONS
SUCC patients undergoing a high prevalence of the risk of the tumor-specific disease and survival were assessed in the study. Apart from this study was conducted to investigate the efficacy of the treatment. Results: The incidence of a 5-year‐old female group (21.8%), followed by the diagnosis of a total of 121 patients with C. s. Capac, and 54 (6.3%), and 6 (5.8%), and 4 (91.4%)


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 13/25	It: 1/4130	batch_loss: 4.4214	batch_accuracy: 24.46%	lr:0.000730
Ep: 13/25	It: 51/4130	batch_loss: 4.4319	batch_accuracy: 23.14%	lr:0.000729
Ep: 13/25	It: 101/4130	batch_loss: 4.3525	batch_accuracy: 26.20%	lr:0.000728
Ep: 13/25	It: 151/4130	batch_loss: 4.3992	batch_accuracy: 25.98%	lr:0.000727
Ep: 13/25	It: 201/4130	batch_loss: 4.4042	batch_accuracy: 25.29%	lr:0.000726
Ep: 13/25	It: 251/4130	batch_loss: 4.4039	batch_accuracy: 24.83%	lr:0.000725
Ep: 13/25	It: 301/4130	batch_loss: 4.4408	batch_accuracy: 24.71%	lr:0.000725
Ep: 13/25	It: 351/4130	batch_loss: 4.4561	batch_accuracy: 25.10%	lr:0.000724
Ep: 13/25	It: 401/4130	batch_loss: 4.4064	batch_accuracy: 25.27%	lr:0.000723
Ep: 13/25	It: 451/4130	batch_loss: 4.4373	batch_accuracy: 24.27%	lr:0.000722
Ep: 13/25	It: 501/4130	batch_loss: 4.4043	batch_accuracy: 25.29%	lr:0.000721
Ep: 13/25	It: 551/4130	batch_loss: 4.4076	batch_accuracy: 25.17%	lr:0.000720
Ep: 13/25	It: 601/4130	batch_loss: 4.4478	batch_accuracy: 24.51%	lr:0.000720
Ep: 13/25	It: 651/4130	batch_loss: 4.5456	batch_accuracy: 23.10%	lr:0.000719
Ep: 13/25	It: 701/4130	batch_loss: 4.5367	batch_accuracy: 24.19%	lr:0.000718
Ep: 13/25	It: 751/4130	batch_loss: 4.4662	batch_accuracy: 24.39%	lr:0.000717
Ep: 13/25	It: 801/4130	batch_loss: 4.4368	batch_accuracy: 24.07%	lr:0.000716
Ep: 13/25	It: 851/4130	batch_loss: 4.4127	batch_accuracy: 24.83%	lr:0.000715
Ep: 13/25	It: 901/4130	batch_loss: 4.4310	batch_accuracy: 23.36%	lr:0.000714
Ep: 13/25	It: 951/4130	batch_loss: 4.3459	batch_accuracy: 25.59%	lr:0.000714
Ep: 13/25	It: 1001/4130	batch_loss: 4.5835	batch_accuracy: 23.34%	lr:0.000713
Ep: 13/25	It: 1051/4130	batch_loss: 4.3814	batch_accuracy: 25.10%	lr:0.000712
Ep: 13/25	It: 1101/4130	batch_loss: 4.4125	batch_accuracy: 24.05%	lr:0.000711
Ep: 13/25	It: 1151/4130	batch_loss: 4.5190	batch_accuracy: 23.44%	lr:0.000710
Ep: 13/25	It: 1201/4130	batch_loss: 4.4905	batch_accuracy: 24.39%	lr:0.000709
Ep: 13/25	It: 1251/4130	batch_loss: 4.4844	batch_accuracy: 24.80%	lr:0.000708
Ep: 13/25	It: 1301/4130	batch_loss: 4.4154	batch_accuracy: 24.63%	lr:0.000708
Ep: 13/25	It: 1351/4130	batch_loss: 4.4235	batch_accuracy: 23.97%	lr:0.000707
Ep: 13/25	It: 1401/4130	batch_loss: 4.3799	batch_accuracy: 24.78%	lr:0.000706
Ep: 13/25	It: 1451/4130	batch_loss: 4.4578	batch_accuracy: 24.58%	lr:0.000705
Ep: 13/25	It: 1501/4130	batch_loss: 4.3615	batch_accuracy: 26.20%	lr:0.000704
Ep: 13/25	It: 1551/4130	batch_loss: 4.4614	batch_accuracy: 24.27%	lr:0.000703
Ep: 13/25	It: 1601/4130	batch_loss: 4.4262	batch_accuracy: 24.98%	lr:0.000702
Ep: 13/25	It: 1651/4130	batch_loss: 4.3900	batch_accuracy: 24.93%	lr:0.000702
Ep: 13/25	It: 1701/4130	batch_loss: 4.4236	batch_accuracy: 25.83%	lr:0.000701
Ep: 13/25	It: 1751/4130	batch_loss: 4.5206	batch_accuracy: 23.54%	lr:0.000700
Ep: 13/25	It: 1801/4130	batch_loss: 4.3828	batch_accuracy: 24.83%	lr:0.000699
Ep: 13/25	It: 1851/4130	batch_loss: 4.4394	batch_accuracy: 24.58%	lr:0.000698
Ep: 13/25	It: 1901/4130	batch_loss: 4.5065	batch_accuracy: 23.66%	lr:0.000697
Ep: 13/25	It: 1951/4130	batch_loss: 4.4361	batch_accuracy: 24.78%	lr:0.000696
Ep: 13/25	It: 2001/4130	batch_loss: 4.5188	batch_accuracy: 24.05%	lr:0.000696
Ep: 13/25	It: 2051/4130	batch_loss: 4.4188	batch_accuracy: 24.46%	lr:0.000695
Ep: 13/25	It: 2101/4130	batch_loss: 4.4645	batch_accuracy: 24.22%	lr:0.000694
Ep: 13/25	It: 2151/4130	batch_loss: 4.4865	batch_accuracy: 22.92%	lr:0.000693
Ep: 13/25	It: 2201/4130	batch_loss: 4.4958	batch_accuracy: 24.10%	lr:0.000692
Ep: 13/25	It: 2251/4130	batch_loss: 4.4312	batch_accuracy: 24.17%	lr:0.000691
Ep: 13/25	It: 2301/4130	batch_loss: 4.3424	batch_accuracy: 25.15%	lr:0.000690
Ep: 13/25	It: 2351/4130	batch_loss: 4.4317	batch_accuracy: 24.32%	lr:0.000689
Ep: 13/25	It: 2401/4130	batch_loss: 4.4436	batch_accuracy: 24.88%	lr:0.000689
Ep: 13/25	It: 2451/4130	batch_loss: 4.4132	batch_accuracy: 24.49%	lr:0.000688
Ep: 13/25	It: 2501/4130	batch_loss: 4.4118	batch_accuracy: 23.97%	lr:0.000687
Ep: 13/25	It: 2551/4130	batch_loss: 4.5248	batch_accuracy: 24.29%	lr:0.000686
Ep: 13/25	It: 2601/4130	batch_loss: 4.3784	batch_accuracy: 23.68%	lr:0.000685
Ep: 13/25	It: 2651/4130	batch_loss: 4.5309	batch_accuracy: 24.02%	lr:0.000684
Ep: 13/25	It: 2701/4130	batch_loss: 4.5794	batch_accuracy: 22.29%	lr:0.000683
Ep: 13/25	It: 2751/4130	batch_loss: 4.4496	batch_accuracy: 26.07%	lr:0.000682
Ep: 13/25	It: 2801/4130	batch_loss: 4.4526	batch_accuracy: 24.27%	lr:0.000682
Ep: 13/25	It: 2851/4130	batch_loss: 4.4388	batch_accuracy: 23.90%	lr:0.000681
Ep: 13/25	It: 2901/4130	batch_loss: 4.4920	batch_accuracy: 23.63%	lr:0.000680
Ep: 13/25	It: 2951/4130	batch_loss: 4.4109	batch_accuracy: 24.22%	lr:0.000679
Ep: 13/25	It: 3001/4130	batch_loss: 4.5483	batch_accuracy: 23.32%	lr:0.000678
Ep: 13/25	It: 3051/4130	batch_loss: 4.5529	batch_accuracy: 23.10%	lr:0.000677
Ep: 13/25	It: 3101/4130	batch_loss: 4.4459	batch_accuracy: 24.78%	lr:0.000676
Ep: 13/25	It: 3151/4130	batch_loss: 4.3961	batch_accuracy: 25.02%	lr:0.000675
Ep: 13/25	It: 3201/4130	batch_loss: 4.3556	batch_accuracy: 25.46%	lr:0.000674
Ep: 13/25	It: 3251/4130	batch_loss: 4.4712	batch_accuracy: 24.58%	lr:0.000674
Ep: 13/25	It: 3301/4130	batch_loss: 4.3155	batch_accuracy: 24.93%	lr:0.000673
Ep: 13/25	It: 3351/4130	batch_loss: 4.2950	batch_accuracy: 26.59%	lr:0.000672
Ep: 13/25	It: 3401/4130	batch_loss: 4.4183	batch_accuracy: 24.61%	lr:0.000671
Ep: 13/25	It: 3451/4130	batch_loss: 4.2783	batch_accuracy: 27.08%	lr:0.000670
Ep: 13/25	It: 3501/4130	batch_loss: 4.3129	batch_accuracy: 25.17%	lr:0.000669
Ep: 13/25	It: 3551/4130	batch_loss: 4.4171	batch_accuracy: 24.34%	lr:0.000668
Ep: 13/25	It: 3601/4130	batch_loss: 4.4082	batch_accuracy: 25.05%	lr:0.000667
Ep: 13/25	It: 3651/4130	batch_loss: 4.3093	batch_accuracy: 25.78%	lr:0.000667
Ep: 13/25	It: 3701/4130	batch_loss: 4.3249	batch_accuracy: 25.15%	lr:0.000666
Ep: 13/25	It: 3751/4130	batch_loss: 4.4699	batch_accuracy: 24.22%	lr:0.000665
Ep: 13/25	It: 3801/4130	batch_loss: 4.4059	batch_accuracy: 25.10%	lr:0.000664
Ep: 13/25	It: 3851/4130	batch_loss: 4.5049	batch_accuracy: 24.76%	lr:0.000663
Ep: 13/25	It: 3901/4130	batch_loss: 4.4308	batch_accuracy: 24.80%	lr:0.000662
Ep: 13/25	It: 3951/4130	batch_loss: 4.4084	batch_accuracy: 25.24%	lr:0.000661
Ep: 13/25	It: 4001/4130	batch_loss: 4.4071	batch_accuracy: 24.32%	lr:0.000660
Ep: 13/25	It: 4051/4130	batch_loss: 4.3940	batch_accuracy: 23.54%	lr:0.000659
Ep: 13/25	It: 4101/4130	batch_loss: 4.4439	batch_accuracy: 24.15%	lr:0.000658
Ep: 13/25	It: 4130/4130	batch_loss: 4.2494	batch_accuracy: 26.43%	lr:0.000658


Generated text for input text "You" is:
Youvi’s. In this article, the first work to the present study to determine the relationship between aest and the relationship between theories in which were the book’s, which had the use of theories. April and theories officers’s, theories officers’ “consistent”, ‘word" of the ‘name’. The study of the article is to determine the extent of the social capital in the context of the County's political perspective. This research will also be the main goal of interest in the future and the future of social policy, social and economic, and social, economic and legal issues.
<eot>
<sot>
Innovation of the Prition, British, and the Budding of the Practice of the Longitudinal Study

Objective:

In the 1950s, in 1968, the Cardiovascular Cultural, May, and Forest Builtonian and Council in the United States and South Africa, the American Surface County.


Modern, Self-Terrorist and East Action
n


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 14/25	It: 1/4130	batch_loss: 4.3545	batch_accuracy: 25.27%	lr:0.000658
Ep: 14/25	It: 51/4130	batch_loss: 4.3982	batch_accuracy: 24.07%	lr:0.000657
Ep: 14/25	It: 101/4130	batch_loss: 4.4228	batch_accuracy: 24.15%	lr:0.000656
Ep: 14/25	It: 151/4130	batch_loss: 4.3748	batch_accuracy: 25.73%	lr:0.000655
Ep: 14/25	It: 201/4130	batch_loss: 4.5869	batch_accuracy: 23.24%	lr:0.000654
Ep: 14/25	It: 251/4130	batch_loss: 4.4448	batch_accuracy: 24.56%	lr:0.000653
Ep: 14/25	It: 301/4130	batch_loss: 4.4037	batch_accuracy: 24.54%	lr:0.000653
Ep: 14/25	It: 351/4130	batch_loss: 4.3372	batch_accuracy: 25.22%	lr:0.000652
Ep: 14/25	It: 401/4130	batch_loss: 4.4184	batch_accuracy: 25.05%	lr:0.000651
Ep: 14/25	It: 451/4130	batch_loss: 4.5041	batch_accuracy: 22.56%	lr:0.000650
Ep: 14/25	It: 501/4130	batch_loss: 4.5064	batch_accuracy: 23.29%	lr:0.000649
Ep: 14/25	It: 551/4130	batch_loss: 4.3079	batch_accuracy: 25.76%	lr:0.000648
Ep: 14/25	It: 601/4130	batch_loss: 4.5248	batch_accuracy: 23.95%	lr:0.000647
Ep: 14/25	It: 651/4130	batch_loss: 4.3460	batch_accuracy: 26.17%	lr:0.000646
Ep: 14/25	It: 701/4130	batch_loss: 4.4255	batch_accuracy: 24.78%	lr:0.000645
Ep: 14/25	It: 751/4130	batch_loss: 4.3406	batch_accuracy: 25.88%	lr:0.000644
Ep: 14/25	It: 801/4130	batch_loss: 4.3993	batch_accuracy: 24.22%	lr:0.000644
Ep: 14/25	It: 851/4130	batch_loss: 4.5128	batch_accuracy: 22.78%	lr:0.000643
Ep: 14/25	It: 901/4130	batch_loss: 4.3512	batch_accuracy: 25.78%	lr:0.000642
Ep: 14/25	It: 951/4130	batch_loss: 4.4965	batch_accuracy: 24.27%	lr:0.000641
Ep: 14/25	It: 1001/4130	batch_loss: 4.4562	batch_accuracy: 23.95%	lr:0.000640
Ep: 14/25	It: 1051/4130	batch_loss: 4.5287	batch_accuracy: 23.71%	lr:0.000639
Ep: 14/25	It: 1101/4130	batch_loss: 4.3648	batch_accuracy: 25.61%	lr:0.000638
Ep: 14/25	It: 1151/4130	batch_loss: 4.4750	batch_accuracy: 24.95%	lr:0.000637
Ep: 14/25	It: 1201/4130	batch_loss: 4.3347	batch_accuracy: 25.17%	lr:0.000636
Ep: 14/25	It: 1251/4130	batch_loss: 4.3865	batch_accuracy: 25.15%	lr:0.000635
Ep: 14/25	It: 1301/4130	batch_loss: 4.3697	batch_accuracy: 25.59%	lr:0.000634
Ep: 14/25	It: 1351/4130	batch_loss: 4.4069	batch_accuracy: 24.85%	lr:0.000634
Ep: 14/25	It: 1401/4130	batch_loss: 4.3800	batch_accuracy: 25.76%	lr:0.000633
Ep: 14/25	It: 1451/4130	batch_loss: 4.3393	batch_accuracy: 25.02%	lr:0.000632
Ep: 14/25	It: 1501/4130	batch_loss: 4.4129	batch_accuracy: 24.34%	lr:0.000631
Ep: 14/25	It: 1551/4130	batch_loss: 4.4904	batch_accuracy: 23.63%	lr:0.000630
Ep: 14/25	It: 1601/4130	batch_loss: 4.4456	batch_accuracy: 23.34%	lr:0.000629
Ep: 14/25	It: 1651/4130	batch_loss: 4.3515	batch_accuracy: 25.95%	lr:0.000628
Ep: 14/25	It: 1701/4130	batch_loss: 4.3900	batch_accuracy: 26.05%	lr:0.000627
Ep: 14/25	It: 1751/4130	batch_loss: 4.4376	batch_accuracy: 24.58%	lr:0.000626
Ep: 14/25	It: 1801/4130	batch_loss: 4.3584	batch_accuracy: 25.90%	lr:0.000625
Ep: 14/25	It: 1851/4130	batch_loss: 4.4084	batch_accuracy: 24.83%	lr:0.000624
Ep: 14/25	It: 1901/4130	batch_loss: 4.3858	batch_accuracy: 25.00%	lr:0.000624
Ep: 14/25	It: 1951/4130	batch_loss: 4.3826	batch_accuracy: 24.71%	lr:0.000623
Ep: 14/25	It: 2001/4130	batch_loss: 4.4244	batch_accuracy: 24.02%	lr:0.000622
Ep: 14/25	It: 2051/4130	batch_loss: 4.4197	batch_accuracy: 25.07%	lr:0.000621
Ep: 14/25	It: 2101/4130	batch_loss: 4.4963	batch_accuracy: 23.90%	lr:0.000620
Ep: 14/25	It: 2151/4130	batch_loss: 4.3750	batch_accuracy: 25.49%	lr:0.000619
Ep: 14/25	It: 2201/4130	batch_loss: 4.4652	batch_accuracy: 24.24%	lr:0.000618
Ep: 14/25	It: 2251/4130	batch_loss: 4.4005	batch_accuracy: 25.20%	lr:0.000617
Ep: 14/25	It: 2301/4130	batch_loss: 4.3474	batch_accuracy: 25.20%	lr:0.000616
Ep: 14/25	It: 2351/4130	batch_loss: 4.3368	batch_accuracy: 24.98%	lr:0.000615
Ep: 14/25	It: 2401/4130	batch_loss: 4.3316	batch_accuracy: 25.68%	lr:0.000614
Ep: 14/25	It: 2451/4130	batch_loss: 4.3961	batch_accuracy: 24.17%	lr:0.000613
Ep: 14/25	It: 2501/4130	batch_loss: 4.4784	batch_accuracy: 23.78%	lr:0.000613
Ep: 14/25	It: 2551/4130	batch_loss: 4.4278	batch_accuracy: 24.56%	lr:0.000612
Ep: 14/25	It: 2601/4130	batch_loss: 4.4467	batch_accuracy: 23.17%	lr:0.000611
Ep: 14/25	It: 2651/4130	batch_loss: 4.4685	batch_accuracy: 23.46%	lr:0.000610
Ep: 14/25	It: 2701/4130	batch_loss: 4.5583	batch_accuracy: 22.41%	lr:0.000609
Ep: 14/25	It: 2751/4130	batch_loss: 4.3738	batch_accuracy: 24.71%	lr:0.000608
Ep: 14/25	It: 2801/4130	batch_loss: 4.3964	batch_accuracy: 26.22%	lr:0.000607
Ep: 14/25	It: 2851/4130	batch_loss: 4.4407	batch_accuracy: 24.51%	lr:0.000606
Ep: 14/25	It: 2901/4130	batch_loss: 4.4822	batch_accuracy: 24.90%	lr:0.000605
Ep: 14/25	It: 2951/4130	batch_loss: 4.4394	batch_accuracy: 24.32%	lr:0.000604
Ep: 14/25	It: 3001/4130	batch_loss: 4.5017	batch_accuracy: 23.54%	lr:0.000603
Ep: 14/25	It: 3051/4130	batch_loss: 4.3690	batch_accuracy: 24.76%	lr:0.000602
Ep: 14/25	It: 3101/4130	batch_loss: 4.4105	batch_accuracy: 25.07%	lr:0.000601
Ep: 14/25	It: 3151/4130	batch_loss: 4.4159	batch_accuracy: 24.37%	lr:0.000601
Ep: 14/25	It: 3201/4130	batch_loss: 4.3514	batch_accuracy: 25.27%	lr:0.000600
Ep: 14/25	It: 3251/4130	batch_loss: 4.3176	batch_accuracy: 25.05%	lr:0.000599
Ep: 14/25	It: 3301/4130	batch_loss: 4.4028	batch_accuracy: 24.49%	lr:0.000598
Ep: 14/25	It: 3351/4130	batch_loss: 4.2559	batch_accuracy: 26.64%	lr:0.000597
Ep: 14/25	It: 3401/4130	batch_loss: 4.4157	batch_accuracy: 24.17%	lr:0.000596
Ep: 14/25	It: 3451/4130	batch_loss: 4.4162	batch_accuracy: 24.22%	lr:0.000595
Ep: 14/25	It: 3501/4130	batch_loss: 4.3919	batch_accuracy: 24.90%	lr:0.000594
Ep: 14/25	It: 3551/4130	batch_loss: 4.4109	batch_accuracy: 24.83%	lr:0.000593
Ep: 14/25	It: 3601/4130	batch_loss: 4.5261	batch_accuracy: 23.34%	lr:0.000592
Ep: 14/25	It: 3651/4130	batch_loss: 4.3471	batch_accuracy: 25.20%	lr:0.000591
Ep: 14/25	It: 3701/4130	batch_loss: 4.3476	batch_accuracy: 25.85%	lr:0.000590
Ep: 14/25	It: 3751/4130	batch_loss: 4.5072	batch_accuracy: 24.71%	lr:0.000589
Ep: 14/25	It: 3801/4130	batch_loss: 4.3424	batch_accuracy: 26.05%	lr:0.000589
Ep: 14/25	It: 3851/4130	batch_loss: 4.3933	batch_accuracy: 23.88%	lr:0.000588
Ep: 14/25	It: 3901/4130	batch_loss: 4.3458	batch_accuracy: 25.88%	lr:0.000587
Ep: 14/25	It: 3951/4130	batch_loss: 4.4662	batch_accuracy: 22.95%	lr:0.000586
Ep: 14/25	It: 4001/4130	batch_loss: 4.4122	batch_accuracy: 25.02%	lr:0.000585
Ep: 14/25	It: 4051/4130	batch_loss: 4.3207	batch_accuracy: 26.39%	lr:0.000584
Ep: 14/25	It: 4101/4130	batch_loss: 4.4678	batch_accuracy: 24.19%	lr:0.000583
Ep: 14/25	It: 4130/4130	batch_loss: 4.4006	batch_accuracy: 25.80%	lr:0.000582


Generated text for input text "You" is:
You, papi, and other forms of them. This paper presents the development of thesis to study on the effects of theories of athor with thesis. The present of this paper provides a new tool to explore how it has recently published questions that there's theory and to investigate the effects of the research and theories of a new and reliable way of interest to their understanding of the role of these findings. The paper focuses on the role of the new approach for the development of the new concept.
<eot>
<sot>
Due to the role of the role of the disease and death.

We have focused on the role of the disease in the elderly of this disease is to be considered. In this article, we found that the patient's population is in the patient, with chronic pain of the care unit (FD) and treatment options to the disease and treatment of patients who are not related to the development of an early and more likely to reduce the risk of cancer treatment, which may be used in this study. The aim of this study was to compare the prevalence of PTS-TI-2 and SFIs, and for this study, and treatment, or a significant reduction in the incidence of


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 15/25	It: 1/4130	batch_loss: 4.5336	batch_accuracy: 24.27%	lr:0.000582
Ep: 15/25	It: 51/4130	batch_loss: 4.4617	batch_accuracy: 23.85%	lr:0.000581
Ep: 15/25	It: 101/4130	batch_loss: 4.4043	batch_accuracy: 25.93%	lr:0.000581
Ep: 15/25	It: 151/4130	batch_loss: 4.3554	batch_accuracy: 24.32%	lr:0.000580
Ep: 15/25	It: 201/4130	batch_loss: 4.4516	batch_accuracy: 22.83%	lr:0.000579
Ep: 15/25	It: 251/4130	batch_loss: 4.4876	batch_accuracy: 24.83%	lr:0.000578
Ep: 15/25	It: 301/4130	batch_loss: 4.3862	batch_accuracy: 25.12%	lr:0.000577
Ep: 15/25	It: 351/4130	batch_loss: 4.2987	batch_accuracy: 25.73%	lr:0.000576
Ep: 15/25	It: 401/4130	batch_loss: 4.4244	batch_accuracy: 24.51%	lr:0.000575
Ep: 15/25	It: 451/4130	batch_loss: 4.3226	batch_accuracy: 25.34%	lr:0.000574
Ep: 15/25	It: 501/4130	batch_loss: 4.4392	batch_accuracy: 24.00%	lr:0.000573
Ep: 15/25	It: 551/4130	batch_loss: 4.5168	batch_accuracy: 23.41%	lr:0.000572
Ep: 15/25	It: 601/4130	batch_loss: 4.4327	batch_accuracy: 25.27%	lr:0.000571
Ep: 15/25	It: 651/4130	batch_loss: 4.3328	batch_accuracy: 25.12%	lr:0.000570
Ep: 15/25	It: 701/4130	batch_loss: 4.3586	batch_accuracy: 25.24%	lr:0.000569
Ep: 15/25	It: 751/4130	batch_loss: 4.4831	batch_accuracy: 23.88%	lr:0.000568
Ep: 15/25	It: 801/4130	batch_loss: 4.4151	batch_accuracy: 24.22%	lr:0.000567
Ep: 15/25	It: 851/4130	batch_loss: 4.3395	batch_accuracy: 26.46%	lr:0.000567
Ep: 15/25	It: 901/4130	batch_loss: 4.4335	batch_accuracy: 25.02%	lr:0.000566
Ep: 15/25	It: 951/4130	batch_loss: 4.4055	batch_accuracy: 24.93%	lr:0.000565
Ep: 15/25	It: 1001/4130	batch_loss: 4.3804	batch_accuracy: 24.85%	lr:0.000564
Ep: 15/25	It: 1051/4130	batch_loss: 4.3553	batch_accuracy: 25.44%	lr:0.000563
Ep: 15/25	It: 1101/4130	batch_loss: 4.5290	batch_accuracy: 24.29%	lr:0.000562
Ep: 15/25	It: 1151/4130	batch_loss: 4.4329	batch_accuracy: 23.61%	lr:0.000561
Ep: 15/25	It: 1201/4130	batch_loss: 4.3627	batch_accuracy: 25.73%	lr:0.000560
Ep: 15/25	It: 1251/4130	batch_loss: 4.2929	batch_accuracy: 26.86%	lr:0.000559
Ep: 15/25	It: 1301/4130	batch_loss: 4.3753	batch_accuracy: 25.00%	lr:0.000558
Ep: 15/25	It: 1351/4130	batch_loss: 4.3295	batch_accuracy: 24.19%	lr:0.000557
Ep: 15/25	It: 1401/4130	batch_loss: 4.2375	batch_accuracy: 26.56%	lr:0.000556
Ep: 15/25	It: 1451/4130	batch_loss: 4.3476	batch_accuracy: 24.61%	lr:0.000555
Ep: 15/25	It: 1501/4130	batch_loss: 4.2396	batch_accuracy: 26.64%	lr:0.000554
Ep: 15/25	It: 1551/4130	batch_loss: 4.3677	batch_accuracy: 25.37%	lr:0.000553
Ep: 15/25	It: 1601/4130	batch_loss: 4.5349	batch_accuracy: 22.71%	lr:0.000553
Ep: 15/25	It: 1651/4130	batch_loss: 4.4099	batch_accuracy: 24.44%	lr:0.000552
Ep: 15/25	It: 1701/4130	batch_loss: 4.3677	batch_accuracy: 25.88%	lr:0.000551
Ep: 15/25	It: 1751/4130	batch_loss: 4.3887	batch_accuracy: 26.22%	lr:0.000550
Ep: 15/25	It: 1801/4130	batch_loss: 4.4287	batch_accuracy: 24.15%	lr:0.000549
Ep: 15/25	It: 1851/4130	batch_loss: 4.5052	batch_accuracy: 23.12%	lr:0.000548
Ep: 15/25	It: 1901/4130	batch_loss: 4.4051	batch_accuracy: 23.73%	lr:0.000547
Ep: 15/25	It: 1951/4130	batch_loss: 4.3893	batch_accuracy: 26.20%	lr:0.000546
Ep: 15/25	It: 2001/4130	batch_loss: 4.4803	batch_accuracy: 23.73%	lr:0.000545
Ep: 15/25	It: 2051/4130	batch_loss: 4.4408	batch_accuracy: 25.59%	lr:0.000544
Ep: 15/25	It: 2101/4130	batch_loss: 4.3105	batch_accuracy: 26.90%	lr:0.000543
Ep: 15/25	It: 2151/4130	batch_loss: 4.3987	batch_accuracy: 24.80%	lr:0.000542
Ep: 15/25	It: 2201/4130	batch_loss: 4.4748	batch_accuracy: 23.95%	lr:0.000541
Ep: 15/25	It: 2251/4130	batch_loss: 4.3067	batch_accuracy: 27.15%	lr:0.000540
Ep: 15/25	It: 2301/4130	batch_loss: 4.4374	batch_accuracy: 24.02%	lr:0.000539
Ep: 15/25	It: 2351/4130	batch_loss: 4.4464	batch_accuracy: 24.32%	lr:0.000538
Ep: 15/25	It: 2401/4130	batch_loss: 4.4418	batch_accuracy: 24.78%	lr:0.000538
Ep: 15/25	It: 2451/4130	batch_loss: 4.3919	batch_accuracy: 24.83%	lr:0.000537
Ep: 15/25	It: 2501/4130	batch_loss: 4.3766	batch_accuracy: 25.66%	lr:0.000536
Ep: 15/25	It: 2551/4130	batch_loss: 4.3162	batch_accuracy: 25.20%	lr:0.000535
Ep: 15/25	It: 2601/4130	batch_loss: 4.4661	batch_accuracy: 23.73%	lr:0.000534
Ep: 15/25	It: 2651/4130	batch_loss: 4.3036	batch_accuracy: 25.02%	lr:0.000533
Ep: 15/25	It: 2701/4130	batch_loss: 4.4600	batch_accuracy: 24.10%	lr:0.000532
Ep: 15/25	It: 2751/4130	batch_loss: 4.2996	batch_accuracy: 25.90%	lr:0.000531
Ep: 15/25	It: 2801/4130	batch_loss: 4.3607	batch_accuracy: 25.00%	lr:0.000530
Ep: 15/25	It: 2851/4130	batch_loss: 4.3418	batch_accuracy: 26.81%	lr:0.000529
Ep: 15/25	It: 2901/4130	batch_loss: 4.4598	batch_accuracy: 23.88%	lr:0.000528
Ep: 15/25	It: 2951/4130	batch_loss: 4.2601	batch_accuracy: 26.05%	lr:0.000527
Ep: 15/25	It: 3001/4130	batch_loss: 4.3167	batch_accuracy: 26.20%	lr:0.000526
Ep: 15/25	It: 3051/4130	batch_loss: 4.3788	batch_accuracy: 25.20%	lr:0.000525
Ep: 15/25	It: 3101/4130	batch_loss: 4.3446	batch_accuracy: 25.46%	lr:0.000524
Ep: 15/25	It: 3151/4130	batch_loss: 4.4112	batch_accuracy: 24.19%	lr:0.000523
Ep: 15/25	It: 3201/4130	batch_loss: 4.4636	batch_accuracy: 23.85%	lr:0.000522
Ep: 15/25	It: 3251/4130	batch_loss: 4.3117	batch_accuracy: 24.90%	lr:0.000522
Ep: 15/25	It: 3301/4130	batch_loss: 4.3604	batch_accuracy: 25.54%	lr:0.000521
Ep: 15/25	It: 3351/4130	batch_loss: 4.4490	batch_accuracy: 25.05%	lr:0.000520
Ep: 15/25	It: 3401/4130	batch_loss: 4.4415	batch_accuracy: 24.54%	lr:0.000519
Ep: 15/25	It: 3451/4130	batch_loss: 4.4431	batch_accuracy: 24.71%	lr:0.000518
Ep: 15/25	It: 3501/4130	batch_loss: 4.5525	batch_accuracy: 22.88%	lr:0.000517
Ep: 15/25	It: 3551/4130	batch_loss: 4.2561	batch_accuracy: 26.39%	lr:0.000516
Ep: 15/25	It: 3601/4130	batch_loss: 4.4372	batch_accuracy: 23.73%	lr:0.000515
Ep: 15/25	It: 3651/4130	batch_loss: 4.3205	batch_accuracy: 25.88%	lr:0.000514
Ep: 15/25	It: 3701/4130	batch_loss: 4.4217	batch_accuracy: 25.44%	lr:0.000513
Ep: 15/25	It: 3751/4130	batch_loss: 4.3241	batch_accuracy: 25.22%	lr:0.000512
Ep: 15/25	It: 3801/4130	batch_loss: 4.2212	batch_accuracy: 27.29%	lr:0.000511
Ep: 15/25	It: 3851/4130	batch_loss: 4.3217	batch_accuracy: 26.03%	lr:0.000510
Ep: 15/25	It: 3901/4130	batch_loss: 4.3389	batch_accuracy: 25.39%	lr:0.000509
Ep: 15/25	It: 3951/4130	batch_loss: 4.3561	batch_accuracy: 25.27%	lr:0.000508
Ep: 15/25	It: 4001/4130	batch_loss: 4.5588	batch_accuracy: 22.88%	lr:0.000507
Ep: 15/25	It: 4051/4130	batch_loss: 4.3972	batch_accuracy: 24.90%	lr:0.000506
Ep: 15/25	It: 4101/4130	batch_loss: 4.3686	batch_accuracy: 25.05%	lr:0.000506
Ep: 15/25	It: 4130/4130	batch_loss: 4.4518	batch_accuracy: 23.60%	lr:0.000505


Generated text for input text "You" is:
You) to the most of theth and more in the same period. These findings suggest that the most important role of theories took, as well as theories that are still known to be ineffective by theories. It can be used for the world, as a significant difference in theories of the world.
<eot>
<sot>
Simple the role of the Ethanolism of the Life of the Locative Function

Abstract The results of the present study suggest that there is a new role in the use of the newborns. In this article, we show that a combination of SWT and the C-doped TiRa's Raman Fuja (Lober-Sugm) and the Raman spectroscopy of the SNPs to the L. S. Format

In this paper, we present a method for determining the effect of the bending surface and the surface. The model is applied to the same temperature-filtering algorithm. The results show that the results obtained with a simple linear correlation, which is also discussed. The results show that the proposed algorithm can improve the control of the MMBW,


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 16/25	It: 1/4130	batch_loss: 4.4033	batch_accuracy: 24.85%	lr:0.000505
Ep: 16/25	It: 51/4130	batch_loss: 4.5237	batch_accuracy: 24.44%	lr:0.000504
Ep: 16/25	It: 101/4130	batch_loss: 4.3408	batch_accuracy: 25.93%	lr:0.000503
Ep: 16/25	It: 151/4130	batch_loss: 4.3480	batch_accuracy: 25.22%	lr:0.000502
Ep: 16/25	It: 201/4130	batch_loss: 4.3951	batch_accuracy: 25.39%	lr:0.000501
Ep: 16/25	It: 251/4130	batch_loss: 4.3983	batch_accuracy: 26.34%	lr:0.000500
Ep: 16/25	It: 301/4130	batch_loss: 4.4272	batch_accuracy: 24.85%	lr:0.000499
Ep: 16/25	It: 351/4130	batch_loss: 4.4433	batch_accuracy: 25.17%	lr:0.000498
Ep: 16/25	It: 401/4130	batch_loss: 4.4747	batch_accuracy: 24.22%	lr:0.000497
Ep: 16/25	It: 451/4130	batch_loss: 4.3901	batch_accuracy: 25.15%	lr:0.000497
Ep: 16/25	It: 501/4130	batch_loss: 4.3798	batch_accuracy: 24.61%	lr:0.000496
Ep: 16/25	It: 551/4130	batch_loss: 4.3643	batch_accuracy: 24.32%	lr:0.000495
Ep: 16/25	It: 601/4130	batch_loss: 4.3505	batch_accuracy: 26.07%	lr:0.000494
Ep: 16/25	It: 651/4130	batch_loss: 4.4280	batch_accuracy: 25.24%	lr:0.000493
Ep: 16/25	It: 701/4130	batch_loss: 4.4023	batch_accuracy: 24.76%	lr:0.000492
Ep: 16/25	It: 751/4130	batch_loss: 4.2949	batch_accuracy: 26.37%	lr:0.000491
Ep: 16/25	It: 801/4130	batch_loss: 4.4606	batch_accuracy: 24.00%	lr:0.000490
Ep: 16/25	It: 851/4130	batch_loss: 4.3461	batch_accuracy: 26.29%	lr:0.000489
Ep: 16/25	It: 901/4130	batch_loss: 4.3275	batch_accuracy: 26.07%	lr:0.000488
Ep: 16/25	It: 951/4130	batch_loss: 4.4223	batch_accuracy: 25.02%	lr:0.000487
Ep: 16/25	It: 1001/4130	batch_loss: 4.3167	batch_accuracy: 25.61%	lr:0.000486
Ep: 16/25	It: 1051/4130	batch_loss: 4.4028	batch_accuracy: 24.93%	lr:0.000485
Ep: 16/25	It: 1101/4130	batch_loss: 4.4104	batch_accuracy: 24.34%	lr:0.000484
Ep: 16/25	It: 1151/4130	batch_loss: 4.4954	batch_accuracy: 24.49%	lr:0.000483
Ep: 16/25	It: 1201/4130	batch_loss: 4.3626	batch_accuracy: 25.54%	lr:0.000482
Ep: 16/25	It: 1251/4130	batch_loss: 4.3071	batch_accuracy: 26.20%	lr:0.000481
Ep: 16/25	It: 1301/4130	batch_loss: 4.4653	batch_accuracy: 23.75%	lr:0.000481
Ep: 16/25	It: 1351/4130	batch_loss: 4.3498	batch_accuracy: 25.63%	lr:0.000480
Ep: 16/25	It: 1401/4130	batch_loss: 4.4325	batch_accuracy: 24.85%	lr:0.000479
Ep: 16/25	It: 1451/4130	batch_loss: 4.3493	batch_accuracy: 25.07%	lr:0.000478
Ep: 16/25	It: 1501/4130	batch_loss: 4.4746	batch_accuracy: 24.29%	lr:0.000477
Ep: 16/25	It: 1551/4130	batch_loss: 4.3321	batch_accuracy: 25.49%	lr:0.000476
Ep: 16/25	It: 1601/4130	batch_loss: 4.4759	batch_accuracy: 24.12%	lr:0.000475
Ep: 16/25	It: 1651/4130	batch_loss: 4.3072	batch_accuracy: 26.25%	lr:0.000474
Ep: 16/25	It: 1701/4130	batch_loss: 4.5653	batch_accuracy: 23.22%	lr:0.000473
Ep: 16/25	It: 1751/4130	batch_loss: 4.4775	batch_accuracy: 24.41%	lr:0.000472
Ep: 16/25	It: 1801/4130	batch_loss: 4.3845	batch_accuracy: 25.81%	lr:0.000471
Ep: 16/25	It: 1851/4130	batch_loss: 4.3158	batch_accuracy: 25.93%	lr:0.000470
Ep: 16/25	It: 1901/4130	batch_loss: 4.3470	batch_accuracy: 26.32%	lr:0.000469
Ep: 16/25	It: 1951/4130	batch_loss: 4.4581	batch_accuracy: 23.90%	lr:0.000468
Ep: 16/25	It: 2001/4130	batch_loss: 4.4867	batch_accuracy: 23.58%	lr:0.000467
Ep: 16/25	It: 2051/4130	batch_loss: 4.2868	batch_accuracy: 26.54%	lr:0.000466
Ep: 16/25	It: 2101/4130	batch_loss: 4.4117	batch_accuracy: 25.51%	lr:0.000465
Ep: 16/25	It: 2151/4130	batch_loss: 4.3639	batch_accuracy: 25.17%	lr:0.000465
Ep: 16/25	It: 2201/4130	batch_loss: 4.3347	batch_accuracy: 25.93%	lr:0.000464
Ep: 16/25	It: 2251/4130	batch_loss: 4.3409	batch_accuracy: 25.68%	lr:0.000463
Ep: 16/25	It: 2301/4130	batch_loss: 4.3868	batch_accuracy: 24.61%	lr:0.000462
Ep: 16/25	It: 2351/4130	batch_loss: 4.4013	batch_accuracy: 24.93%	lr:0.000461
Ep: 16/25	It: 2401/4130	batch_loss: 4.5117	batch_accuracy: 22.66%	lr:0.000460
Ep: 16/25	It: 2451/4130	batch_loss: 4.3633	batch_accuracy: 26.20%	lr:0.000459
Ep: 16/25	It: 2501/4130	batch_loss: 4.2949	batch_accuracy: 25.56%	lr:0.000458
Ep: 16/25	It: 2551/4130	batch_loss: 4.3411	batch_accuracy: 25.32%	lr:0.000457
Ep: 16/25	It: 2601/4130	batch_loss: 4.3903	batch_accuracy: 25.49%	lr:0.000456
Ep: 16/25	It: 2651/4130	batch_loss: 4.3140	batch_accuracy: 25.61%	lr:0.000455
Ep: 16/25	It: 2701/4130	batch_loss: 4.3637	batch_accuracy: 25.20%	lr:0.000454
Ep: 16/25	It: 2751/4130	batch_loss: 4.2883	batch_accuracy: 26.17%	lr:0.000453
Ep: 16/25	It: 2801/4130	batch_loss: 4.3618	batch_accuracy: 25.71%	lr:0.000452
Ep: 16/25	It: 2851/4130	batch_loss: 4.2564	batch_accuracy: 26.76%	lr:0.000451
Ep: 16/25	It: 2901/4130	batch_loss: 4.4141	batch_accuracy: 24.73%	lr:0.000450
Ep: 16/25	It: 2951/4130	batch_loss: 4.3589	batch_accuracy: 26.05%	lr:0.000450
Ep: 16/25	It: 3001/4130	batch_loss: 4.4831	batch_accuracy: 23.90%	lr:0.000449
Ep: 16/25	It: 3051/4130	batch_loss: 4.4270	batch_accuracy: 24.02%	lr:0.000448
Ep: 16/25	It: 3101/4130	batch_loss: 4.3404	batch_accuracy: 25.07%	lr:0.000447
Ep: 16/25	It: 3151/4130	batch_loss: 4.3291	batch_accuracy: 25.49%	lr:0.000446
Ep: 16/25	It: 3201/4130	batch_loss: 4.3493	batch_accuracy: 25.42%	lr:0.000445
Ep: 16/25	It: 3251/4130	batch_loss: 4.3942	batch_accuracy: 24.76%	lr:0.000444
Ep: 16/25	It: 3301/4130	batch_loss: 4.4123	batch_accuracy: 24.56%	lr:0.000443
Ep: 16/25	It: 3351/4130	batch_loss: 4.4086	batch_accuracy: 24.88%	lr:0.000442
Ep: 16/25	It: 3401/4130	batch_loss: 4.3667	batch_accuracy: 25.05%	lr:0.000441
Ep: 16/25	It: 3451/4130	batch_loss: 4.3247	batch_accuracy: 25.90%	lr:0.000440
Ep: 16/25	It: 3501/4130	batch_loss: 4.4526	batch_accuracy: 23.41%	lr:0.000439
Ep: 16/25	It: 3551/4130	batch_loss: 4.3753	batch_accuracy: 25.37%	lr:0.000438
Ep: 16/25	It: 3601/4130	batch_loss: 4.5078	batch_accuracy: 23.34%	lr:0.000437
Ep: 16/25	It: 3651/4130	batch_loss: 4.3828	batch_accuracy: 24.78%	lr:0.000436
Ep: 16/25	It: 3701/4130	batch_loss: 4.3519	batch_accuracy: 26.49%	lr:0.000436
Ep: 16/25	It: 3751/4130	batch_loss: 4.3452	batch_accuracy: 26.71%	lr:0.000435
Ep: 16/25	It: 3801/4130	batch_loss: 4.2445	batch_accuracy: 27.22%	lr:0.000434
Ep: 16/25	It: 3851/4130	batch_loss: 4.3272	batch_accuracy: 25.63%	lr:0.000433
Ep: 16/25	It: 3901/4130	batch_loss: 4.3538	batch_accuracy: 26.54%	lr:0.000432
Ep: 16/25	It: 3951/4130	batch_loss: 4.3768	batch_accuracy: 24.54%	lr:0.000431
Ep: 16/25	It: 4001/4130	batch_loss: 4.2457	batch_accuracy: 26.83%	lr:0.000430
Ep: 16/25	It: 4051/4130	batch_loss: 4.3619	batch_accuracy: 25.42%	lr:0.000429
Ep: 16/25	It: 4101/4130	batch_loss: 4.3933	batch_accuracy: 24.22%	lr:0.000428
Ep: 16/25	It: 4130/4130	batch_loss: 4.4199	batch_accuracy: 24.23%	lr:0.000428


Generated text for input text "You" is:
Youvered. We have adipose tissue, that the most important factor of the CL-1 (n = 4) of the cells, the TH, and M-6, with a positive effector of 6. The expression of these cells toothalin was not in vitro. This was also shown to be in combination with the cytotoxicity of T cells in the T cells of SF-1 and TGF‐beta‐TNF‐α-γ and GTPase (SOX-5), but not for MCP and AA1, and that a protein-binding inhibitor.
<eot>
<sot>
Surface Tumor Base Bandon‐Loke-Piecewise in L-1.

The purpose of this study was to determine the effects of the expression of the enzyme‐binding protein kinase activity and the regulation of the IL-6 (T1D) receptor.


RESULTS
The expression of PF3 and TGF-γ-4-6 cells was observed in the immunosorbing enzyme. The expression of the IL-4-positive cells in the cell membrane and the phosphorylation of BP-1


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 17/25	It: 1/4130	batch_loss: 4.3962	batch_accuracy: 24.88%	lr:0.000428
Ep: 17/25	It: 51/4130	batch_loss: 4.3666	batch_accuracy: 25.83%	lr:0.000427
Ep: 17/25	It: 101/4130	batch_loss: 4.5173	batch_accuracy: 23.85%	lr:0.000426
Ep: 17/25	It: 151/4130	batch_loss: 4.3079	batch_accuracy: 26.05%	lr:0.000425
Ep: 17/25	It: 201/4130	batch_loss: 4.3428	batch_accuracy: 25.37%	lr:0.000424
Ep: 17/25	It: 251/4130	batch_loss: 4.3848	batch_accuracy: 25.07%	lr:0.000423
Ep: 17/25	It: 301/4130	batch_loss: 4.4275	batch_accuracy: 24.27%	lr:0.000422
Ep: 17/25	It: 351/4130	batch_loss: 4.4169	batch_accuracy: 24.54%	lr:0.000421
Ep: 17/25	It: 401/4130	batch_loss: 4.4002	batch_accuracy: 25.78%	lr:0.000420
Ep: 17/25	It: 451/4130	batch_loss: 4.4134	batch_accuracy: 24.44%	lr:0.000419
Ep: 17/25	It: 501/4130	batch_loss: 4.4317	batch_accuracy: 23.88%	lr:0.000418
Ep: 17/25	It: 551/4130	batch_loss: 4.4401	batch_accuracy: 24.10%	lr:0.000417
Ep: 17/25	It: 601/4130	batch_loss: 4.4616	batch_accuracy: 24.41%	lr:0.000416
Ep: 17/25	It: 651/4130	batch_loss: 4.3620	batch_accuracy: 25.59%	lr:0.000415
Ep: 17/25	It: 701/4130	batch_loss: 4.2500	batch_accuracy: 26.42%	lr:0.000415
Ep: 17/25	It: 751/4130	batch_loss: 4.4503	batch_accuracy: 24.44%	lr:0.000414
Ep: 17/25	It: 801/4130	batch_loss: 4.4004	batch_accuracy: 24.17%	lr:0.000413
Ep: 17/25	It: 851/4130	batch_loss: 4.3634	batch_accuracy: 25.49%	lr:0.000412
Ep: 17/25	It: 901/4130	batch_loss: 4.3153	batch_accuracy: 25.98%	lr:0.000411
Ep: 17/25	It: 951/4130	batch_loss: 4.3590	batch_accuracy: 25.59%	lr:0.000410
Ep: 17/25	It: 1001/4130	batch_loss: 4.4401	batch_accuracy: 24.37%	lr:0.000409
Ep: 17/25	It: 1051/4130	batch_loss: 4.4752	batch_accuracy: 24.95%	lr:0.000408
Ep: 17/25	It: 1101/4130	batch_loss: 4.3885	batch_accuracy: 25.24%	lr:0.000407
Ep: 17/25	It: 1151/4130	batch_loss: 4.4529	batch_accuracy: 23.24%	lr:0.000406
Ep: 17/25	It: 1201/4130	batch_loss: 4.2728	batch_accuracy: 25.59%	lr:0.000405
Ep: 17/25	It: 1251/4130	batch_loss: 4.3056	batch_accuracy: 25.49%	lr:0.000404
Ep: 17/25	It: 1301/4130	batch_loss: 4.4201	batch_accuracy: 24.80%	lr:0.000403
Ep: 17/25	It: 1351/4130	batch_loss: 4.3532	batch_accuracy: 25.46%	lr:0.000403
Ep: 17/25	It: 1401/4130	batch_loss: 4.3946	batch_accuracy: 23.36%	lr:0.000402
Ep: 17/25	It: 1451/4130	batch_loss: 4.3829	batch_accuracy: 25.17%	lr:0.000401
Ep: 17/25	It: 1501/4130	batch_loss: 4.4139	batch_accuracy: 25.37%	lr:0.000400
Ep: 17/25	It: 1551/4130	batch_loss: 4.3945	batch_accuracy: 24.54%	lr:0.000399
Ep: 17/25	It: 1601/4130	batch_loss: 4.3270	batch_accuracy: 26.32%	lr:0.000398
Ep: 17/25	It: 1651/4130	batch_loss: 4.3719	batch_accuracy: 25.29%	lr:0.000397
Ep: 17/25	It: 1701/4130	batch_loss: 4.5173	batch_accuracy: 23.56%	lr:0.000396
Ep: 17/25	It: 1751/4130	batch_loss: 4.3714	batch_accuracy: 24.63%	lr:0.000395
Ep: 17/25	It: 1801/4130	batch_loss: 4.3742	batch_accuracy: 26.37%	lr:0.000394
Ep: 17/25	It: 1851/4130	batch_loss: 4.3157	batch_accuracy: 25.83%	lr:0.000393
Ep: 17/25	It: 1901/4130	batch_loss: 4.2377	batch_accuracy: 25.81%	lr:0.000392
Ep: 17/25	It: 1951/4130	batch_loss: 4.4125	batch_accuracy: 24.88%	lr:0.000392
Ep: 17/25	It: 2001/4130	batch_loss: 4.3325	batch_accuracy: 25.85%	lr:0.000391
Ep: 17/25	It: 2051/4130	batch_loss: 4.3623	batch_accuracy: 25.37%	lr:0.000390
Ep: 17/25	It: 2101/4130	batch_loss: 4.4674	batch_accuracy: 24.10%	lr:0.000389
Ep: 17/25	It: 2151/4130	batch_loss: 4.2633	batch_accuracy: 26.49%	lr:0.000388
Ep: 17/25	It: 2201/4130	batch_loss: 4.3568	batch_accuracy: 26.76%	lr:0.000387
Ep: 17/25	It: 2251/4130	batch_loss: 4.3669	batch_accuracy: 25.98%	lr:0.000386
Ep: 17/25	It: 2301/4130	batch_loss: 4.3033	batch_accuracy: 26.46%	lr:0.000385
Ep: 17/25	It: 2351/4130	batch_loss: 4.5469	batch_accuracy: 23.56%	lr:0.000384
Ep: 17/25	It: 2401/4130	batch_loss: 4.3199	batch_accuracy: 25.34%	lr:0.000383
Ep: 17/25	It: 2451/4130	batch_loss: 4.5002	batch_accuracy: 23.80%	lr:0.000382
Ep: 17/25	It: 2501/4130	batch_loss: 4.3251	batch_accuracy: 26.00%	lr:0.000381
Ep: 17/25	It: 2551/4130	batch_loss: 4.4983	batch_accuracy: 23.66%	lr:0.000381
Ep: 17/25	It: 2601/4130	batch_loss: 4.3978	batch_accuracy: 25.22%	lr:0.000380
Ep: 17/25	It: 2651/4130	batch_loss: 4.3693	batch_accuracy: 25.61%	lr:0.000379
Ep: 17/25	It: 2701/4130	batch_loss: 4.4813	batch_accuracy: 24.27%	lr:0.000378
Ep: 17/25	It: 2751/4130	batch_loss: 4.3903	batch_accuracy: 24.41%	lr:0.000377
Ep: 17/25	It: 2801/4130	batch_loss: 4.3879	batch_accuracy: 25.76%	lr:0.000376
Ep: 17/25	It: 2851/4130	batch_loss: 4.3867	batch_accuracy: 25.73%	lr:0.000375
Ep: 17/25	It: 2901/4130	batch_loss: 4.4072	batch_accuracy: 24.37%	lr:0.000374
Ep: 17/25	It: 2951/4130	batch_loss: 4.4094	batch_accuracy: 24.10%	lr:0.000373
Ep: 17/25	It: 3001/4130	batch_loss: 4.3253	batch_accuracy: 24.19%	lr:0.000372
Ep: 17/25	It: 3051/4130	batch_loss: 4.5022	batch_accuracy: 24.00%	lr:0.000371
Ep: 17/25	It: 3101/4130	batch_loss: 4.2752	batch_accuracy: 25.76%	lr:0.000371
Ep: 17/25	It: 3151/4130	batch_loss: 4.3414	batch_accuracy: 26.17%	lr:0.000370
Ep: 17/25	It: 3201/4130	batch_loss: 4.2526	batch_accuracy: 26.90%	lr:0.000369
Ep: 17/25	It: 3251/4130	batch_loss: 4.4525	batch_accuracy: 24.34%	lr:0.000368
Ep: 17/25	It: 3301/4130	batch_loss: 4.2139	batch_accuracy: 27.47%	lr:0.000367
Ep: 17/25	It: 3351/4130	batch_loss: 4.3437	batch_accuracy: 24.66%	lr:0.000366
Ep: 17/25	It: 3401/4130	batch_loss: 4.4200	batch_accuracy: 24.29%	lr:0.000365
Ep: 17/25	It: 3451/4130	batch_loss: 4.4589	batch_accuracy: 24.15%	lr:0.000364
Ep: 17/25	It: 3501/4130	batch_loss: 4.5108	batch_accuracy: 23.85%	lr:0.000363
Ep: 17/25	It: 3551/4130	batch_loss: 4.3076	batch_accuracy: 25.12%	lr:0.000362
Ep: 17/25	It: 3601/4130	batch_loss: 4.3759	batch_accuracy: 25.24%	lr:0.000362
Ep: 17/25	It: 3651/4130	batch_loss: 4.4454	batch_accuracy: 24.76%	lr:0.000361
Ep: 17/25	It: 3701/4130	batch_loss: 4.4056	batch_accuracy: 23.95%	lr:0.000360
Ep: 17/25	It: 3751/4130	batch_loss: 4.3163	batch_accuracy: 26.71%	lr:0.000359
Ep: 17/25	It: 3801/4130	batch_loss: 4.2741	batch_accuracy: 26.12%	lr:0.000358
Ep: 17/25	It: 3851/4130	batch_loss: 4.3130	batch_accuracy: 26.95%	lr:0.000357
Ep: 17/25	It: 3901/4130	batch_loss: 4.1941	batch_accuracy: 26.49%	lr:0.000356
Ep: 17/25	It: 3951/4130	batch_loss: 4.4151	batch_accuracy: 24.63%	lr:0.000355
Ep: 17/25	It: 4001/4130	batch_loss: 4.3355	batch_accuracy: 26.03%	lr:0.000354
Ep: 17/25	It: 4051/4130	batch_loss: 4.3108	batch_accuracy: 25.46%	lr:0.000353
Ep: 17/25	It: 4101/4130	batch_loss: 4.3456	batch_accuracy: 26.15%	lr:0.000353
Ep: 17/25	It: 4130/4130	batch_loss: 4.3853	batch_accuracy: 25.19%	lr:0.000352


Generated text for input text "You" is:
Youzard, and came, and the sandu, as the basis of his and aui. The author's owner's book's book and the ‘n’. Au’,” is inade to do not be the ‘n’. The author’ (200s, the Journal of the Language and Language (p. P. B., S., 2002; M, P. Massa, Music, J. K. Political, and C. Partin, 2002, and 2009; Family, 1993). The book, the author's knowledge and the most significant challenges for the health, in the past two years of research, and the most of the research are to be a major cause of a large area. In order to determine the effects of their ownership and their owners in the world. The book was not well known as a means of research that are also an alternative in the world. This is based on the results of an overview of the social context.
<eot>
<sot>
[Analysis of Ethical Epidemiology: The Error Countries of the Policy (Dr


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 18/25	It: 1/4130	batch_loss: 4.5272	batch_accuracy: 23.27%	lr:0.000352
Ep: 18/25	It: 51/4130	batch_loss: 4.3527	batch_accuracy: 26.17%	lr:0.000351
Ep: 18/25	It: 101/4130	batch_loss: 4.5062	batch_accuracy: 24.17%	lr:0.000350
Ep: 18/25	It: 151/4130	batch_loss: 4.4627	batch_accuracy: 24.56%	lr:0.000349
Ep: 18/25	It: 201/4130	batch_loss: 4.3852	batch_accuracy: 26.07%	lr:0.000348
Ep: 18/25	It: 251/4130	batch_loss: 4.4039	batch_accuracy: 24.22%	lr:0.000348
Ep: 18/25	It: 301/4130	batch_loss: 4.3421	batch_accuracy: 25.59%	lr:0.000347
Ep: 18/25	It: 351/4130	batch_loss: 4.1731	batch_accuracy: 27.73%	lr:0.000346
Ep: 18/25	It: 401/4130	batch_loss: 4.3146	batch_accuracy: 25.46%	lr:0.000345
Ep: 18/25	It: 451/4130	batch_loss: 4.3893	batch_accuracy: 24.73%	lr:0.000344
Ep: 18/25	It: 501/4130	batch_loss: 4.3191	batch_accuracy: 25.46%	lr:0.000343
Ep: 18/25	It: 551/4130	batch_loss: 4.3195	batch_accuracy: 25.61%	lr:0.000342
Ep: 18/25	It: 601/4130	batch_loss: 4.4536	batch_accuracy: 23.93%	lr:0.000341
Ep: 18/25	It: 651/4130	batch_loss: 4.4027	batch_accuracy: 24.73%	lr:0.000340
Ep: 18/25	It: 701/4130	batch_loss: 4.3849	batch_accuracy: 24.37%	lr:0.000340
Ep: 18/25	It: 751/4130	batch_loss: 4.3062	batch_accuracy: 25.32%	lr:0.000339
Ep: 18/25	It: 801/4130	batch_loss: 4.4138	batch_accuracy: 24.00%	lr:0.000338
Ep: 18/25	It: 851/4130	batch_loss: 4.2885	batch_accuracy: 26.86%	lr:0.000337
Ep: 18/25	It: 901/4130	batch_loss: 4.4172	batch_accuracy: 25.20%	lr:0.000336
Ep: 18/25	It: 951/4130	batch_loss: 4.2980	batch_accuracy: 26.54%	lr:0.000335
Ep: 18/25	It: 1001/4130	batch_loss: 4.4535	batch_accuracy: 24.71%	lr:0.000334
Ep: 18/25	It: 1051/4130	batch_loss: 4.3026	batch_accuracy: 25.17%	lr:0.000333
Ep: 18/25	It: 1101/4130	batch_loss: 4.3044	batch_accuracy: 27.47%	lr:0.000332
Ep: 18/25	It: 1151/4130	batch_loss: 4.2934	batch_accuracy: 25.56%	lr:0.000332
Ep: 18/25	It: 1201/4130	batch_loss: 4.2832	batch_accuracy: 26.59%	lr:0.000331
Ep: 18/25	It: 1251/4130	batch_loss: 4.4221	batch_accuracy: 24.93%	lr:0.000330
Ep: 18/25	It: 1301/4130	batch_loss: 4.3116	batch_accuracy: 25.32%	lr:0.000329
Ep: 18/25	It: 1351/4130	batch_loss: 4.3773	batch_accuracy: 25.07%	lr:0.000328
Ep: 18/25	It: 1401/4130	batch_loss: 4.2898	batch_accuracy: 26.71%	lr:0.000327
Ep: 18/25	It: 1451/4130	batch_loss: 4.4005	batch_accuracy: 26.27%	lr:0.000326
Ep: 18/25	It: 1501/4130	batch_loss: 4.3744	batch_accuracy: 25.20%	lr:0.000325
Ep: 18/25	It: 1551/4130	batch_loss: 4.4485	batch_accuracy: 25.34%	lr:0.000325
Ep: 18/25	It: 1601/4130	batch_loss: 4.2446	batch_accuracy: 26.73%	lr:0.000324
Ep: 18/25	It: 1651/4130	batch_loss: 4.2797	batch_accuracy: 26.03%	lr:0.000323
Ep: 18/25	It: 1701/4130	batch_loss: 4.4637	batch_accuracy: 24.05%	lr:0.000322
Ep: 18/25	It: 1751/4130	batch_loss: 4.4373	batch_accuracy: 24.98%	lr:0.000321
Ep: 18/25	It: 1801/4130	batch_loss: 4.4094	batch_accuracy: 26.39%	lr:0.000320
Ep: 18/25	It: 1851/4130	batch_loss: 4.2898	batch_accuracy: 26.76%	lr:0.000319
Ep: 18/25	It: 1901/4130	batch_loss: 4.3676	batch_accuracy: 26.17%	lr:0.000318
Ep: 18/25	It: 1951/4130	batch_loss: 4.3547	batch_accuracy: 25.61%	lr:0.000318
Ep: 18/25	It: 2001/4130	batch_loss: 4.3182	batch_accuracy: 25.68%	lr:0.000317
Ep: 18/25	It: 2051/4130	batch_loss: 4.3571	batch_accuracy: 25.88%	lr:0.000316
Ep: 18/25	It: 2101/4130	batch_loss: 4.2890	batch_accuracy: 25.32%	lr:0.000315
Ep: 18/25	It: 2151/4130	batch_loss: 4.5160	batch_accuracy: 24.17%	lr:0.000314
Ep: 18/25	It: 2201/4130	batch_loss: 4.3724	batch_accuracy: 24.98%	lr:0.000313
Ep: 18/25	It: 2251/4130	batch_loss: 4.2757	batch_accuracy: 26.34%	lr:0.000312
Ep: 18/25	It: 2301/4130	batch_loss: 4.4254	batch_accuracy: 24.76%	lr:0.000311
Ep: 18/25	It: 2351/4130	batch_loss: 4.2362	batch_accuracy: 25.39%	lr:0.000311
Ep: 18/25	It: 2401/4130	batch_loss: 4.3034	batch_accuracy: 26.49%	lr:0.000310
Ep: 18/25	It: 2451/4130	batch_loss: 4.3329	batch_accuracy: 26.20%	lr:0.000309
Ep: 18/25	It: 2501/4130	batch_loss: 4.2503	batch_accuracy: 26.42%	lr:0.000308
Ep: 18/25	It: 2551/4130	batch_loss: 4.4577	batch_accuracy: 23.46%	lr:0.000307
Ep: 18/25	It: 2601/4130	batch_loss: 4.4401	batch_accuracy: 24.24%	lr:0.000306
Ep: 18/25	It: 2651/4130	batch_loss: 4.3188	batch_accuracy: 24.90%	lr:0.000305
Ep: 18/25	It: 2701/4130	batch_loss: 4.3630	batch_accuracy: 26.39%	lr:0.000305
Ep: 18/25	It: 2751/4130	batch_loss: 4.2873	batch_accuracy: 26.17%	lr:0.000304
Ep: 18/25	It: 2801/4130	batch_loss: 4.3469	batch_accuracy: 26.86%	lr:0.000303
Ep: 18/25	It: 2851/4130	batch_loss: 4.3523	batch_accuracy: 25.42%	lr:0.000302
Ep: 18/25	It: 2901/4130	batch_loss: 4.3160	batch_accuracy: 25.68%	lr:0.000301
Ep: 18/25	It: 2951/4130	batch_loss: 4.4152	batch_accuracy: 25.02%	lr:0.000300
Ep: 18/25	It: 3001/4130	batch_loss: 4.3705	batch_accuracy: 25.29%	lr:0.000299
Ep: 18/25	It: 3051/4130	batch_loss: 4.4053	batch_accuracy: 26.29%	lr:0.000299
Ep: 18/25	It: 3101/4130	batch_loss: 4.2698	batch_accuracy: 24.58%	lr:0.000298
Ep: 18/25	It: 3151/4130	batch_loss: 4.4380	batch_accuracy: 24.05%	lr:0.000297
Ep: 18/25	It: 3201/4130	batch_loss: 4.2214	batch_accuracy: 27.47%	lr:0.000296
Ep: 18/25	It: 3251/4130	batch_loss: 4.4102	batch_accuracy: 24.15%	lr:0.000295
Ep: 18/25	It: 3301/4130	batch_loss: 4.2500	batch_accuracy: 26.34%	lr:0.000294
Ep: 18/25	It: 3351/4130	batch_loss: 4.3684	batch_accuracy: 25.63%	lr:0.000293
Ep: 18/25	It: 3401/4130	batch_loss: 4.3869	batch_accuracy: 25.95%	lr:0.000293
Ep: 18/25	It: 3451/4130	batch_loss: 4.3722	batch_accuracy: 24.76%	lr:0.000292
Ep: 18/25	It: 3501/4130	batch_loss: 4.3179	batch_accuracy: 25.46%	lr:0.000291
Ep: 18/25	It: 3551/4130	batch_loss: 4.3247	batch_accuracy: 24.68%	lr:0.000290
Ep: 18/25	It: 3601/4130	batch_loss: 4.2930	batch_accuracy: 24.07%	lr:0.000289
Ep: 18/25	It: 3651/4130	batch_loss: 4.4020	batch_accuracy: 24.17%	lr:0.000288
Ep: 18/25	It: 3701/4130	batch_loss: 4.3373	batch_accuracy: 25.27%	lr:0.000287
Ep: 18/25	It: 3751/4130	batch_loss: 4.2419	batch_accuracy: 26.98%	lr:0.000287
Ep: 18/25	It: 3801/4130	batch_loss: 4.3649	batch_accuracy: 24.56%	lr:0.000286
Ep: 18/25	It: 3851/4130	batch_loss: 4.4178	batch_accuracy: 25.24%	lr:0.000285
Ep: 18/25	It: 3901/4130	batch_loss: 4.3362	batch_accuracy: 25.12%	lr:0.000284
Ep: 18/25	It: 3951/4130	batch_loss: 4.2935	batch_accuracy: 24.95%	lr:0.000283
Ep: 18/25	It: 4001/4130	batch_loss: 4.3541	batch_accuracy: 24.98%	lr:0.000282
Ep: 18/25	It: 4051/4130	batch_loss: 4.3862	batch_accuracy: 25.54%	lr:0.000282
Ep: 18/25	It: 4101/4130	batch_loss: 4.3239	batch_accuracy: 25.27%	lr:0.000281
Ep: 18/25	It: 4130/4130	batch_loss: 4.4384	batch_accuracy: 25.77%	lr:0.000280


Generated text for input text "You" is:
Youzizi, p. (201)


This paper examines the potential of the nature of theories, theth, in-han and fairly.

RESULTS
The prevalence of the most unified. We are azepin of which had the highest percentage of the children who experienced a significant increase in the onset of depression and mortality rate. There was no significant difference in the overall outcome. A total of 79 patients (8.6%) in the control group (18.8%), of the age of age group (5.3%) with S. pneumoniae (9.7%) had a significant difference (p < 0.001) and in a group of 83.2% (0.6%) were significantly higher in the P. pneumonia (0.5%) and non-Euccinoma-related disease (0.8%) than those (2.5%) of the cases. The incidence of the treatment is a common cause of the disease was associated with a significant difference in the efficacy of C. budget. The incidence of the left ventricular cortex was observed in the left and right ventricles, but the number of patients had an adverse effect on a dose of


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 19/25	It: 1/4130	batch_loss: 4.3074	batch_accuracy: 26.51%	lr:0.000280
Ep: 19/25	It: 51/4130	batch_loss: 4.3008	batch_accuracy: 26.46%	lr:0.000279
Ep: 19/25	It: 101/4130	batch_loss: 4.3594	batch_accuracy: 26.81%	lr:0.000279
Ep: 19/25	It: 151/4130	batch_loss: 4.3871	batch_accuracy: 24.90%	lr:0.000278
Ep: 19/25	It: 201/4130	batch_loss: 4.3597	batch_accuracy: 24.85%	lr:0.000277
Ep: 19/25	It: 251/4130	batch_loss: 4.4347	batch_accuracy: 24.49%	lr:0.000276
Ep: 19/25	It: 301/4130	batch_loss: 4.3489	batch_accuracy: 26.20%	lr:0.000275
Ep: 19/25	It: 351/4130	batch_loss: 4.3089	batch_accuracy: 26.42%	lr:0.000274
Ep: 19/25	It: 401/4130	batch_loss: 4.2818	batch_accuracy: 27.39%	lr:0.000274
Ep: 19/25	It: 451/4130	batch_loss: 4.2804	batch_accuracy: 26.17%	lr:0.000273
Ep: 19/25	It: 501/4130	batch_loss: 4.3702	batch_accuracy: 24.95%	lr:0.000272
Ep: 19/25	It: 551/4130	batch_loss: 4.4684	batch_accuracy: 22.85%	lr:0.000271
Ep: 19/25	It: 601/4130	batch_loss: 4.4400	batch_accuracy: 24.61%	lr:0.000270
Ep: 19/25	It: 651/4130	batch_loss: 4.4164	batch_accuracy: 24.63%	lr:0.000269
Ep: 19/25	It: 701/4130	batch_loss: 4.4284	batch_accuracy: 24.29%	lr:0.000269
Ep: 19/25	It: 751/4130	batch_loss: 4.3429	batch_accuracy: 25.54%	lr:0.000268
Ep: 19/25	It: 801/4130	batch_loss: 4.3296	batch_accuracy: 25.24%	lr:0.000267
Ep: 19/25	It: 851/4130	batch_loss: 4.3259	batch_accuracy: 25.51%	lr:0.000266
Ep: 19/25	It: 901/4130	batch_loss: 4.3888	batch_accuracy: 23.49%	lr:0.000265
Ep: 19/25	It: 951/4130	batch_loss: 4.3780	batch_accuracy: 25.27%	lr:0.000264
Ep: 19/25	It: 1001/4130	batch_loss: 4.3899	batch_accuracy: 24.39%	lr:0.000264
Ep: 19/25	It: 1051/4130	batch_loss: 4.4481	batch_accuracy: 24.85%	lr:0.000263
Ep: 19/25	It: 1101/4130	batch_loss: 4.4189	batch_accuracy: 24.41%	lr:0.000262
Ep: 19/25	It: 1151/4130	batch_loss: 4.3380	batch_accuracy: 25.34%	lr:0.000261
Ep: 19/25	It: 1201/4130	batch_loss: 4.3848	batch_accuracy: 24.83%	lr:0.000260
Ep: 19/25	It: 1251/4130	batch_loss: 4.3853	batch_accuracy: 25.81%	lr:0.000260
Ep: 19/25	It: 1301/4130	batch_loss: 4.3302	batch_accuracy: 27.00%	lr:0.000259
Ep: 19/25	It: 1351/4130	batch_loss: 4.3356	batch_accuracy: 25.39%	lr:0.000258
Ep: 19/25	It: 1401/4130	batch_loss: 4.4003	batch_accuracy: 23.78%	lr:0.000257
Ep: 19/25	It: 1451/4130	batch_loss: 4.2769	batch_accuracy: 25.71%	lr:0.000256
Ep: 19/25	It: 1501/4130	batch_loss: 4.4022	batch_accuracy: 24.44%	lr:0.000255
Ep: 19/25	It: 1551/4130	batch_loss: 4.3597	batch_accuracy: 24.80%	lr:0.000255
Ep: 19/25	It: 1601/4130	batch_loss: 4.4170	batch_accuracy: 24.41%	lr:0.000254
Ep: 19/25	It: 1651/4130	batch_loss: 4.3569	batch_accuracy: 24.24%	lr:0.000253
Ep: 19/25	It: 1701/4130	batch_loss: 4.3546	batch_accuracy: 24.54%	lr:0.000252
Ep: 19/25	It: 1751/4130	batch_loss: 4.3317	batch_accuracy: 24.80%	lr:0.000251
Ep: 19/25	It: 1801/4130	batch_loss: 4.4543	batch_accuracy: 23.14%	lr:0.000251
Ep: 19/25	It: 1851/4130	batch_loss: 4.4010	batch_accuracy: 23.61%	lr:0.000250
Ep: 19/25	It: 1901/4130	batch_loss: 4.3295	batch_accuracy: 25.12%	lr:0.000249
Ep: 19/25	It: 1951/4130	batch_loss: 4.2998	batch_accuracy: 25.90%	lr:0.000248
Ep: 19/25	It: 2001/4130	batch_loss: 4.3651	batch_accuracy: 24.73%	lr:0.000247
Ep: 19/25	It: 2051/4130	batch_loss: 4.3737	batch_accuracy: 24.98%	lr:0.000247
Ep: 19/25	It: 2101/4130	batch_loss: 4.3455	batch_accuracy: 25.76%	lr:0.000246
Ep: 19/25	It: 2151/4130	batch_loss: 4.3018	batch_accuracy: 26.49%	lr:0.000245
Ep: 19/25	It: 2201/4130	batch_loss: 4.1556	batch_accuracy: 28.93%	lr:0.000244
Ep: 19/25	It: 2251/4130	batch_loss: 4.1909	batch_accuracy: 28.17%	lr:0.000243
Ep: 19/25	It: 2301/4130	batch_loss: 4.3548	batch_accuracy: 23.93%	lr:0.000243
Ep: 19/25	It: 2351/4130	batch_loss: 4.2481	batch_accuracy: 26.17%	lr:0.000242
Ep: 19/25	It: 2401/4130	batch_loss: 4.3806	batch_accuracy: 25.51%	lr:0.000241
Ep: 19/25	It: 2451/4130	batch_loss: 4.4566	batch_accuracy: 24.00%	lr:0.000240
Ep: 19/25	It: 2501/4130	batch_loss: 4.3082	batch_accuracy: 25.22%	lr:0.000239
Ep: 19/25	It: 2551/4130	batch_loss: 4.2942	batch_accuracy: 26.42%	lr:0.000239
Ep: 19/25	It: 2601/4130	batch_loss: 4.3625	batch_accuracy: 24.95%	lr:0.000238
Ep: 19/25	It: 2651/4130	batch_loss: 4.2892	batch_accuracy: 27.59%	lr:0.000237
Ep: 19/25	It: 2701/4130	batch_loss: 4.5158	batch_accuracy: 24.10%	lr:0.000236
Ep: 19/25	It: 2751/4130	batch_loss: 4.3610	batch_accuracy: 25.78%	lr:0.000235
Ep: 19/25	It: 2801/4130	batch_loss: 4.3277	batch_accuracy: 25.78%	lr:0.000235
Ep: 19/25	It: 2851/4130	batch_loss: 4.3045	batch_accuracy: 25.83%	lr:0.000234
Ep: 19/25	It: 2901/4130	batch_loss: 4.2551	batch_accuracy: 26.76%	lr:0.000233
Ep: 19/25	It: 2951/4130	batch_loss: 4.1386	batch_accuracy: 27.22%	lr:0.000232
Ep: 19/25	It: 3001/4130	batch_loss: 4.4195	batch_accuracy: 25.42%	lr:0.000232
Ep: 19/25	It: 3051/4130	batch_loss: 4.4153	batch_accuracy: 25.44%	lr:0.000231
Ep: 19/25	It: 3101/4130	batch_loss: 4.5017	batch_accuracy: 25.76%	lr:0.000230
Ep: 19/25	It: 3151/4130	batch_loss: 4.2946	batch_accuracy: 25.10%	lr:0.000229
Ep: 19/25	It: 3201/4130	batch_loss: 4.2140	batch_accuracy: 26.54%	lr:0.000228
Ep: 19/25	It: 3251/4130	batch_loss: 4.5405	batch_accuracy: 23.02%	lr:0.000228
Ep: 19/25	It: 3301/4130	batch_loss: 4.3769	batch_accuracy: 24.85%	lr:0.000227
Ep: 19/25	It: 3351/4130	batch_loss: 4.4431	batch_accuracy: 25.27%	lr:0.000226
Ep: 19/25	It: 3401/4130	batch_loss: 4.3507	batch_accuracy: 26.54%	lr:0.000225
Ep: 19/25	It: 3451/4130	batch_loss: 4.3835	batch_accuracy: 25.73%	lr:0.000224
Ep: 19/25	It: 3501/4130	batch_loss: 4.2821	batch_accuracy: 25.68%	lr:0.000224
Ep: 19/25	It: 3551/4130	batch_loss: 4.1959	batch_accuracy: 27.34%	lr:0.000223
Ep: 19/25	It: 3601/4130	batch_loss: 4.3987	batch_accuracy: 24.68%	lr:0.000222
Ep: 19/25	It: 3651/4130	batch_loss: 4.3688	batch_accuracy: 25.20%	lr:0.000221
Ep: 19/25	It: 3701/4130	batch_loss: 4.2749	batch_accuracy: 27.05%	lr:0.000221
Ep: 19/25	It: 3751/4130	batch_loss: 4.3574	batch_accuracy: 25.51%	lr:0.000220
Ep: 19/25	It: 3801/4130	batch_loss: 4.4401	batch_accuracy: 25.46%	lr:0.000219
Ep: 19/25	It: 3851/4130	batch_loss: 4.3970	batch_accuracy: 25.05%	lr:0.000218
Ep: 19/25	It: 3901/4130	batch_loss: 4.4061	batch_accuracy: 24.90%	lr:0.000218
Ep: 19/25	It: 3951/4130	batch_loss: 4.3336	batch_accuracy: 24.95%	lr:0.000217
Ep: 19/25	It: 4001/4130	batch_loss: 4.3584	batch_accuracy: 25.39%	lr:0.000216
Ep: 19/25	It: 4051/4130	batch_loss: 4.3287	batch_accuracy: 25.81%	lr:0.000215
Ep: 19/25	It: 4101/4130	batch_loss: 4.2552	batch_accuracy: 26.00%	lr:0.000214
Ep: 19/25	It: 4130/4130	batch_loss: 4.2626	batch_accuracy: 26.08%	lr:0.000214


Generated text for input text "You" is:
Youis. The results indicate that it may increase in their ability to be used for the same group, the use of the data.

CONCLUSION
There-fish-costeriors of the most important aspects officially available to evaluate the effectiveness of the proposed algorithm. The model was used to evaluate the performance of the proposed method to identify the performance and accuracy of the proposed method.
<eot>
<sot>
The Risk of Council-Selection in a Petri-Association of Salt Partz.

Accurate-based model for a high-quality-based approach for a large number of applications. We also demonstrate that these factors for the detection of new technologies for this purpose of this paper can be considered for the development of smart grids, and the design of this work in the process of communication.
<eot>
<sot>
Evaluation of the Aerial Sequence and Long Women and Maintenance

Abstract: We also show that in the use of an online approach is the most important issue. The article argues that the proposed method is the most common problem of a set of data from the paper


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
Ep: 20/25	It: 1/4130	batch_loss: 4.2835	batch_accuracy: 26.42%	lr:0.000214
Ep: 20/25	It: 51/4130	batch_loss: 4.3342	batch_accuracy: 26.32%	lr:0.000213
Ep: 20/25	It: 101/4130	batch_loss: 4.4211	batch_accuracy: 25.12%	lr:0.000213
Ep: 20/25	It: 151/4130	batch_loss: 4.3941	batch_accuracy: 24.66%	lr:0.000212
Ep: 20/25	It: 201/4130	batch_loss: 4.3340	batch_accuracy: 25.68%	lr:0.000211
Ep: 20/25	It: 251/4130	batch_loss: 4.3384	batch_accuracy: 25.27%	lr:0.000210
Ep: 20/25	It: 301/4130	batch_loss: 4.3160	batch_accuracy: 25.78%	lr:0.000209
Ep: 20/25	It: 351/4130	batch_loss: 4.2473	batch_accuracy: 27.17%	lr:0.000209
Ep: 20/25	It: 401/4130	batch_loss: 4.4285	batch_accuracy: 24.90%	lr:0.000208
Ep: 20/25	It: 451/4130	batch_loss: 4.3309	batch_accuracy: 26.76%	lr:0.000207
Ep: 20/25	It: 501/4130	batch_loss: 4.2949	batch_accuracy: 26.22%	lr:0.000206
Ep: 20/25	It: 551/4130	batch_loss: 4.3649	batch_accuracy: 26.86%	lr:0.000206
Ep: 20/25	It: 601/4130	batch_loss: 4.3656	batch_accuracy: 25.59%	lr:0.000205
Ep: 20/25	It: 651/4130	batch_loss: 4.3101	batch_accuracy: 24.80%	lr:0.000204
Ep: 20/25	It: 701/4130	batch_loss: 4.3785	batch_accuracy: 25.32%	lr:0.000203
Ep: 20/25	It: 751/4130	batch_loss: 4.3855	batch_accuracy: 24.19%	lr:0.000203
Ep: 20/25	It: 801/4130	batch_loss: 4.3131	batch_accuracy: 25.42%	lr:0.000202
Ep: 20/25	It: 851/4130	batch_loss: 4.3529	batch_accuracy: 25.49%	lr:0.000201
Ep: 20/25	It: 901/4130	batch_loss: 4.3772	batch_accuracy: 25.12%	lr:0.000200
Ep: 20/25	It: 951/4130	batch_loss: 4.3359	batch_accuracy: 25.88%	lr:0.000200
Ep: 20/25	It: 1001/4130	batch_loss: 4.4681	batch_accuracy: 23.22%	lr:0.000199
Ep: 20/25	It: 1051/4130	batch_loss: 4.2721	batch_accuracy: 26.10%	lr:0.000198
Ep: 20/25	It: 1101/4130	batch_loss: 4.2877	batch_accuracy: 26.81%	lr:0.000198
Ep: 20/25	It: 1151/4130	batch_loss: 4.2511	batch_accuracy: 26.78%	lr:0.000197
Ep: 20/25	It: 1201/4130	batch_loss: 4.3955	batch_accuracy: 24.32%	lr:0.000196
Ep: 20/25	It: 1251/4130	batch_loss: 4.3108	batch_accuracy: 25.85%	lr:0.000195
Ep: 20/25	It: 1301/4130	batch_loss: 4.3606	batch_accuracy: 25.22%	lr:0.000195
Ep: 20/25	It: 1351/4130	batch_loss: 4.2366	batch_accuracy: 26.37%	lr:0.000194
Ep: 20/25	It: 1401/4130	batch_loss: 4.3250	batch_accuracy: 25.63%	lr:0.000193
Ep: 20/25	It: 1451/4130	batch_loss: 4.3739	batch_accuracy: 26.07%	lr:0.000192
Ep: 20/25	It: 1501/4130	batch_loss: 4.3012	batch_accuracy: 25.68%	lr:0.000192
Ep: 20/25	It: 1551/4130	batch_loss: 4.3044	batch_accuracy: 24.07%	lr:0.000191
Ep: 20/25	It: 1601/4130	batch_loss: 4.4307	batch_accuracy: 24.05%	lr:0.000190
Ep: 20/25	It: 1651/4130	batch_loss: 4.3985	batch_accuracy: 24.44%	lr:0.000189
Ep: 20/25	It: 1701/4130	batch_loss: 4.2913	batch_accuracy: 24.51%	lr:0.000189
Ep: 20/25	It: 1751/4130	batch_loss: 4.4275	batch_accuracy: 25.93%	lr:0.000188
Ep: 20/25	It: 1801/4130	batch_loss: 4.2134	batch_accuracy: 26.56%	lr:0.000187
Ep: 20/25	It: 1851/4130	batch_loss: 4.3019	batch_accuracy: 25.49%	lr:0.000187
Ep: 20/25	It: 1901/4130	batch_loss: 4.3575	batch_accuracy: 24.63%	lr:0.000186
Ep: 20/25	It: 1951/4130	batch_loss: 4.2733	batch_accuracy: 26.86%	lr:0.000185
Ep: 20/25	It: 2001/4130	batch_loss: 4.3181	batch_accuracy: 25.73%	lr:0.000184
Ep: 20/25	It: 2051/4130	batch_loss: 4.3628	batch_accuracy: 25.85%	lr:0.000184
Ep: 20/25	It: 2101/4130	batch_loss: 4.3543	batch_accuracy: 26.07%	lr:0.000183
Ep: 20/25	It: 2151/4130	batch_loss: 4.2369	batch_accuracy: 27.22%	lr:0.000182
Ep: 20/25	It: 2201/4130	batch_loss: 4.3897	batch_accuracy: 24.76%	lr:0.000182
Ep: 20/25	It: 2251/4130	batch_loss: 4.2725	batch_accuracy: 26.15%	lr:0.000181
Ep: 20/25	It: 2301/4130	batch_loss: 4.4058	batch_accuracy: 24.83%	lr:0.000180
Ep: 20/25	It: 2351/4130	batch_loss: 4.3671	batch_accuracy: 25.07%	lr:0.000179
Ep: 20/25	It: 2401/4130	batch_loss: 4.3497	batch_accuracy: 24.83%	lr:0.000179
Ep: 20/25	It: 2451/4130	batch_loss: 4.2225	batch_accuracy: 26.78%	lr:0.000178
Ep: 20/25	It: 2501/4130	batch_loss: 4.3824	batch_accuracy: 26.44%	lr:0.000177
Ep: 20/25	It: 2551/4130	batch_loss: 4.4111	batch_accuracy: 25.46%	lr:0.000177
Ep: 20/25	It: 2601/4130	batch_loss: 4.2461	batch_accuracy: 27.29%	lr:0.000176
Ep: 20/25	It: 2651/4130	batch_loss: 4.2967	batch_accuracy: 26.71%	lr:0.000175
Ep: 20/25	It: 2701/4130	batch_loss: 4.3660	batch_accuracy: 25.51%	lr:0.000175
Ep: 20/25	It: 2751/4130	batch_loss: 4.3181	batch_accuracy: 25.51%	lr:0.000174
Ep: 20/25	It: 2801/4130	batch_loss: 4.2677	batch_accuracy: 26.98%	lr:0.000173
Ep: 20/25	It: 2851/4130	batch_loss: 4.3377	batch_accuracy: 25.98%	lr:0.000172
Ep: 20/25	It: 2901/4130	batch_loss: 4.2001	batch_accuracy: 26.73%	lr:0.000172
Ep: 20/25	It: 2951/4130	batch_loss: 4.3697	batch_accuracy: 25.27%	lr:0.000171
Ep: 20/25	It: 3001/4130	batch_loss: 4.4436	batch_accuracy: 25.05%	lr:0.000170
Ep: 20/25	It: 3051/4130	batch_loss: 4.2878	batch_accuracy: 25.05%	lr:0.000170
Ep: 20/25	It: 3101/4130	batch_loss: 4.2523	batch_accuracy: 26.32%	lr:0.000169
Ep: 20/25	It: 3151/4130	batch_loss: 4.3692	batch_accuracy: 24.85%	lr:0.000168
Ep: 20/25	It: 3201/4130	batch_loss: 4.4475	batch_accuracy: 24.61%	lr:0.000168
Ep: 20/25	It: 3251/4130	batch_loss: 4.3292	batch_accuracy: 25.20%	lr:0.000167
Ep: 20/25	It: 3301/4130	batch_loss: 4.4766	batch_accuracy: 24.46%	lr:0.000166
Ep: 20/25	It: 3351/4130	batch_loss: 4.3529	batch_accuracy: 24.61%	lr:0.000166
Ep: 20/25	It: 3401/4130	batch_loss: 4.2335	batch_accuracy: 26.22%	lr:0.000165
Ep: 20/25	It: 3451/4130	batch_loss: 4.3164	batch_accuracy: 26.37%	lr:0.000164
Ep: 20/25	It: 3501/4130	batch_loss: 4.3771	batch_accuracy: 24.76%	lr:0.000163
Ep: 20/25	It: 3551/4130	batch_loss: 4.2899	batch_accuracy: 25.12%	lr:0.000163
Ep: 20/25	It: 3601/4130	batch_loss: 4.3784	batch_accuracy: 24.56%	lr:0.000162
Ep: 20/25	It: 3651/4130	batch_loss: 4.3163	batch_accuracy: 24.83%	lr:0.000161
Ep: 20/25	It: 3701/4130	batch_loss: 4.3552	batch_accuracy: 24.88%	lr:0.000161
Ep: 20/25	It: 3751/4130	batch_loss: 4.3020	batch_accuracy: 26.44%	lr:0.000160
Ep: 20/25	It: 3801/4130	batch_loss: 4.3168	batch_accuracy: 25.15%	lr:0.000159
Ep: 20/25	It: 3851/4130	batch_loss: 4.3508	batch_accuracy: 25.51%	lr:0.000159
Ep: 20/25	It: 3901/4130	batch_loss: 4.3510	batch_accuracy: 25.07%	lr:0.000158
Ep: 20/25	It: 3951/4130	batch_loss: 4.3849	batch_accuracy: 25.24%	lr:0.000157
Ep: 20/25	It: 4001/4130	batch_loss: 4.4368	batch_accuracy: 23.97%	lr:0.000157
Ep: 20/25	It: 4051/4130	batch_loss: 4.3990	batch_accuracy: 25.05%	lr:0.000156
Ep: 20/25	It: 4101/4130	batch_loss: 4.2831	batch_accuracy: 25.78%	lr:0.000155
Ep: 20/25	It: 4130/4130	batch_loss: 4.3972	batch_accuracy: 25.35%	lr:0.000155


Generated text for input text "You" is:
Youves, the tapine, and ple-thirthum-da was the same in the seniolium as in aqueous, with the n=3, and inv, a single nucleotide-1-2/s in the pg1, which is a promising candidate in the NP2-Sculiosome, and the Mg-2 is not a key factor of protein expression. The results show that HAG is a factor in the cytoplasmic cell line, as a result of this transcription factor, which could also influence the activity of the TGF‐β. A novel protein-dependent gene encoding DNA and protein kinase inhibitors, and the expression of the TGF-2 expression and cell migration in human hepatocytes, and the expression of the CDK3-1β‐β‐1α1.1′ and N-terminal A2A1/2 and PGE1 expression was associated with CLP3 expression. In this report, we identified that the T cell cycle of the BFP4 is a promising therapeutic method for cell proliferation, and also inhibited apoptosis of PF-3 cells with CN1β (P


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 21/25	It: 1/4130	batch_loss: 4.4241	batch_accuracy: 24.95%	lr:0.000155
Ep: 21/25	It: 51/4130	batch_loss: 4.3595	batch_accuracy: 25.61%	lr:0.000154
Ep: 21/25	It: 101/4130	batch_loss: 4.5123	batch_accuracy: 24.15%	lr:0.000154
Ep: 21/25	It: 151/4130	batch_loss: 4.3239	batch_accuracy: 26.15%	lr:0.000153
Ep: 21/25	It: 201/4130	batch_loss: 4.4109	batch_accuracy: 25.61%	lr:0.000152
Ep: 21/25	It: 251/4130	batch_loss: 4.4356	batch_accuracy: 24.00%	lr:0.000152
Ep: 21/25	It: 301/4130	batch_loss: 4.4398	batch_accuracy: 24.54%	lr:0.000151
Ep: 21/25	It: 351/4130	batch_loss: 4.3793	batch_accuracy: 24.61%	lr:0.000150
Ep: 21/25	It: 401/4130	batch_loss: 4.3741	batch_accuracy: 24.54%	lr:0.000150
Ep: 21/25	It: 451/4130	batch_loss: 4.3429	batch_accuracy: 25.10%	lr:0.000149
Ep: 21/25	It: 501/4130	batch_loss: 4.3011	batch_accuracy: 26.27%	lr:0.000148
Ep: 21/25	It: 551/4130	batch_loss: 4.3412	batch_accuracy: 24.80%	lr:0.000148
Ep: 21/25	It: 601/4130	batch_loss: 4.3415	batch_accuracy: 25.12%	lr:0.000147
Ep: 21/25	It: 651/4130	batch_loss: 4.2894	batch_accuracy: 26.00%	lr:0.000146
Ep: 21/25	It: 701/4130	batch_loss: 4.3222	batch_accuracy: 26.46%	lr:0.000146
Ep: 21/25	It: 751/4130	batch_loss: 4.2269	batch_accuracy: 26.34%	lr:0.000145
Ep: 21/25	It: 801/4130	batch_loss: 4.3912	batch_accuracy: 25.85%	lr:0.000144
Ep: 21/25	It: 851/4130	batch_loss: 4.3577	batch_accuracy: 25.02%	lr:0.000144
Ep: 21/25	It: 901/4130	batch_loss: 4.3561	batch_accuracy: 25.54%	lr:0.000143
Ep: 21/25	It: 951/4130	batch_loss: 4.3364	batch_accuracy: 25.68%	lr:0.000143
Ep: 21/25	It: 1001/4130	batch_loss: 4.3597	batch_accuracy: 25.22%	lr:0.000142
Ep: 21/25	It: 1051/4130	batch_loss: 4.2084	batch_accuracy: 27.42%	lr:0.000141
Ep: 21/25	It: 1101/4130	batch_loss: 4.2430	batch_accuracy: 25.93%	lr:0.000141
Ep: 21/25	It: 1151/4130	batch_loss: 4.4363	batch_accuracy: 24.37%	lr:0.000140
Ep: 21/25	It: 1201/4130	batch_loss: 4.2882	batch_accuracy: 26.66%	lr:0.000139
Ep: 21/25	It: 1251/4130	batch_loss: 4.3906	batch_accuracy: 25.78%	lr:0.000139
Ep: 21/25	It: 1301/4130	batch_loss: 4.3545	batch_accuracy: 25.63%	lr:0.000138
Ep: 21/25	It: 1351/4130	batch_loss: 4.5055	batch_accuracy: 24.71%	lr:0.000137
Ep: 21/25	It: 1401/4130	batch_loss: 4.2917	batch_accuracy: 26.27%	lr:0.000137
Ep: 21/25	It: 1451/4130	batch_loss: 4.2236	batch_accuracy: 26.78%	lr:0.000136
Ep: 21/25	It: 1501/4130	batch_loss: 4.2858	batch_accuracy: 25.85%	lr:0.000136
Ep: 21/25	It: 1551/4130	batch_loss: 4.3755	batch_accuracy: 25.39%	lr:0.000135
Ep: 21/25	It: 1601/4130	batch_loss: 4.2622	batch_accuracy: 26.44%	lr:0.000134
Ep: 21/25	It: 1651/4130	batch_loss: 4.3778	batch_accuracy: 24.10%	lr:0.000134
Ep: 21/25	It: 1701/4130	batch_loss: 4.3750	batch_accuracy: 24.78%	lr:0.000133
Ep: 21/25	It: 1751/4130	batch_loss: 4.2921	batch_accuracy: 25.68%	lr:0.000132
Ep: 21/25	It: 1801/4130	batch_loss: 4.3961	batch_accuracy: 24.88%	lr:0.000132
Ep: 21/25	It: 1851/4130	batch_loss: 4.3669	batch_accuracy: 25.90%	lr:0.000131
Ep: 21/25	It: 1901/4130	batch_loss: 4.3818	batch_accuracy: 25.59%	lr:0.000131
Ep: 21/25	It: 1951/4130	batch_loss: 4.4136	batch_accuracy: 24.37%	lr:0.000130
Ep: 21/25	It: 2001/4130	batch_loss: 4.4178	batch_accuracy: 24.80%	lr:0.000129
Ep: 21/25	It: 2051/4130	batch_loss: 4.3189	batch_accuracy: 24.98%	lr:0.000129
Ep: 21/25	It: 2101/4130	batch_loss: 4.3191	batch_accuracy: 26.73%	lr:0.000128
Ep: 21/25	It: 2151/4130	batch_loss: 4.4305	batch_accuracy: 24.51%	lr:0.000128
Ep: 21/25	It: 2201/4130	batch_loss: 4.3194	batch_accuracy: 25.78%	lr:0.000127
Ep: 21/25	It: 2251/4130	batch_loss: 4.3377	batch_accuracy: 25.66%	lr:0.000126
Ep: 21/25	It: 2301/4130	batch_loss: 4.3240	batch_accuracy: 25.05%	lr:0.000126
Ep: 21/25	It: 2351/4130	batch_loss: 4.3050	batch_accuracy: 26.15%	lr:0.000125
Ep: 21/25	It: 2401/4130	batch_loss: 4.3909	batch_accuracy: 24.88%	lr:0.000125
Ep: 21/25	It: 2451/4130	batch_loss: 4.2452	batch_accuracy: 26.61%	lr:0.000124
Ep: 21/25	It: 2501/4130	batch_loss: 4.2480	batch_accuracy: 25.66%	lr:0.000123
Ep: 21/25	It: 2551/4130	batch_loss: 4.4307	batch_accuracy: 24.07%	lr:0.000123
Ep: 21/25	It: 2601/4130	batch_loss: 4.2582	batch_accuracy: 26.88%	lr:0.000122
Ep: 21/25	It: 2651/4130	batch_loss: 4.3393	batch_accuracy: 26.10%	lr:0.000122
Ep: 21/25	It: 2701/4130	batch_loss: 4.3489	batch_accuracy: 25.27%	lr:0.000121
Ep: 21/25	It: 2751/4130	batch_loss: 4.3344	batch_accuracy: 26.27%	lr:0.000120
Ep: 21/25	It: 2801/4130	batch_loss: 4.3737	batch_accuracy: 25.81%	lr:0.000120
Ep: 21/25	It: 2851/4130	batch_loss: 4.3332	batch_accuracy: 26.56%	lr:0.000119
Ep: 21/25	It: 2901/4130	batch_loss: 4.3383	batch_accuracy: 25.63%	lr:0.000119
Ep: 21/25	It: 2951/4130	batch_loss: 4.4252	batch_accuracy: 24.93%	lr:0.000118
Ep: 21/25	It: 3001/4130	batch_loss: 4.4022	batch_accuracy: 25.17%	lr:0.000117
Ep: 21/25	It: 3051/4130	batch_loss: 4.2985	batch_accuracy: 25.90%	lr:0.000117
Ep: 21/25	It: 3101/4130	batch_loss: 4.3489	batch_accuracy: 25.12%	lr:0.000116
Ep: 21/25	It: 3151/4130	batch_loss: 4.3034	batch_accuracy: 25.24%	lr:0.000116
Ep: 21/25	It: 3201/4130	batch_loss: 4.2979	batch_accuracy: 26.22%	lr:0.000115
Ep: 21/25	It: 3251/4130	batch_loss: 4.3262	batch_accuracy: 26.73%	lr:0.000114
Ep: 21/25	It: 3301/4130	batch_loss: 4.3642	batch_accuracy: 24.61%	lr:0.000114
Ep: 21/25	It: 3351/4130	batch_loss: 4.2640	batch_accuracy: 26.78%	lr:0.000113
Ep: 21/25	It: 3401/4130	batch_loss: 4.2781	batch_accuracy: 27.15%	lr:0.000113
Ep: 21/25	It: 3451/4130	batch_loss: 4.3216	batch_accuracy: 25.68%	lr:0.000112
Ep: 21/25	It: 3501/4130	batch_loss: 4.3585	batch_accuracy: 25.90%	lr:0.000112
Ep: 21/25	It: 3551/4130	batch_loss: 4.4793	batch_accuracy: 24.41%	lr:0.000111
Ep: 21/25	It: 3601/4130	batch_loss: 4.4354	batch_accuracy: 23.88%	lr:0.000110
Ep: 21/25	It: 3651/4130	batch_loss: 4.3417	batch_accuracy: 25.83%	lr:0.000110
Ep: 21/25	It: 3701/4130	batch_loss: 4.3368	batch_accuracy: 23.90%	lr:0.000109
Ep: 21/25	It: 3751/4130	batch_loss: 4.3788	batch_accuracy: 24.63%	lr:0.000109
Ep: 21/25	It: 3801/4130	batch_loss: 4.3681	batch_accuracy: 25.85%	lr:0.000108
Ep: 21/25	It: 3851/4130	batch_loss: 4.2451	batch_accuracy: 26.81%	lr:0.000108
Ep: 21/25	It: 3901/4130	batch_loss: 4.2773	batch_accuracy: 26.32%	lr:0.000107
Ep: 21/25	It: 3951/4130	batch_loss: 4.2851	batch_accuracy: 26.05%	lr:0.000107
Ep: 21/25	It: 4001/4130	batch_loss: 4.3137	batch_accuracy: 25.61%	lr:0.000106
Ep: 21/25	It: 4051/4130	batch_loss: 4.3286	batch_accuracy: 24.80%	lr:0.000105
Ep: 21/25	It: 4101/4130	batch_loss: 4.3630	batch_accuracy: 26.03%	lr:0.000105
Ep: 21/25	It: 4130/4130	batch_loss: 4.4254	batch_accuracy: 24.52%	lr:0.000105


Generated text for input text "You" is:
Youves of the cyl-1 and ~103(2)) and T2 (c)-3,3-p, and P-2) as anc(1)(6)2 and 1 (5-3,3-tc2), and 2. The MnO-1-SiR‐4,8, and S-pi-Fa-A1. We conclude that SFP1, TiO2, and T1 and G-1-2, as well as a CF-1 receptor antagonist.
<eot>
<sot>
Application of Pseudomonas and the Grave of the Nigeria: A Case Study of Basic and Clostrus

ABSTRACT The study was conducted in this study and in the study of the Escherichia coli (Mn(P)H2-5‐pyrosine-induced apoptosis in the cell-mediated immunocompatibility in the presence of the anti-multiple-mediated growth factor in the rat macrophages. The cytoplasm (p-FGF-α) is not known to induce apoptosis in vitro


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 22/25	It: 1/4130	batch_loss: 4.3313	batch_accuracy: 25.76%	lr:0.000105
Ep: 22/25	It: 51/4130	batch_loss: 4.4562	batch_accuracy: 24.46%	lr:0.000104
Ep: 22/25	It: 101/4130	batch_loss: 4.4285	batch_accuracy: 24.41%	lr:0.000103
Ep: 22/25	It: 151/4130	batch_loss: 4.3026	batch_accuracy: 27.00%	lr:0.000103
Ep: 22/25	It: 201/4130	batch_loss: 4.3071	batch_accuracy: 24.80%	lr:0.000102
Ep: 22/25	It: 251/4130	batch_loss: 4.2631	batch_accuracy: 26.20%	lr:0.000102
Ep: 22/25	It: 301/4130	batch_loss: 4.3221	batch_accuracy: 25.88%	lr:0.000101
Ep: 22/25	It: 351/4130	batch_loss: 4.4107	batch_accuracy: 24.02%	lr:0.000101
Ep: 22/25	It: 401/4130	batch_loss: 4.3878	batch_accuracy: 24.78%	lr:0.000100
Ep: 22/25	It: 451/4130	batch_loss: 4.2125	batch_accuracy: 27.20%	lr:0.000100
Ep: 22/25	It: 501/4130	batch_loss: 4.2400	batch_accuracy: 27.12%	lr:0.000099
Ep: 22/25	It: 551/4130	batch_loss: 4.3629	batch_accuracy: 24.44%	lr:0.000099
Ep: 22/25	It: 601/4130	batch_loss: 4.2973	batch_accuracy: 26.25%	lr:0.000098
Ep: 22/25	It: 651/4130	batch_loss: 4.3585	batch_accuracy: 26.42%	lr:0.000097
Ep: 22/25	It: 701/4130	batch_loss: 4.3614	batch_accuracy: 25.12%	lr:0.000097
Ep: 22/25	It: 751/4130	batch_loss: 4.2614	batch_accuracy: 26.49%	lr:0.000096
Ep: 22/25	It: 801/4130	batch_loss: 4.2427	batch_accuracy: 27.00%	lr:0.000096
Ep: 22/25	It: 851/4130	batch_loss: 4.3680	batch_accuracy: 25.61%	lr:0.000095
Ep: 22/25	It: 901/4130	batch_loss: 4.3415	batch_accuracy: 25.56%	lr:0.000095
Ep: 22/25	It: 951/4130	batch_loss: 4.3875	batch_accuracy: 24.27%	lr:0.000094
Ep: 22/25	It: 1001/4130	batch_loss: 4.4447	batch_accuracy: 25.24%	lr:0.000094
Ep: 22/25	It: 1051/4130	batch_loss: 4.2563	batch_accuracy: 26.56%	lr:0.000093
Ep: 22/25	It: 1101/4130	batch_loss: 4.3422	batch_accuracy: 25.51%	lr:0.000093
Ep: 22/25	It: 1151/4130	batch_loss: 4.2511	batch_accuracy: 26.93%	lr:0.000092
Ep: 22/25	It: 1201/4130	batch_loss: 4.4064	batch_accuracy: 25.24%	lr:0.000092
Ep: 22/25	It: 1251/4130	batch_loss: 4.2032	batch_accuracy: 27.42%	lr:0.000091
Ep: 22/25	It: 1301/4130	batch_loss: 4.2854	batch_accuracy: 25.88%	lr:0.000091
Ep: 22/25	It: 1351/4130	batch_loss: 4.3900	batch_accuracy: 24.41%	lr:0.000090
Ep: 22/25	It: 1401/4130	batch_loss: 4.3106	batch_accuracy: 24.58%	lr:0.000090
Ep: 22/25	It: 1451/4130	batch_loss: 4.3118	batch_accuracy: 26.22%	lr:0.000089
Ep: 22/25	It: 1501/4130	batch_loss: 4.2907	batch_accuracy: 26.27%	lr:0.000089
Ep: 22/25	It: 1551/4130	batch_loss: 4.2832	batch_accuracy: 24.98%	lr:0.000088
Ep: 22/25	It: 1601/4130	batch_loss: 4.2656	batch_accuracy: 25.85%	lr:0.000088
Ep: 22/25	It: 1651/4130	batch_loss: 4.2756	batch_accuracy: 26.76%	lr:0.000087
Ep: 22/25	It: 1701/4130	batch_loss: 4.2995	batch_accuracy: 26.22%	lr:0.000087
Ep: 22/25	It: 1751/4130	batch_loss: 4.1713	batch_accuracy: 27.73%	lr:0.000086
Ep: 22/25	It: 1801/4130	batch_loss: 4.1933	batch_accuracy: 27.34%	lr:0.000086
Ep: 22/25	It: 1851/4130	batch_loss: 4.4531	batch_accuracy: 23.93%	lr:0.000085
Ep: 22/25	It: 1901/4130	batch_loss: 4.2649	batch_accuracy: 26.12%	lr:0.000085
Ep: 22/25	It: 1951/4130	batch_loss: 4.2856	batch_accuracy: 26.27%	lr:0.000084
Ep: 22/25	It: 2001/4130	batch_loss: 4.3795	batch_accuracy: 25.05%	lr:0.000084
Ep: 22/25	It: 2051/4130	batch_loss: 4.3382	batch_accuracy: 25.24%	lr:0.000083
Ep: 22/25	It: 2101/4130	batch_loss: 4.3047	batch_accuracy: 26.15%	lr:0.000083
Ep: 22/25	It: 2151/4130	batch_loss: 4.3631	batch_accuracy: 25.88%	lr:0.000082
Ep: 22/25	It: 2201/4130	batch_loss: 4.3082	batch_accuracy: 26.34%	lr:0.000082
Ep: 22/25	It: 2251/4130	batch_loss: 4.2846	batch_accuracy: 27.20%	lr:0.000081
Ep: 22/25	It: 2301/4130	batch_loss: 4.3723	batch_accuracy: 24.41%	lr:0.000081
Ep: 22/25	It: 2351/4130	batch_loss: 4.3576	batch_accuracy: 25.44%	lr:0.000080
Ep: 22/25	It: 2401/4130	batch_loss: 4.5714	batch_accuracy: 22.90%	lr:0.000080
Ep: 22/25	It: 2451/4130	batch_loss: 4.3200	batch_accuracy: 26.12%	lr:0.000079
Ep: 22/25	It: 2501/4130	batch_loss: 4.3618	batch_accuracy: 25.76%	lr:0.000079
Ep: 22/25	It: 2551/4130	batch_loss: 4.2829	batch_accuracy: 26.61%	lr:0.000078
Ep: 22/25	It: 2601/4130	batch_loss: 4.3342	batch_accuracy: 27.15%	lr:0.000078
Ep: 22/25	It: 2651/4130	batch_loss: 4.2919	batch_accuracy: 27.27%	lr:0.000077
Ep: 22/25	It: 2701/4130	batch_loss: 4.3755	batch_accuracy: 24.44%	lr:0.000077
Ep: 22/25	It: 2751/4130	batch_loss: 4.3177	batch_accuracy: 26.64%	lr:0.000076
Ep: 22/25	It: 2801/4130	batch_loss: 4.2202	batch_accuracy: 25.95%	lr:0.000076
Ep: 22/25	It: 2851/4130	batch_loss: 4.3076	batch_accuracy: 26.05%	lr:0.000075
Ep: 22/25	It: 2901/4130	batch_loss: 4.2870	batch_accuracy: 26.00%	lr:0.000075
Ep: 22/25	It: 2951/4130	batch_loss: 4.2314	batch_accuracy: 27.03%	lr:0.000074
Ep: 22/25	It: 3001/4130	batch_loss: 4.3302	batch_accuracy: 25.59%	lr:0.000074
Ep: 22/25	It: 3051/4130	batch_loss: 4.3038	batch_accuracy: 25.00%	lr:0.000074
Ep: 22/25	It: 3101/4130	batch_loss: 4.4342	batch_accuracy: 24.68%	lr:0.000073
Ep: 22/25	It: 3151/4130	batch_loss: 4.3316	batch_accuracy: 25.24%	lr:0.000073
Ep: 22/25	It: 3201/4130	batch_loss: 4.4397	batch_accuracy: 24.71%	lr:0.000072
Ep: 22/25	It: 3251/4130	batch_loss: 4.2408	batch_accuracy: 26.95%	lr:0.000072
Ep: 22/25	It: 3301/4130	batch_loss: 4.3397	batch_accuracy: 25.32%	lr:0.000071
Ep: 22/25	It: 3351/4130	batch_loss: 4.4041	batch_accuracy: 24.22%	lr:0.000071
Ep: 22/25	It: 3401/4130	batch_loss: 4.2582	batch_accuracy: 26.46%	lr:0.000070
Ep: 22/25	It: 3451/4130	batch_loss: 4.3165	batch_accuracy: 25.98%	lr:0.000070
Ep: 22/25	It: 3501/4130	batch_loss: 4.2530	batch_accuracy: 26.90%	lr:0.000069
Ep: 22/25	It: 3551/4130	batch_loss: 4.2050	batch_accuracy: 26.25%	lr:0.000069
Ep: 22/25	It: 3601/4130	batch_loss: 4.2487	batch_accuracy: 25.68%	lr:0.000069
Ep: 22/25	It: 3651/4130	batch_loss: 4.3548	batch_accuracy: 24.90%	lr:0.000068
Ep: 22/25	It: 3701/4130	batch_loss: 4.3525	batch_accuracy: 25.95%	lr:0.000068
Ep: 22/25	It: 3751/4130	batch_loss: 4.4700	batch_accuracy: 24.07%	lr:0.000067
Ep: 22/25	It: 3801/4130	batch_loss: 4.3375	batch_accuracy: 25.29%	lr:0.000067
Ep: 22/25	It: 3851/4130	batch_loss: 4.3126	batch_accuracy: 25.98%	lr:0.000066
Ep: 22/25	It: 3901/4130	batch_loss: 4.3495	batch_accuracy: 26.12%	lr:0.000066
Ep: 22/25	It: 3951/4130	batch_loss: 4.2782	batch_accuracy: 25.54%	lr:0.000065
Ep: 22/25	It: 4001/4130	batch_loss: 4.2975	batch_accuracy: 25.66%	lr:0.000065
Ep: 22/25	It: 4051/4130	batch_loss: 4.2105	batch_accuracy: 28.00%	lr:0.000065
Ep: 22/25	It: 4101/4130	batch_loss: 4.4049	batch_accuracy: 24.56%	lr:0.000064
Ep: 22/25	It: 4130/4130	batch_loss: 4.2775	batch_accuracy: 26.05%	lr:0.000064


Generated text for input text "You" is:
Youzine, and tail. In 1999.

RESULTS
P and 2 of 63 women (9%) were enrolled by the AC in the United States (83) in the United States and 56% of whom (5%) were women, 83; (5%) females and males and 51 (6%) were treated with the PCI and 82 (7%). Conclusions: The prevalence of CFE in the age group was higher than the women, while there were significant differences between the mean age of the women, the mean age group (n=5). Conclusions: P<0.001, respectively) who received the median age of 73 months of age, sex and age was significantly more in the control group than between the patients. Pediatric symptoms and anxiety were the mean age, age, and sex (n = 1) of women aged 12.2 months after the first follow-up (n = 61) was 83% (86%) and 59 (n = 57) were not associated with severe symptoms. A total of 122 women who had the highest number of women. The majority of patients were diagnosed as the major cause of age. There were no significant difference


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 23/25	It: 1/4130	batch_loss: 4.3380	batch_accuracy: 25.90%	lr:0.000064
Ep: 23/25	It: 51/4130	batch_loss: 4.3510	batch_accuracy: 24.83%	lr:0.000064
Ep: 23/25	It: 101/4130	batch_loss: 4.4252	batch_accuracy: 24.88%	lr:0.000063
Ep: 23/25	It: 151/4130	batch_loss: 4.4369	batch_accuracy: 23.83%	lr:0.000063
Ep: 23/25	It: 201/4130	batch_loss: 4.3274	batch_accuracy: 25.61%	lr:0.000062
Ep: 23/25	It: 251/4130	batch_loss: 4.3989	batch_accuracy: 24.39%	lr:0.000062
Ep: 23/25	It: 301/4130	batch_loss: 4.3414	batch_accuracy: 25.90%	lr:0.000061
Ep: 23/25	It: 351/4130	batch_loss: 4.3416	batch_accuracy: 25.24%	lr:0.000061
Ep: 23/25	It: 401/4130	batch_loss: 4.3419	batch_accuracy: 25.93%	lr:0.000061
Ep: 23/25	It: 451/4130	batch_loss: 4.3883	batch_accuracy: 25.78%	lr:0.000060
Ep: 23/25	It: 501/4130	batch_loss: 4.5191	batch_accuracy: 23.24%	lr:0.000060
Ep: 23/25	It: 551/4130	batch_loss: 4.2310	batch_accuracy: 27.25%	lr:0.000059
Ep: 23/25	It: 601/4130	batch_loss: 4.3260	batch_accuracy: 26.73%	lr:0.000059
Ep: 23/25	It: 651/4130	batch_loss: 4.3689	batch_accuracy: 25.17%	lr:0.000059
Ep: 23/25	It: 701/4130	batch_loss: 4.3726	batch_accuracy: 24.83%	lr:0.000058
Ep: 23/25	It: 751/4130	batch_loss: 4.3553	batch_accuracy: 26.54%	lr:0.000058
Ep: 23/25	It: 801/4130	batch_loss: 4.1654	batch_accuracy: 26.81%	lr:0.000057
Ep: 23/25	It: 851/4130	batch_loss: 4.3833	batch_accuracy: 25.49%	lr:0.000057
Ep: 23/25	It: 901/4130	batch_loss: 4.3918	batch_accuracy: 24.44%	lr:0.000057
Ep: 23/25	It: 951/4130	batch_loss: 4.4160	batch_accuracy: 25.61%	lr:0.000056
Ep: 23/25	It: 1001/4130	batch_loss: 4.3015	batch_accuracy: 25.29%	lr:0.000056
Ep: 23/25	It: 1051/4130	batch_loss: 4.2158	batch_accuracy: 27.25%	lr:0.000055
Ep: 23/25	It: 1101/4130	batch_loss: 4.3913	batch_accuracy: 25.10%	lr:0.000055
Ep: 23/25	It: 1151/4130	batch_loss: 4.1810	batch_accuracy: 27.27%	lr:0.000055
Ep: 23/25	It: 1201/4130	batch_loss: 4.3532	batch_accuracy: 24.58%	lr:0.000054
Ep: 23/25	It: 1251/4130	batch_loss: 4.2721	batch_accuracy: 26.22%	lr:0.000054
Ep: 23/25	It: 1301/4130	batch_loss: 4.3402	batch_accuracy: 26.10%	lr:0.000053
Ep: 23/25	It: 1351/4130	batch_loss: 4.4064	batch_accuracy: 24.78%	lr:0.000053
Ep: 23/25	It: 1401/4130	batch_loss: 4.2624	batch_accuracy: 27.32%	lr:0.000053
Ep: 23/25	It: 1451/4130	batch_loss: 4.3465	batch_accuracy: 25.90%	lr:0.000052
Ep: 23/25	It: 1501/4130	batch_loss: 4.1005	batch_accuracy: 27.91%	lr:0.000052
Ep: 23/25	It: 1551/4130	batch_loss: 4.3165	batch_accuracy: 24.78%	lr:0.000051
Ep: 23/25	It: 1601/4130	batch_loss: 4.3044	batch_accuracy: 26.64%	lr:0.000051
Ep: 23/25	It: 1651/4130	batch_loss: 4.2805	batch_accuracy: 25.76%	lr:0.000051
Ep: 23/25	It: 1701/4130	batch_loss: 4.2759	batch_accuracy: 24.73%	lr:0.000050
Ep: 23/25	It: 1751/4130	batch_loss: 4.3448	batch_accuracy: 25.39%	lr:0.000050
Ep: 23/25	It: 1801/4130	batch_loss: 4.2377	batch_accuracy: 25.68%	lr:0.000050
Ep: 23/25	It: 1851/4130	batch_loss: 4.2780	batch_accuracy: 26.86%	lr:0.000049
Ep: 23/25	It: 1901/4130	batch_loss: 4.3473	batch_accuracy: 24.95%	lr:0.000049
Ep: 23/25	It: 1951/4130	batch_loss: 4.4067	batch_accuracy: 24.34%	lr:0.000049
Ep: 23/25	It: 2001/4130	batch_loss: 4.2741	batch_accuracy: 27.00%	lr:0.000048
Ep: 23/25	It: 2051/4130	batch_loss: 4.3786	batch_accuracy: 25.51%	lr:0.000048
Ep: 23/25	It: 2101/4130	batch_loss: 4.2956	batch_accuracy: 26.83%	lr:0.000047
Ep: 23/25	It: 2151/4130	batch_loss: 4.3167	batch_accuracy: 26.03%	lr:0.000047
Ep: 23/25	It: 2201/4130	batch_loss: 4.4317	batch_accuracy: 25.12%	lr:0.000047
Ep: 23/25	It: 2251/4130	batch_loss: 4.3228	batch_accuracy: 26.27%	lr:0.000046
Ep: 23/25	It: 2301/4130	batch_loss: 4.2977	batch_accuracy: 25.59%	lr:0.000046
Ep: 23/25	It: 2351/4130	batch_loss: 4.3280	batch_accuracy: 25.98%	lr:0.000046
Ep: 23/25	It: 2401/4130	batch_loss: 4.3850	batch_accuracy: 26.29%	lr:0.000045
Ep: 23/25	It: 2451/4130	batch_loss: 4.2910	batch_accuracy: 26.71%	lr:0.000045
Ep: 23/25	It: 2501/4130	batch_loss: 4.3549	batch_accuracy: 25.17%	lr:0.000045
Ep: 23/25	It: 2551/4130	batch_loss: 4.4083	batch_accuracy: 24.68%	lr:0.000044
Ep: 23/25	It: 2601/4130	batch_loss: 4.3104	batch_accuracy: 26.49%	lr:0.000044
Ep: 23/25	It: 2651/4130	batch_loss: 4.3405	batch_accuracy: 24.93%	lr:0.000044
Ep: 23/25	It: 2701/4130	batch_loss: 4.3594	batch_accuracy: 24.73%	lr:0.000043
Ep: 23/25	It: 2751/4130	batch_loss: 4.4391	batch_accuracy: 24.44%	lr:0.000043
Ep: 23/25	It: 2801/4130	batch_loss: 4.2722	batch_accuracy: 26.17%	lr:0.000043
Ep: 23/25	It: 2851/4130	batch_loss: 4.2673	batch_accuracy: 26.27%	lr:0.000042
Ep: 23/25	It: 2901/4130	batch_loss: 4.3229	batch_accuracy: 25.22%	lr:0.000042
Ep: 23/25	It: 2951/4130	batch_loss: 4.2217	batch_accuracy: 27.61%	lr:0.000042
Ep: 23/25	It: 3001/4130	batch_loss: 4.2673	batch_accuracy: 26.78%	lr:0.000041
Ep: 23/25	It: 3051/4130	batch_loss: 4.3359	batch_accuracy: 26.15%	lr:0.000041
Ep: 23/25	It: 3101/4130	batch_loss: 4.3825	batch_accuracy: 25.00%	lr:0.000041
Ep: 23/25	It: 3151/4130	batch_loss: 4.3299	batch_accuracy: 25.98%	lr:0.000040
Ep: 23/25	It: 3201/4130	batch_loss: 4.3836	batch_accuracy: 24.90%	lr:0.000040
Ep: 23/25	It: 3251/4130	batch_loss: 4.3586	batch_accuracy: 24.63%	lr:0.000040
Ep: 23/25	It: 3301/4130	batch_loss: 4.2058	batch_accuracy: 26.54%	lr:0.000039
Ep: 23/25	It: 3351/4130	batch_loss: 4.3226	batch_accuracy: 25.83%	lr:0.000039
Ep: 23/25	It: 3401/4130	batch_loss: 4.4348	batch_accuracy: 23.66%	lr:0.000039
Ep: 23/25	It: 3451/4130	batch_loss: 4.4884	batch_accuracy: 23.27%	lr:0.000038
Ep: 23/25	It: 3501/4130	batch_loss: 4.3126	batch_accuracy: 26.42%	lr:0.000038
Ep: 23/25	It: 3551/4130	batch_loss: 4.3105	batch_accuracy: 26.20%	lr:0.000038
Ep: 23/25	It: 3601/4130	batch_loss: 4.3869	batch_accuracy: 25.59%	lr:0.000037
Ep: 23/25	It: 3651/4130	batch_loss: 4.3316	batch_accuracy: 26.46%	lr:0.000037
Ep: 23/25	It: 3701/4130	batch_loss: 4.2652	batch_accuracy: 25.63%	lr:0.000037
Ep: 23/25	It: 3751/4130	batch_loss: 4.2037	batch_accuracy: 26.90%	lr:0.000036
Ep: 23/25	It: 3801/4130	batch_loss: 4.3866	batch_accuracy: 23.83%	lr:0.000036
Ep: 23/25	It: 3851/4130	batch_loss: 4.3522	batch_accuracy: 25.17%	lr:0.000036
Ep: 23/25	It: 3901/4130	batch_loss: 4.2422	batch_accuracy: 26.76%	lr:0.000036
Ep: 23/25	It: 3951/4130	batch_loss: 4.2204	batch_accuracy: 27.27%	lr:0.000035
Ep: 23/25	It: 4001/4130	batch_loss: 4.2397	batch_accuracy: 26.44%	lr:0.000035
Ep: 23/25	It: 4051/4130	batch_loss: 4.2542	batch_accuracy: 26.27%	lr:0.000035
Ep: 23/25	It: 4101/4130	batch_loss: 4.3459	batch_accuracy: 24.63%	lr:0.000034
Ep: 23/25	It: 4130/4130	batch_loss: 4.2848	batch_accuracy: 26.21%	lr:0.000034


Generated text for input text "You" is:
Youves the bill. In this study, we reported anthropometric analysis of adia and the PBB2, using two subunts. This approach is a new model that has been used to determine theories, that the proposed algorithm is a simple method forgetting. The system allows it is shown to be used to obtain the proposed data of the FR-1S. The proposed approach uses a novel method based on the MEMS and SUMMARS.
<eot>
<sot>
Simultaneous Factor

The authors report on a multi-cultural model that includes the first one. The results are consistent with the proposed method for the first time, which is used for the proposed method. The results are discussed. The results show that the proposed controller is not well as the proposed algorithm for the proposed approach.
<eot>
<sot>
The Effect of Evaluation of the Carbon Sensor for the Petried Soland

This paper proposes a new framework to analyze the effects of the proposed method and the proposed method, based on the results of the proposed method for the detection of the proposed method. The results show that the proposed algorithm is


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 24/25	It: 1/4130	batch_loss: 4.4139	batch_accuracy: 26.20%	lr:0.000034
Ep: 24/25	It: 51/4130	batch_loss: 4.3636	batch_accuracy: 25.02%	lr:0.000034
Ep: 24/25	It: 101/4130	batch_loss: 4.3593	batch_accuracy: 25.78%	lr:0.000034
Ep: 24/25	It: 151/4130	batch_loss: 4.3230	batch_accuracy: 26.29%	lr:0.000033
Ep: 24/25	It: 201/4130	batch_loss: 4.3266	batch_accuracy: 25.42%	lr:0.000033
Ep: 24/25	It: 251/4130	batch_loss: 4.3446	batch_accuracy: 25.63%	lr:0.000033
Ep: 24/25	It: 301/4130	batch_loss: 4.2922	batch_accuracy: 26.12%	lr:0.000033
Ep: 24/25	It: 351/4130	batch_loss: 4.3668	batch_accuracy: 25.24%	lr:0.000032
Ep: 24/25	It: 401/4130	batch_loss: 4.4183	batch_accuracy: 25.29%	lr:0.000032
Ep: 24/25	It: 451/4130	batch_loss: 4.2823	batch_accuracy: 26.03%	lr:0.000032
Ep: 24/25	It: 501/4130	batch_loss: 4.3992	batch_accuracy: 24.51%	lr:0.000031
Ep: 24/25	It: 551/4130	batch_loss: 4.4045	batch_accuracy: 25.15%	lr:0.000031
Ep: 24/25	It: 601/4130	batch_loss: 4.3586	batch_accuracy: 24.83%	lr:0.000031
Ep: 24/25	It: 651/4130	batch_loss: 4.2171	batch_accuracy: 27.03%	lr:0.000031
Ep: 24/25	It: 701/4130	batch_loss: 4.3917	batch_accuracy: 25.29%	lr:0.000030
Ep: 24/25	It: 751/4130	batch_loss: 4.3516	batch_accuracy: 25.61%	lr:0.000030
Ep: 24/25	It: 801/4130	batch_loss: 4.4428	batch_accuracy: 24.68%	lr:0.000030
Ep: 24/25	It: 851/4130	batch_loss: 4.4087	batch_accuracy: 24.58%	lr:0.000030
Ep: 24/25	It: 901/4130	batch_loss: 4.2815	batch_accuracy: 26.07%	lr:0.000029
Ep: 24/25	It: 951/4130	batch_loss: 4.3010	batch_accuracy: 25.95%	lr:0.000029
Ep: 24/25	It: 1001/4130	batch_loss: 4.4759	batch_accuracy: 23.54%	lr:0.000029
Ep: 24/25	It: 1051/4130	batch_loss: 4.2578	batch_accuracy: 27.17%	lr:0.000028
Ep: 24/25	It: 1101/4130	batch_loss: 4.4354	batch_accuracy: 24.49%	lr:0.000028
Ep: 24/25	It: 1151/4130	batch_loss: 4.2396	batch_accuracy: 26.34%	lr:0.000028
Ep: 24/25	It: 1201/4130	batch_loss: 4.3170	batch_accuracy: 26.56%	lr:0.000028
Ep: 24/25	It: 1251/4130	batch_loss: 4.3673	batch_accuracy: 24.61%	lr:0.000027
Ep: 24/25	It: 1301/4130	batch_loss: 4.2980	batch_accuracy: 26.78%	lr:0.000027
Ep: 24/25	It: 1351/4130	batch_loss: 4.3063	batch_accuracy: 25.46%	lr:0.000027
Ep: 24/25	It: 1401/4130	batch_loss: 4.2630	batch_accuracy: 26.00%	lr:0.000027
Ep: 24/25	It: 1451/4130	batch_loss: 4.3977	batch_accuracy: 24.90%	lr:0.000027
Ep: 24/25	It: 1501/4130	batch_loss: 4.2600	batch_accuracy: 26.39%	lr:0.000026
Ep: 24/25	It: 1551/4130	batch_loss: 4.3116	batch_accuracy: 26.73%	lr:0.000026
Ep: 24/25	It: 1601/4130	batch_loss: 4.3165	batch_accuracy: 25.46%	lr:0.000026
Ep: 24/25	It: 1651/4130	batch_loss: 4.3861	batch_accuracy: 24.88%	lr:0.000026
Ep: 24/25	It: 1701/4130	batch_loss: 4.2774	batch_accuracy: 24.76%	lr:0.000025
Ep: 24/25	It: 1751/4130	batch_loss: 4.3316	batch_accuracy: 25.37%	lr:0.000025
Ep: 24/25	It: 1801/4130	batch_loss: 4.3778	batch_accuracy: 26.03%	lr:0.000025
Ep: 24/25	It: 1851/4130	batch_loss: 4.3777	batch_accuracy: 25.49%	lr:0.000025
Ep: 24/25	It: 1901/4130	batch_loss: 4.3444	batch_accuracy: 24.78%	lr:0.000024
Ep: 24/25	It: 1951/4130	batch_loss: 4.3323	batch_accuracy: 25.90%	lr:0.000024
Ep: 24/25	It: 2001/4130	batch_loss: 4.4129	batch_accuracy: 25.59%	lr:0.000024
Ep: 24/25	It: 2051/4130	batch_loss: 4.2402	batch_accuracy: 26.42%	lr:0.000024
Ep: 24/25	It: 2101/4130	batch_loss: 4.2941	batch_accuracy: 26.34%	lr:0.000024
Ep: 24/25	It: 2151/4130	batch_loss: 4.3706	batch_accuracy: 25.46%	lr:0.000023
Ep: 24/25	It: 2201/4130	batch_loss: 4.3898	batch_accuracy: 25.05%	lr:0.000023
Ep: 24/25	It: 2251/4130	batch_loss: 4.4366	batch_accuracy: 24.66%	lr:0.000023
Ep: 24/25	It: 2301/4130	batch_loss: 4.3414	batch_accuracy: 26.78%	lr:0.000023
Ep: 24/25	It: 2351/4130	batch_loss: 4.2674	batch_accuracy: 26.68%	lr:0.000022
Ep: 24/25	It: 2401/4130	batch_loss: 4.3012	batch_accuracy: 24.90%	lr:0.000022
Ep: 24/25	It: 2451/4130	batch_loss: 4.2845	batch_accuracy: 26.20%	lr:0.000022
Ep: 24/25	It: 2501/4130	batch_loss: 4.3448	batch_accuracy: 25.42%	lr:0.000022
Ep: 24/25	It: 2551/4130	batch_loss: 4.3114	batch_accuracy: 25.54%	lr:0.000022
Ep: 24/25	It: 2601/4130	batch_loss: 4.4109	batch_accuracy: 24.46%	lr:0.000021
Ep: 24/25	It: 2651/4130	batch_loss: 4.2546	batch_accuracy: 26.27%	lr:0.000021
Ep: 24/25	It: 2701/4130	batch_loss: 4.3275	batch_accuracy: 25.61%	lr:0.000021
Ep: 24/25	It: 2751/4130	batch_loss: 4.3849	batch_accuracy: 25.00%	lr:0.000021
Ep: 24/25	It: 2801/4130	batch_loss: 4.3282	batch_accuracy: 26.51%	lr:0.000021
Ep: 24/25	It: 2851/4130	batch_loss: 4.3477	batch_accuracy: 24.88%	lr:0.000020
Ep: 24/25	It: 2901/4130	batch_loss: 4.2780	batch_accuracy: 26.44%	lr:0.000020
Ep: 24/25	It: 2951/4130	batch_loss: 4.3168	batch_accuracy: 25.20%	lr:0.000020
Ep: 24/25	It: 3001/4130	batch_loss: 4.2865	batch_accuracy: 26.17%	lr:0.000020
Ep: 24/25	It: 3051/4130	batch_loss: 4.4489	batch_accuracy: 25.02%	lr:0.000020
Ep: 24/25	It: 3101/4130	batch_loss: 4.2418	batch_accuracy: 26.66%	lr:0.000019
Ep: 24/25	It: 3151/4130	batch_loss: 4.3761	batch_accuracy: 25.95%	lr:0.000019
Ep: 24/25	It: 3201/4130	batch_loss: 4.3760	batch_accuracy: 25.76%	lr:0.000019
Ep: 24/25	It: 3251/4130	batch_loss: 4.3173	batch_accuracy: 26.17%	lr:0.000019
Ep: 24/25	It: 3301/4130	batch_loss: 4.3242	batch_accuracy: 25.34%	lr:0.000019
Ep: 24/25	It: 3351/4130	batch_loss: 4.3349	batch_accuracy: 25.15%	lr:0.000019
Ep: 24/25	It: 3401/4130	batch_loss: 4.5216	batch_accuracy: 23.27%	lr:0.000018
Ep: 24/25	It: 3451/4130	batch_loss: 4.3382	batch_accuracy: 25.22%	lr:0.000018
Ep: 24/25	It: 3501/4130	batch_loss: 4.2753	batch_accuracy: 26.68%	lr:0.000018
Ep: 24/25	It: 3551/4130	batch_loss: 4.3003	batch_accuracy: 25.95%	lr:0.000018
Ep: 24/25	It: 3601/4130	batch_loss: 4.5272	batch_accuracy: 24.22%	lr:0.000018
Ep: 24/25	It: 3651/4130	batch_loss: 4.2932	batch_accuracy: 25.81%	lr:0.000018
Ep: 24/25	It: 3701/4130	batch_loss: 4.3206	batch_accuracy: 26.49%	lr:0.000017
Ep: 24/25	It: 3751/4130	batch_loss: 4.4599	batch_accuracy: 24.27%	lr:0.000017
Ep: 24/25	It: 3801/4130	batch_loss: 4.3759	batch_accuracy: 25.37%	lr:0.000017
Ep: 24/25	It: 3851/4130	batch_loss: 4.2654	batch_accuracy: 26.73%	lr:0.000017
Ep: 24/25	It: 3901/4130	batch_loss: 4.4283	batch_accuracy: 24.76%	lr:0.000017
Ep: 24/25	It: 3951/4130	batch_loss: 4.3227	batch_accuracy: 25.68%	lr:0.000017
Ep: 24/25	It: 4001/4130	batch_loss: 4.4043	batch_accuracy: 24.54%	lr:0.000016
Ep: 24/25	It: 4051/4130	batch_loss: 4.3545	batch_accuracy: 25.76%	lr:0.000016
Ep: 24/25	It: 4101/4130	batch_loss: 4.2537	batch_accuracy: 26.54%	lr:0.000016
Ep: 24/25	It: 4130/4130	batch_loss: 4.3262	batch_accuracy: 25.54%	lr:0.000016


Generated text for input text "You" is:
You, and bird and the friadus of the say, and the woman and in the tight were the left at one week inocyanine and airs of the right towns, with the width of the right of the born. The main side effect was determined by AFs and the mean age of the study. In this study, this study also demonstrated that the total number of the three-dimensional and non-linearity of the civil. The mean values of the cortex were not in each population.
<eot>
<sot>
Synthesis of the Time-Arga (Si) and Principles

The study of the Southern Marine-Baunhi, Slavian (Myjea) and the Peatra, Partzan, Ni-Parofan, and British, and P. Brazil. Musican. M. Br. Five Bean, P. Suhni., Part 2, 1983. Positive Lajan, B. A. P. P. Bre


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 25/25	It: 1/4130	batch_loss: 4.2476	batch_accuracy: 26.07%	lr:0.000016
Ep: 25/25	It: 51/4130	batch_loss: 4.3378	batch_accuracy: 25.27%	lr:0.000016
Ep: 25/25	It: 101/4130	batch_loss: 4.4386	batch_accuracy: 23.83%	lr:0.000016
Ep: 25/25	It: 151/4130	batch_loss: 4.3834	batch_accuracy: 26.22%	lr:0.000016
Ep: 25/25	It: 201/4130	batch_loss: 4.2683	batch_accuracy: 26.17%	lr:0.000016
Ep: 25/25	It: 251/4130	batch_loss: 4.3548	batch_accuracy: 24.88%	lr:0.000015
Ep: 25/25	It: 301/4130	batch_loss: 4.2803	batch_accuracy: 26.05%	lr:0.000015
Ep: 25/25	It: 351/4130	batch_loss: 4.3212	batch_accuracy: 25.63%	lr:0.000015
Ep: 25/25	It: 401/4130	batch_loss: 4.3610	batch_accuracy: 24.32%	lr:0.000015
Ep: 25/25	It: 451/4130	batch_loss: 4.3008	batch_accuracy: 25.88%	lr:0.000015
Ep: 25/25	It: 501/4130	batch_loss: 4.4879	batch_accuracy: 22.63%	lr:0.000015
Ep: 25/25	It: 551/4130	batch_loss: 4.3085	batch_accuracy: 25.15%	lr:0.000015
Ep: 25/25	It: 601/4130	batch_loss: 4.3282	batch_accuracy: 26.29%	lr:0.000014
Ep: 25/25	It: 651/4130	batch_loss: 4.2986	batch_accuracy: 25.68%	lr:0.000014
Ep: 25/25	It: 701/4130	batch_loss: 4.2328	batch_accuracy: 26.51%	lr:0.000014
Ep: 25/25	It: 751/4130	batch_loss: 4.4439	batch_accuracy: 24.19%	lr:0.000014
Ep: 25/25	It: 801/4130	batch_loss: 4.3749	batch_accuracy: 24.83%	lr:0.000014
Ep: 25/25	It: 851/4130	batch_loss: 4.3476	batch_accuracy: 26.05%	lr:0.000014
Ep: 25/25	It: 901/4130	batch_loss: 4.2258	batch_accuracy: 26.93%	lr:0.000014
Ep: 25/25	It: 951/4130	batch_loss: 4.3738	batch_accuracy: 24.83%	lr:0.000014
Ep: 25/25	It: 1001/4130	batch_loss: 4.3805	batch_accuracy: 25.78%	lr:0.000014
Ep: 25/25	It: 1051/4130	batch_loss: 4.3127	batch_accuracy: 26.81%	lr:0.000013
Ep: 25/25	It: 1101/4130	batch_loss: 4.3437	batch_accuracy: 25.98%	lr:0.000013
Ep: 25/25	It: 1151/4130	batch_loss: 4.4229	batch_accuracy: 24.88%	lr:0.000013
Ep: 25/25	It: 1201/4130	batch_loss: 4.1962	batch_accuracy: 27.54%	lr:0.000013
Ep: 25/25	It: 1251/4130	batch_loss: 4.2732	batch_accuracy: 26.00%	lr:0.000013
Ep: 25/25	It: 1301/4130	batch_loss: 4.1611	batch_accuracy: 28.78%	lr:0.000013
Ep: 25/25	It: 1351/4130	batch_loss: 4.3488	batch_accuracy: 26.46%	lr:0.000013
Ep: 25/25	It: 1401/4130	batch_loss: 4.3296	batch_accuracy: 26.27%	lr:0.000013
Ep: 25/25	It: 1451/4130	batch_loss: 4.4000	batch_accuracy: 24.63%	lr:0.000013
Ep: 25/25	It: 1501/4130	batch_loss: 4.3348	batch_accuracy: 25.88%	lr:0.000012
Ep: 25/25	It: 1551/4130	batch_loss: 4.3883	batch_accuracy: 24.29%	lr:0.000012
Ep: 25/25	It: 1601/4130	batch_loss: 4.2744	batch_accuracy: 26.03%	lr:0.000012
Ep: 25/25	It: 1651/4130	batch_loss: 4.2771	batch_accuracy: 25.88%	lr:0.000012
Ep: 25/25	It: 1701/4130	batch_loss: 4.4109	batch_accuracy: 24.83%	lr:0.000012
Ep: 25/25	It: 1751/4130	batch_loss: 4.3914	batch_accuracy: 25.34%	lr:0.000012
Ep: 25/25	It: 1801/4130	batch_loss: 4.5071	batch_accuracy: 23.46%	lr:0.000012
Ep: 25/25	It: 1851/4130	batch_loss: 4.3353	batch_accuracy: 25.73%	lr:0.000012
Ep: 25/25	It: 1901/4130	batch_loss: 4.2338	batch_accuracy: 26.44%	lr:0.000012
Ep: 25/25	It: 1951/4130	batch_loss: 4.1354	batch_accuracy: 28.17%	lr:0.000012
Ep: 25/25	It: 2001/4130	batch_loss: 4.2790	batch_accuracy: 26.34%	lr:0.000012
Ep: 25/25	It: 2051/4130	batch_loss: 4.3284	batch_accuracy: 25.37%	lr:0.000012
Ep: 25/25	It: 2101/4130	batch_loss: 4.4169	batch_accuracy: 24.56%	lr:0.000011
Ep: 25/25	It: 2151/4130	batch_loss: 4.1569	batch_accuracy: 27.69%	lr:0.000011
Ep: 25/25	It: 2201/4130	batch_loss: 4.3054	batch_accuracy: 26.71%	lr:0.000011
Ep: 25/25	It: 2251/4130	batch_loss: 4.3003	batch_accuracy: 24.51%	lr:0.000011
Ep: 25/25	It: 2301/4130	batch_loss: 4.2162	batch_accuracy: 26.34%	lr:0.000011
Ep: 25/25	It: 2351/4130	batch_loss: 4.2578	batch_accuracy: 26.73%	lr:0.000011
Ep: 25/25	It: 2401/4130	batch_loss: 4.3298	batch_accuracy: 25.73%	lr:0.000011
Ep: 25/25	It: 2451/4130	batch_loss: 4.2898	batch_accuracy: 26.29%	lr:0.000011
Ep: 25/25	It: 2501/4130	batch_loss: 4.3602	batch_accuracy: 25.81%	lr:0.000011
Ep: 25/25	It: 2551/4130	batch_loss: 4.3740	batch_accuracy: 25.90%	lr:0.000011
Ep: 25/25	It: 2601/4130	batch_loss: 4.2020	batch_accuracy: 26.78%	lr:0.000011
Ep: 25/25	It: 2651/4130	batch_loss: 4.3582	batch_accuracy: 25.10%	lr:0.000011
Ep: 25/25	It: 2701/4130	batch_loss: 4.2432	batch_accuracy: 27.56%	lr:0.000011
Ep: 25/25	It: 2751/4130	batch_loss: 4.3851	batch_accuracy: 24.51%	lr:0.000011
Ep: 25/25	It: 2801/4130	batch_loss: 4.3181	batch_accuracy: 26.12%	lr:0.000011
Ep: 25/25	It: 2851/4130	batch_loss: 4.1541	batch_accuracy: 28.30%	lr:0.000011
Ep: 25/25	It: 2901/4130	batch_loss: 4.2877	batch_accuracy: 26.34%	lr:0.000011
Ep: 25/25	It: 2951/4130	batch_loss: 4.4663	batch_accuracy: 24.10%	lr:0.000010
Ep: 25/25	It: 3001/4130	batch_loss: 4.3225	batch_accuracy: 26.15%	lr:0.000010
Ep: 25/25	It: 3051/4130	batch_loss: 4.3687	batch_accuracy: 24.68%	lr:0.000010
Ep: 25/25	It: 3101/4130	batch_loss: 4.5362	batch_accuracy: 23.90%	lr:0.000010
Ep: 25/25	It: 3151/4130	batch_loss: 4.2961	batch_accuracy: 25.17%	lr:0.000010
Ep: 25/25	It: 3201/4130	batch_loss: 4.3739	batch_accuracy: 25.00%	lr:0.000010
Ep: 25/25	It: 3251/4130	batch_loss: 4.2301	batch_accuracy: 26.54%	lr:0.000010
Ep: 25/25	It: 3301/4130	batch_loss: 4.2720	batch_accuracy: 27.15%	lr:0.000010
Ep: 25/25	It: 3351/4130	batch_loss: 4.2604	batch_accuracy: 27.32%	lr:0.000010
Ep: 25/25	It: 3401/4130	batch_loss: 4.3772	batch_accuracy: 25.46%	lr:0.000010
Ep: 25/25	It: 3451/4130	batch_loss: 4.2284	batch_accuracy: 25.78%	lr:0.000010
Ep: 25/25	It: 3501/4130	batch_loss: 4.3245	batch_accuracy: 25.51%	lr:0.000010
Ep: 25/25	It: 3551/4130	batch_loss: 4.1972	batch_accuracy: 27.83%	lr:0.000010
Ep: 25/25	It: 3601/4130	batch_loss: 4.3022	batch_accuracy: 25.98%	lr:0.000010
Ep: 25/25	It: 3651/4130	batch_loss: 4.4814	batch_accuracy: 25.20%	lr:0.000010
Ep: 25/25	It: 3701/4130	batch_loss: 4.2348	batch_accuracy: 26.10%	lr:0.000010
Ep: 25/25	It: 3751/4130	batch_loss: 4.3579	batch_accuracy: 25.15%	lr:0.000010
Ep: 25/25	It: 3801/4130	batch_loss: 4.3212	batch_accuracy: 25.49%	lr:0.000010
Ep: 25/25	It: 3851/4130	batch_loss: 4.3201	batch_accuracy: 25.20%	lr:0.000010
Ep: 25/25	It: 3901/4130	batch_loss: 4.3479	batch_accuracy: 24.34%	lr:0.000010
Ep: 25/25	It: 3951/4130	batch_loss: 4.2814	batch_accuracy: 25.71%	lr:0.000010
Ep: 25/25	It: 4001/4130	batch_loss: 4.3634	batch_accuracy: 25.81%	lr:0.000010
Ep: 25/25	It: 4051/4130	batch_loss: 4.4158	batch_accuracy: 25.44%	lr:0.000010
Ep: 25/25	It: 4101/4130	batch_loss: 4.3309	batch_accuracy: 25.17%	lr:0.000010
Ep: 25/25	It: 4130/4130	batch_loss: 4.3030	batch_accuracy: 25.89%	lr:0.000010


Generated text for input text "You" is:
Youvertiana, M. B. J. Mn, J. Buan et al. Sni, 2009. 2011. Bre, Mira, Panu, Samha, Banon, L. J. A., 1981. L, Jav, 2009. Perspective, and L. H. H. A., P. Blood. The Part 1.04) was a major cause of the disease. The results suggest that the Mn, C. gondia, the C-terminal P. papillaryis and P. sacrifice.
<eot>
<sot>
Secretion of the Type II of Primary Pressure Pressure of C. M. C. Breast, M. A. Lawa. T. C. B. P. A. Loo-Hodgress, A. L. C. Fiber.
<eot>
<sot>
Thermal Function of the British and Burk, Mard.

Analysis of the Laws (A


Ended at 2025-04-18 15:42:57
Duration: 1:30:42.120966
Job finished at Fri Apr 18 15:43:46 CDT 2025

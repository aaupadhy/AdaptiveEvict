# Large Language Models (LLM) from Scratch in PyTorch
### Simplified Scratch Pytorch Implementation of Large Language Models (LLM) with Detailed Steps (Refer to <a href="gpt.py">gpt.py</a> and <a href="llama.py">llama.py</a>)

### Key Points
<ul>
  <li> Contains two models: <a href="gpt.py">GPT</a> and <a href="llama.py">LLAMA</a>.</li>
  <li> GPT model serves as the base simple decoder-only transformer and is easier to learn.</li>
  <li> LLAMA contains advanced concepts: Rotational Positional Encoding (RoPe), SwishGLU, RMSNorm, Mixture of Experts, etc. (Refer below.) </li>
  <li> These models are scaled-down versions of their original architectures. Number of parameters: k (GPT) and k (LLAMA) </li>
  <li> Downloads the Taylor Swift song lyrics dataset by default for training. </li>
</ul>  

## Implementation Status (LLAMA):
:white_check_mark: ByTe-Pair Tokenization <a href="tokenizer.py">[Here]</a>   <br>
:white_check_mark: Temperature, Top-p and Top-k   <a href="https://github.com/s-chh/PyTorch-Scratch-LLM/blob/186ccf6de0ee0b81a27191c0dafaacf66f6acd30/solver.py#L131">[Here]</a>   <br> 
:white_check_mark: RMSNorm      <a href="https://github.com/s-chh/PyTorch-Scratch-LLM/blob/186ccf6de0ee0b81a27191c0dafaacf66f6acd30/llama.py#L232">[Here]</a>  
:white_check_mark: SWiGLU      <a href="https://github.com/s-chh/PyTorch-Scratch-LLM/blob/186ccf6de0ee0b81a27191c0dafaacf66f6acd30/llama.py#L257">[Here]</a>  
:white_check_mark: Rotational Positional Embedding (RoPe)  <a href="https://github.com/s-chh/PyTorch-Scratch-LLM/blob/186ccf6de0ee0b81a27191c0dafaacf66f6acd30/llama.py#L44">[Here]</a>   <br>
:white_check_mark: KV Cache <a href="https://github.com/s-chh/PyTorch-Scratch-LLM/blob/186ccf6de0ee0b81a27191c0dafaacf66f6acd30/llama.py#L186">[Here]</a>   <br>
:white_check_mark: Mixture of Experts <a href="https://github.com/s-chh/PyTorch-Scratch-LLM/blob/186ccf6de0ee0b81a27191c0dafaacf66f6acd30/llama.py#L310">[Here]</a>   <br>
:white_square_button: Grouped Query Attention <br>
:white_square_button: Infini Attention

Feel free to comment if you want anything integrated here.


## Run command: <br>

```
python main.py --network_type llama
```
 
- The network can be selected between llama and gpt.
- I have tested the model on Taylor Swift song lyrics.
- By default, the Taylor Swift song lyrics dataset will be downloaded to the text file (default name: "data.txt").
- To use a custom dataset, replace the content of the file or provide a different text file using the data_file argument.

## Sample Output 
A sample output generated by my trained model:
<code>
Model training in progress.
</code>

## Input Arguments of <a href="main.py">main.py</a>

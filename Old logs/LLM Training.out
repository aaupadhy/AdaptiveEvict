Job started at Sun Apr 20 20:07:55 CDT 2025
Running on g071.grace.hprc.tamu.edu
Sun Apr 20 20:07:55 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-PCIE-40GB          On  | 00000000:D8:00.0 Off |                  Off |
| N/A   34C    P0              33W / 250W |      0MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
INFO:get_data:Preparing dataset.
INFO:get_data:Loading dataset from Hugging Face...
INFO:get_data:Using all available documents in the dataset
INFO:get_data:Processing dataset...
INFO:get_data:Data preparation complete.
INFO:get_data:Using data from ./data/data.txt



Started at 2025-04-20 20:10:25
('batch_size', 64)
('data_file', 'data.txt')
('data_path', './data/')
('dropout', 0.1)
('embed_dim', 256)
('epochs', 10)
('forward_mul', 4)
('gen_tokens_len', 256)
('input_text', 'You')
('load_model', False)
('load_tokenizer', False)
('lr', 0.001)
('max_merged_tokens', 20000)
('model_path', './saved_models')
('n_heads', 8)
('n_layers', 8)
('n_workers', 2)
('network_type', 'llama')
('temperature', 1.0)
('test_only', False)
('top_k', 10)
('top_p', 0.6)
('train_tokens_len', 256)
('warmup_epochs', 5)

Tokenizing data file...
73049927 tokens created from the file. Each epoch will have 285351 batches.
Number of trainable parameters in the model: 38832769
Number of tokens per parameters: 1.8811.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 1/10	It: 1/4459	batch_loss: 10.1851	batch_accuracy: 0.01%	lr:0.000010
Ep: 1/10	It: 51/4459	batch_loss: 9.8108	batch_accuracy: 4.00%	lr:0.000012
Ep: 1/10	It: 101/4459	batch_loss: 9.3506	batch_accuracy: 8.89%	lr:0.000014
Ep: 1/10	It: 151/4459	batch_loss: 8.9759	batch_accuracy: 8.48%	lr:0.000017
Ep: 1/10	It: 201/4459	batch_loss: 8.6511	batch_accuracy: 9.01%	lr:0.000019
Ep: 1/10	It: 251/4459	batch_loss: 8.4434	batch_accuracy: 8.57%	lr:0.000021
Ep: 1/10	It: 301/4459	batch_loss: 8.2267	batch_accuracy: 9.21%	lr:0.000023
Ep: 1/10	It: 351/4459	batch_loss: 8.0022	batch_accuracy: 9.63%	lr:0.000026
Ep: 1/10	It: 401/4459	batch_loss: 7.8100	batch_accuracy: 10.27%	lr:0.000028
Ep: 1/10	It: 451/4459	batch_loss: 7.5913	batch_accuracy: 10.74%	lr:0.000030
Ep: 1/10	It: 501/4459	batch_loss: 7.5154	batch_accuracy: 11.13%	lr:0.000032
Ep: 1/10	It: 551/4459	batch_loss: 7.3987	batch_accuracy: 12.24%	lr:0.000034
Ep: 1/10	It: 601/4459	batch_loss: 7.2090	batch_accuracy: 13.61%	lr:0.000037
Ep: 1/10	It: 651/4459	batch_loss: 7.1547	batch_accuracy: 14.61%	lr:0.000039
Ep: 1/10	It: 701/4459	batch_loss: 7.0152	batch_accuracy: 15.62%	lr:0.000041
Ep: 1/10	It: 751/4459	batch_loss: 6.8993	batch_accuracy: 16.53%	lr:0.000043
Ep: 1/10	It: 801/4459	batch_loss: 6.8038	batch_accuracy: 17.18%	lr:0.000046
Ep: 1/10	It: 851/4459	batch_loss: 6.7537	batch_accuracy: 18.04%	lr:0.000048
Ep: 1/10	It: 901/4459	batch_loss: 6.7976	batch_accuracy: 16.89%	lr:0.000050
Ep: 1/10	It: 951/4459	batch_loss: 6.5344	batch_accuracy: 19.49%	lr:0.000052
Ep: 1/10	It: 1001/4459	batch_loss: 6.5674	batch_accuracy: 18.28%	lr:0.000054
Ep: 1/10	It: 1051/4459	batch_loss: 6.4769	batch_accuracy: 19.20%	lr:0.000057
Ep: 1/10	It: 1101/4459	batch_loss: 6.3447	batch_accuracy: 19.77%	lr:0.000059
Ep: 1/10	It: 1151/4459	batch_loss: 6.3547	batch_accuracy: 19.38%	lr:0.000061
Ep: 1/10	It: 1201/4459	batch_loss: 6.3063	batch_accuracy: 19.64%	lr:0.000063
Ep: 1/10	It: 1251/4459	batch_loss: 6.2929	batch_accuracy: 18.78%	lr:0.000066
Ep: 1/10	It: 1301/4459	batch_loss: 6.2244	batch_accuracy: 19.37%	lr:0.000068
Ep: 1/10	It: 1351/4459	batch_loss: 6.0683	batch_accuracy: 20.68%	lr:0.000070
Ep: 1/10	It: 1401/4459	batch_loss: 6.0727	batch_accuracy: 20.63%	lr:0.000072
Ep: 1/10	It: 1451/4459	batch_loss: 5.9547	batch_accuracy: 20.65%	lr:0.000074
Ep: 1/10	It: 1501/4459	batch_loss: 5.9628	batch_accuracy: 20.34%	lr:0.000077
Ep: 1/10	It: 1551/4459	batch_loss: 5.9918	batch_accuracy: 20.56%	lr:0.000079
Ep: 1/10	It: 1601/4459	batch_loss: 5.8563	batch_accuracy: 21.23%	lr:0.000081
Ep: 1/10	It: 1651/4459	batch_loss: 5.6717	batch_accuracy: 23.24%	lr:0.000083
Ep: 1/10	It: 1701/4459	batch_loss: 5.7188	batch_accuracy: 22.33%	lr:0.000086
Ep: 1/10	It: 1751/4459	batch_loss: 5.8305	batch_accuracy: 21.86%	lr:0.000088
Ep: 1/10	It: 1801/4459	batch_loss: 5.6454	batch_accuracy: 22.45%	lr:0.000090
Ep: 1/10	It: 1851/4459	batch_loss: 5.6751	batch_accuracy: 21.70%	lr:0.000092
Ep: 1/10	It: 1901/4459	batch_loss: 5.6672	batch_accuracy: 22.31%	lr:0.000094
Ep: 1/10	It: 1951/4459	batch_loss: 5.4825	batch_accuracy: 23.51%	lr:0.000097
Ep: 1/10	It: 2001/4459	batch_loss: 5.6468	batch_accuracy: 22.27%	lr:0.000099
Ep: 1/10	It: 2051/4459	batch_loss: 5.3839	batch_accuracy: 23.81%	lr:0.000101
Ep: 1/10	It: 2101/4459	batch_loss: 5.4238	batch_accuracy: 23.47%	lr:0.000103
Ep: 1/10	It: 2151/4459	batch_loss: 5.4802	batch_accuracy: 23.19%	lr:0.000106
Ep: 1/10	It: 2201/4459	batch_loss: 5.3446	batch_accuracy: 24.20%	lr:0.000108
Ep: 1/10	It: 2251/4459	batch_loss: 5.1773	batch_accuracy: 25.59%	lr:0.000110
Ep: 1/10	It: 2301/4459	batch_loss: 5.2264	batch_accuracy: 24.98%	lr:0.000112
Ep: 1/10	It: 2351/4459	batch_loss: 5.1959	batch_accuracy: 24.52%	lr:0.000114
Ep: 1/10	It: 2401/4459	batch_loss: 5.2173	batch_accuracy: 25.21%	lr:0.000117
Ep: 1/10	It: 2451/4459	batch_loss: 5.3077	batch_accuracy: 24.33%	lr:0.000119
Ep: 1/10	It: 2501/4459	batch_loss: 5.0970	batch_accuracy: 26.21%	lr:0.000121
Ep: 1/10	It: 2551/4459	batch_loss: 5.1680	batch_accuracy: 25.42%	lr:0.000123
Ep: 1/10	It: 2601/4459	batch_loss: 5.0993	batch_accuracy: 24.93%	lr:0.000125
Ep: 1/10	It: 2651/4459	batch_loss: 5.0575	batch_accuracy: 25.78%	lr:0.000128
Ep: 1/10	It: 2701/4459	batch_loss: 5.0524	batch_accuracy: 26.56%	lr:0.000130
Ep: 1/10	It: 2751/4459	batch_loss: 4.9421	batch_accuracy: 27.26%	lr:0.000132
Ep: 1/10	It: 2801/4459	batch_loss: 5.0798	batch_accuracy: 25.58%	lr:0.000134
Ep: 1/10	It: 2851/4459	batch_loss: 4.9811	batch_accuracy: 26.29%	lr:0.000137
Ep: 1/10	It: 2901/4459	batch_loss: 5.0006	batch_accuracy: 26.28%	lr:0.000139
Ep: 1/10	It: 2951/4459	batch_loss: 4.9459	batch_accuracy: 26.15%	lr:0.000141
Ep: 1/10	It: 3001/4459	batch_loss: 4.9198	batch_accuracy: 26.59%	lr:0.000143
Ep: 1/10	It: 3051/4459	batch_loss: 4.8308	batch_accuracy: 26.72%	lr:0.000145
Ep: 1/10	It: 3101/4459	batch_loss: 4.8624	batch_accuracy: 26.64%	lr:0.000148
Ep: 1/10	It: 3151/4459	batch_loss: 4.8065	batch_accuracy: 27.12%	lr:0.000150
Ep: 1/10	It: 3201/4459	batch_loss: 4.7139	batch_accuracy: 28.24%	lr:0.000152
Ep: 1/10	It: 3251/4459	batch_loss: 4.8671	batch_accuracy: 26.61%	lr:0.000154
Ep: 1/10	It: 3301/4459	batch_loss: 4.7875	batch_accuracy: 26.63%	lr:0.000157
Ep: 1/10	It: 3351/4459	batch_loss: 4.8133	batch_accuracy: 27.08%	lr:0.000159
Ep: 1/10	It: 3401/4459	batch_loss: 4.8415	batch_accuracy: 27.11%	lr:0.000161
Ep: 1/10	It: 3451/4459	batch_loss: 4.7530	batch_accuracy: 27.16%	lr:0.000163
Ep: 1/10	It: 3501/4459	batch_loss: 4.8042	batch_accuracy: 26.47%	lr:0.000165
Ep: 1/10	It: 3551/4459	batch_loss: 4.6635	batch_accuracy: 27.51%	lr:0.000168
Ep: 1/10	It: 3601/4459	batch_loss: 4.6315	batch_accuracy: 28.05%	lr:0.000170
Ep: 1/10	It: 3651/4459	batch_loss: 4.5640	batch_accuracy: 28.77%	lr:0.000172
Ep: 1/10	It: 3701/4459	batch_loss: 4.6368	batch_accuracy: 28.51%	lr:0.000174
Ep: 1/10	It: 3751/4459	batch_loss: 4.5608	batch_accuracy: 28.66%	lr:0.000177
Ep: 1/10	It: 3801/4459	batch_loss: 4.6413	batch_accuracy: 27.97%	lr:0.000179
Ep: 1/10	It: 3851/4459	batch_loss: 4.5445	batch_accuracy: 28.22%	lr:0.000181
Ep: 1/10	It: 3901/4459	batch_loss: 4.5367	batch_accuracy: 29.23%	lr:0.000183
Ep: 1/10	It: 3951/4459	batch_loss: 4.5265	batch_accuracy: 29.28%	lr:0.000185
Ep: 1/10	It: 4001/4459	batch_loss: 4.5129	batch_accuracy: 28.56%	lr:0.000188
Ep: 1/10	It: 4051/4459	batch_loss: 4.4146	batch_accuracy: 29.49%	lr:0.000190
Ep: 1/10	It: 4101/4459	batch_loss: 4.4809	batch_accuracy: 29.15%	lr:0.000192
Ep: 1/10	It: 4151/4459	batch_loss: 4.4188	batch_accuracy: 29.33%	lr:0.000194
Ep: 1/10	It: 4201/4459	batch_loss: 4.4521	batch_accuracy: 28.86%	lr:0.000197
Ep: 1/10	It: 4251/4459	batch_loss: 4.4172	batch_accuracy: 29.96%	lr:0.000199
Ep: 1/10	It: 4301/4459	batch_loss: 4.4123	batch_accuracy: 29.49%	lr:0.000201
Ep: 1/10	It: 4351/4459	batch_loss: 4.2781	batch_accuracy: 30.15%	lr:0.000203
Ep: 1/10	It: 4401/4459	batch_loss: 4.3896	batch_accuracy: 29.22%	lr:0.000205
Ep: 1/10	It: 4451/4459	batch_loss: 4.4605	batch_accuracy: 28.36%	lr:0.000208
Ep: 1/10	It: 4459/4459	batch_loss: 4.4736	batch_accuracy: 29.34%	lr:0.000208


Generated text for input text "You" is:
You might be at first glance, or even even though, they're just a few ways to create a sense of connection that has been made for all things and what truly is the power of the universe.

As we explore this process, let us take a closer look at what we mean by the extract above. We will also explore different types of objects, their types, and applications, and its implications for various scenarios. So let's explore some common elements and their significance:

* Understanding the historical and historical significance of these elements in the context of the film.
* How can we ensure that the art of a character is able to produce their own emotions.
* Can it be a piece of jewelry that is both the and a.

Imagine you have a place where you're going to learn from their favorite characters, like a special place in your favorite series. That's where our own story is called 'n.' But what makes this is so special? What if we don't go to the same way for this place? Let's take a closer look at what we mean by our previous example.

The idea of a series is a set of steps that can be found in our own set of numbers. The first number and numbers, and


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 2/10	It: 1/4459	batch_loss: 4.4138	batch_accuracy: 29.19%	lr:0.000208
Ep: 2/10	It: 51/4459	batch_loss: 4.4106	batch_accuracy: 28.85%	lr:0.000210
Ep: 2/10	It: 101/4459	batch_loss: 4.3637	batch_accuracy: 29.25%	lr:0.000212
Ep: 2/10	It: 151/4459	batch_loss: 4.3294	batch_accuracy: 29.57%	lr:0.000215
Ep: 2/10	It: 201/4459	batch_loss: 4.3085	batch_accuracy: 30.95%	lr:0.000217
Ep: 2/10	It: 251/4459	batch_loss: 4.3075	batch_accuracy: 30.69%	lr:0.000219
Ep: 2/10	It: 301/4459	batch_loss: 4.3532	batch_accuracy: 29.08%	lr:0.000221
Ep: 2/10	It: 351/4459	batch_loss: 4.3126	batch_accuracy: 29.96%	lr:0.000224
Ep: 2/10	It: 401/4459	batch_loss: 4.2220	batch_accuracy: 30.33%	lr:0.000226
Ep: 2/10	It: 451/4459	batch_loss: 4.2611	batch_accuracy: 30.76%	lr:0.000228
Ep: 2/10	It: 501/4459	batch_loss: 4.2728	batch_accuracy: 29.97%	lr:0.000230
Ep: 2/10	It: 551/4459	batch_loss: 4.3019	batch_accuracy: 30.00%	lr:0.000232
Ep: 2/10	It: 601/4459	batch_loss: 4.3663	batch_accuracy: 28.77%	lr:0.000235
Ep: 2/10	It: 651/4459	batch_loss: 4.2443	batch_accuracy: 30.33%	lr:0.000237
Ep: 2/10	It: 701/4459	batch_loss: 4.2393	batch_accuracy: 30.00%	lr:0.000239
Ep: 2/10	It: 751/4459	batch_loss: 4.1463	batch_accuracy: 31.56%	lr:0.000241
Ep: 2/10	It: 801/4459	batch_loss: 4.1406	batch_accuracy: 31.29%	lr:0.000244
Ep: 2/10	It: 851/4459	batch_loss: 4.1732	batch_accuracy: 30.88%	lr:0.000246
Ep: 2/10	It: 901/4459	batch_loss: 4.1740	batch_accuracy: 31.15%	lr:0.000248
Ep: 2/10	It: 951/4459	batch_loss: 4.1046	batch_accuracy: 31.71%	lr:0.000250
Ep: 2/10	It: 1001/4459	batch_loss: 4.2969	batch_accuracy: 28.99%	lr:0.000252
Ep: 2/10	It: 1051/4459	batch_loss: 4.0825	batch_accuracy: 31.62%	lr:0.000255
Ep: 2/10	It: 1101/4459	batch_loss: 4.1980	batch_accuracy: 31.16%	lr:0.000257
Ep: 2/10	It: 1151/4459	batch_loss: 4.0667	batch_accuracy: 31.63%	lr:0.000259
Ep: 2/10	It: 1201/4459	batch_loss: 4.1985	batch_accuracy: 30.40%	lr:0.000261
Ep: 2/10	It: 1251/4459	batch_loss: 4.1665	batch_accuracy: 30.48%	lr:0.000264
Ep: 2/10	It: 1301/4459	batch_loss: 4.0601	batch_accuracy: 31.59%	lr:0.000266
Ep: 2/10	It: 1351/4459	batch_loss: 4.1558	batch_accuracy: 30.63%	lr:0.000268
Ep: 2/10	It: 1401/4459	batch_loss: 4.0238	batch_accuracy: 31.52%	lr:0.000270
Ep: 2/10	It: 1451/4459	batch_loss: 4.1202	batch_accuracy: 31.05%	lr:0.000272
Ep: 2/10	It: 1501/4459	batch_loss: 4.0448	batch_accuracy: 31.38%	lr:0.000275
Ep: 2/10	It: 1551/4459	batch_loss: 3.9919	batch_accuracy: 32.48%	lr:0.000277
Ep: 2/10	It: 1601/4459	batch_loss: 4.0240	batch_accuracy: 31.34%	lr:0.000279
Ep: 2/10	It: 1651/4459	batch_loss: 4.1203	batch_accuracy: 31.23%	lr:0.000281
Ep: 2/10	It: 1701/4459	batch_loss: 4.0316	batch_accuracy: 31.42%	lr:0.000284
Ep: 2/10	It: 1751/4459	batch_loss: 4.0340	batch_accuracy: 31.87%	lr:0.000286
Ep: 2/10	It: 1801/4459	batch_loss: 3.9763	batch_accuracy: 32.75%	lr:0.000288
Ep: 2/10	It: 1851/4459	batch_loss: 4.0728	batch_accuracy: 31.61%	lr:0.000290
Ep: 2/10	It: 1901/4459	batch_loss: 4.0276	batch_accuracy: 32.45%	lr:0.000292
Ep: 2/10	It: 1951/4459	batch_loss: 3.9801	batch_accuracy: 32.10%	lr:0.000295
Ep: 2/10	It: 2001/4459	batch_loss: 4.0708	batch_accuracy: 31.36%	lr:0.000297
Ep: 2/10	It: 2051/4459	batch_loss: 3.9959	batch_accuracy: 31.85%	lr:0.000299
Ep: 2/10	It: 2101/4459	batch_loss: 4.0069	batch_accuracy: 31.86%	lr:0.000301
Ep: 2/10	It: 2151/4459	batch_loss: 4.0584	batch_accuracy: 31.40%	lr:0.000304
Ep: 2/10	It: 2201/4459	batch_loss: 4.0133	batch_accuracy: 31.78%	lr:0.000306
Ep: 2/10	It: 2251/4459	batch_loss: 3.9048	batch_accuracy: 32.71%	lr:0.000308
Ep: 2/10	It: 2301/4459	batch_loss: 3.8546	batch_accuracy: 33.84%	lr:0.000310
Ep: 2/10	It: 2351/4459	batch_loss: 3.8460	batch_accuracy: 33.14%	lr:0.000312
Ep: 2/10	It: 2401/4459	batch_loss: 3.9180	batch_accuracy: 32.58%	lr:0.000315
Ep: 2/10	It: 2451/4459	batch_loss: 3.8804	batch_accuracy: 33.17%	lr:0.000317
Ep: 2/10	It: 2501/4459	batch_loss: 3.9737	batch_accuracy: 31.62%	lr:0.000319
Ep: 2/10	It: 2551/4459	batch_loss: 3.9038	batch_accuracy: 32.78%	lr:0.000321
Ep: 2/10	It: 2601/4459	batch_loss: 3.9119	batch_accuracy: 32.40%	lr:0.000323
Ep: 2/10	It: 2651/4459	batch_loss: 3.9346	batch_accuracy: 32.75%	lr:0.000326
Ep: 2/10	It: 2701/4459	batch_loss: 3.9744	batch_accuracy: 32.09%	lr:0.000328
Ep: 2/10	It: 2751/4459	batch_loss: 3.9391	batch_accuracy: 32.62%	lr:0.000330
Ep: 2/10	It: 2801/4459	batch_loss: 3.8906	batch_accuracy: 33.39%	lr:0.000332
Ep: 2/10	It: 2851/4459	batch_loss: 3.9595	batch_accuracy: 32.64%	lr:0.000335
Ep: 2/10	It: 2901/4459	batch_loss: 3.8031	batch_accuracy: 33.85%	lr:0.000337
Ep: 2/10	It: 2951/4459	batch_loss: 3.9444	batch_accuracy: 32.47%	lr:0.000339
Ep: 2/10	It: 3001/4459	batch_loss: 3.9167	batch_accuracy: 32.84%	lr:0.000341
Ep: 2/10	It: 3051/4459	batch_loss: 3.8172	batch_accuracy: 33.81%	lr:0.000343
Ep: 2/10	It: 3101/4459	batch_loss: 3.8198	batch_accuracy: 33.18%	lr:0.000346
Ep: 2/10	It: 3151/4459	batch_loss: 3.8542	batch_accuracy: 32.55%	lr:0.000348
Ep: 2/10	It: 3201/4459	batch_loss: 3.8965	batch_accuracy: 32.45%	lr:0.000350
Ep: 2/10	It: 3251/4459	batch_loss: 3.8513	batch_accuracy: 33.15%	lr:0.000352
Ep: 2/10	It: 3301/4459	batch_loss: 3.7673	batch_accuracy: 34.45%	lr:0.000355
Ep: 2/10	It: 3351/4459	batch_loss: 3.8374	batch_accuracy: 33.20%	lr:0.000357
Ep: 2/10	It: 3401/4459	batch_loss: 3.8814	batch_accuracy: 32.51%	lr:0.000359
Ep: 2/10	It: 3451/4459	batch_loss: 3.8568	batch_accuracy: 33.27%	lr:0.000361
Ep: 2/10	It: 3501/4459	batch_loss: 3.7512	batch_accuracy: 34.11%	lr:0.000363
Ep: 2/10	It: 3551/4459	batch_loss: 3.7893	batch_accuracy: 33.79%	lr:0.000366
Ep: 2/10	It: 3601/4459	batch_loss: 3.7871	batch_accuracy: 33.87%	lr:0.000368
Ep: 2/10	It: 3651/4459	batch_loss: 3.7615	batch_accuracy: 34.33%	lr:0.000370
Ep: 2/10	It: 3701/4459	batch_loss: 3.7810	batch_accuracy: 33.28%	lr:0.000372
Ep: 2/10	It: 3751/4459	batch_loss: 3.7324	batch_accuracy: 34.48%	lr:0.000375
Ep: 2/10	It: 3801/4459	batch_loss: 3.7637	batch_accuracy: 33.94%	lr:0.000377
Ep: 2/10	It: 3851/4459	batch_loss: 3.8315	batch_accuracy: 33.34%	lr:0.000379
Ep: 2/10	It: 3901/4459	batch_loss: 3.7660	batch_accuracy: 34.50%	lr:0.000381
Ep: 2/10	It: 3951/4459	batch_loss: 3.8413	batch_accuracy: 33.15%	lr:0.000383
Ep: 2/10	It: 4001/4459	batch_loss: 3.8130	batch_accuracy: 33.48%	lr:0.000386
Ep: 2/10	It: 4051/4459	batch_loss: 3.8947	batch_accuracy: 32.66%	lr:0.000388
Ep: 2/10	It: 4101/4459	batch_loss: 3.7395	batch_accuracy: 34.75%	lr:0.000390
Ep: 2/10	It: 4151/4459	batch_loss: 3.7464	batch_accuracy: 34.49%	lr:0.000392
Ep: 2/10	It: 4201/4459	batch_loss: 3.6899	batch_accuracy: 35.49%	lr:0.000395
Ep: 2/10	It: 4251/4459	batch_loss: 3.6887	batch_accuracy: 34.80%	lr:0.000397
Ep: 2/10	It: 4301/4459	batch_loss: 3.7610	batch_accuracy: 33.75%	lr:0.000399
Ep: 2/10	It: 4351/4459	batch_loss: 3.7219	batch_accuracy: 34.26%	lr:0.000401
Ep: 2/10	It: 4401/4459	batch_loss: 3.7127	batch_accuracy: 34.88%	lr:0.000403
Ep: 2/10	It: 4451/4459	batch_loss: 3.8578	batch_accuracy: 33.01%	lr:0.000406
Ep: 2/10	It: 4459/4459	batch_loss: 3.8279	batch_accuracy: 33.12%	lr:0.000406


Generated text for input text "You" is:
You know, "I think we're right!" Just like how we can create toys around us called “bird.” This idea helps us see things like when they come together – they are all together to make things easier to see!

Now imagine having more toys in different ways that we want to keep in mind. We want to see if we can't use a friend or friend, like if we want to figure out how to see something as we get our toy box. The idea of this special number is called the "destest number." But don't that mean that the number of toys you can do to get one?

Now, imagine that our toys get really down and down, but instead of it, we need to see all the boxes. We would like we use the numbers instead of our toy box and even try adding a number. This is like how many marbles are, and they are all that many people will call them out.

So, what happens when we add up to our toy box, like our friend's number line? You want to find out how many numbers will call the "something," which looks like that!" And if you're looking at the number of candies on our number, they could see that they would


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 3/10	It: 1/4459	batch_loss: 3.7906	batch_accuracy: 33.55%	lr:0.000406
Ep: 3/10	It: 51/4459	batch_loss: 3.7492	batch_accuracy: 33.80%	lr:0.000408
Ep: 3/10	It: 101/4459	batch_loss: 3.7139	batch_accuracy: 34.31%	lr:0.000410
Ep: 3/10	It: 151/4459	batch_loss: 3.7448	batch_accuracy: 34.52%	lr:0.000413
Ep: 3/10	It: 201/4459	batch_loss: 3.6690	batch_accuracy: 35.51%	lr:0.000415
Ep: 3/10	It: 251/4459	batch_loss: 3.8184	batch_accuracy: 33.48%	lr:0.000417
Ep: 3/10	It: 301/4459	batch_loss: 3.7808	batch_accuracy: 34.33%	lr:0.000419
Ep: 3/10	It: 351/4459	batch_loss: 3.6807	batch_accuracy: 34.84%	lr:0.000422
Ep: 3/10	It: 401/4459	batch_loss: 3.7281	batch_accuracy: 33.83%	lr:0.000424
Ep: 3/10	It: 451/4459	batch_loss: 3.7767	batch_accuracy: 34.16%	lr:0.000426
Ep: 3/10	It: 501/4459	batch_loss: 3.7156	batch_accuracy: 33.95%	lr:0.000428
Ep: 3/10	It: 551/4459	batch_loss: 3.5386	batch_accuracy: 36.49%	lr:0.000430
Ep: 3/10	It: 601/4459	batch_loss: 3.8173	batch_accuracy: 32.86%	lr:0.000433
Ep: 3/10	It: 651/4459	batch_loss: 3.7379	batch_accuracy: 34.44%	lr:0.000435
Ep: 3/10	It: 701/4459	batch_loss: 3.6204	batch_accuracy: 35.85%	lr:0.000437
Ep: 3/10	It: 751/4459	batch_loss: 3.7569	batch_accuracy: 34.36%	lr:0.000439
Ep: 3/10	It: 801/4459	batch_loss: 3.6472	batch_accuracy: 34.97%	lr:0.000442
Ep: 3/10	It: 851/4459	batch_loss: 3.7729	batch_accuracy: 33.58%	lr:0.000444
Ep: 3/10	It: 901/4459	batch_loss: 3.6755	batch_accuracy: 35.45%	lr:0.000446
Ep: 3/10	It: 951/4459	batch_loss: 3.7489	batch_accuracy: 34.56%	lr:0.000448
Ep: 3/10	It: 1001/4459	batch_loss: 3.5881	batch_accuracy: 36.10%	lr:0.000450
Ep: 3/10	It: 1051/4459	batch_loss: 3.6326	batch_accuracy: 34.59%	lr:0.000453
Ep: 3/10	It: 1101/4459	batch_loss: 3.6280	batch_accuracy: 35.54%	lr:0.000455
Ep: 3/10	It: 1151/4459	batch_loss: 3.5343	batch_accuracy: 36.31%	lr:0.000457
Ep: 3/10	It: 1201/4459	batch_loss: 3.7204	batch_accuracy: 34.33%	lr:0.000459
Ep: 3/10	It: 1251/4459	batch_loss: 3.6867	batch_accuracy: 34.77%	lr:0.000462
Ep: 3/10	It: 1301/4459	batch_loss: 3.6248	batch_accuracy: 36.27%	lr:0.000464
Ep: 3/10	It: 1351/4459	batch_loss: 3.6880	batch_accuracy: 34.91%	lr:0.000466
Ep: 3/10	It: 1401/4459	batch_loss: 3.7027	batch_accuracy: 34.66%	lr:0.000468
Ep: 3/10	It: 1451/4459	batch_loss: 3.6842	batch_accuracy: 34.31%	lr:0.000470
Ep: 3/10	It: 1501/4459	batch_loss: 3.6348	batch_accuracy: 35.49%	lr:0.000473
Ep: 3/10	It: 1551/4459	batch_loss: 3.6870	batch_accuracy: 34.83%	lr:0.000475
Ep: 3/10	It: 1601/4459	batch_loss: 3.7386	batch_accuracy: 34.30%	lr:0.000477
Ep: 3/10	It: 1651/4459	batch_loss: 3.6814	batch_accuracy: 35.06%	lr:0.000479
Ep: 3/10	It: 1701/4459	batch_loss: 3.7281	batch_accuracy: 34.30%	lr:0.000482
Ep: 3/10	It: 1751/4459	batch_loss: 3.5662	batch_accuracy: 35.49%	lr:0.000484
Ep: 3/10	It: 1801/4459	batch_loss: 3.5252	batch_accuracy: 36.17%	lr:0.000486
Ep: 3/10	It: 1851/4459	batch_loss: 3.6267	batch_accuracy: 35.75%	lr:0.000488
Ep: 3/10	It: 1901/4459	batch_loss: 3.5732	batch_accuracy: 35.73%	lr:0.000490
Ep: 3/10	It: 1951/4459	batch_loss: 3.6456	batch_accuracy: 35.71%	lr:0.000493
Ep: 3/10	It: 2001/4459	batch_loss: 3.5976	batch_accuracy: 36.02%	lr:0.000495
Ep: 3/10	It: 2051/4459	batch_loss: 3.5948	batch_accuracy: 35.40%	lr:0.000497
Ep: 3/10	It: 2101/4459	batch_loss: 3.5931	batch_accuracy: 36.47%	lr:0.000499
Ep: 3/10	It: 2151/4459	batch_loss: 3.6350	batch_accuracy: 35.20%	lr:0.000502
Ep: 3/10	It: 2201/4459	batch_loss: 3.5316	batch_accuracy: 36.45%	lr:0.000504
Ep: 3/10	It: 2251/4459	batch_loss: 3.7284	batch_accuracy: 34.83%	lr:0.000506
Ep: 3/10	It: 2301/4459	batch_loss: 3.6208	batch_accuracy: 34.97%	lr:0.000508
Ep: 3/10	It: 2351/4459	batch_loss: 3.4929	batch_accuracy: 36.71%	lr:0.000510
Ep: 3/10	It: 2401/4459	batch_loss: 3.6416	batch_accuracy: 35.28%	lr:0.000513
Ep: 3/10	It: 2451/4459	batch_loss: 3.6714	batch_accuracy: 35.40%	lr:0.000515
Ep: 3/10	It: 2501/4459	batch_loss: 3.5590	batch_accuracy: 36.44%	lr:0.000517
Ep: 3/10	It: 2551/4459	batch_loss: 3.5025	batch_accuracy: 37.07%	lr:0.000519
Ep: 3/10	It: 2601/4459	batch_loss: 3.5290	batch_accuracy: 36.33%	lr:0.000521
Ep: 3/10	It: 2651/4459	batch_loss: 3.6379	batch_accuracy: 35.42%	lr:0.000524
Ep: 3/10	It: 2701/4459	batch_loss: 3.5102	batch_accuracy: 36.98%	lr:0.000526
Ep: 3/10	It: 2751/4459	batch_loss: 3.6272	batch_accuracy: 35.06%	lr:0.000528
Ep: 3/10	It: 2801/4459	batch_loss: 3.5938	batch_accuracy: 35.89%	lr:0.000530
Ep: 3/10	It: 2851/4459	batch_loss: 3.6251	batch_accuracy: 35.66%	lr:0.000533
Ep: 3/10	It: 2901/4459	batch_loss: 3.5668	batch_accuracy: 36.02%	lr:0.000535
Ep: 3/10	It: 2951/4459	batch_loss: 3.6169	batch_accuracy: 35.00%	lr:0.000537
Ep: 3/10	It: 3001/4459	batch_loss: 3.5417	batch_accuracy: 36.52%	lr:0.000539
Ep: 3/10	It: 3051/4459	batch_loss: 3.5313	batch_accuracy: 36.03%	lr:0.000541
Ep: 3/10	It: 3101/4459	batch_loss: 3.4864	batch_accuracy: 37.04%	lr:0.000544
Ep: 3/10	It: 3151/4459	batch_loss: 3.5305	batch_accuracy: 36.38%	lr:0.000546
Ep: 3/10	It: 3201/4459	batch_loss: 3.5126	batch_accuracy: 36.78%	lr:0.000548
Ep: 3/10	It: 3251/4459	batch_loss: 3.4294	batch_accuracy: 37.84%	lr:0.000550
Ep: 3/10	It: 3301/4459	batch_loss: 3.4862	batch_accuracy: 36.85%	lr:0.000553
Ep: 3/10	It: 3351/4459	batch_loss: 3.5789	batch_accuracy: 36.41%	lr:0.000555
Ep: 3/10	It: 3401/4459	batch_loss: 3.5428	batch_accuracy: 36.16%	lr:0.000557
Ep: 3/10	It: 3451/4459	batch_loss: 3.5592	batch_accuracy: 35.75%	lr:0.000559
Ep: 3/10	It: 3501/4459	batch_loss: 3.5668	batch_accuracy: 35.98%	lr:0.000561
Ep: 3/10	It: 3551/4459	batch_loss: 3.4378	batch_accuracy: 37.07%	lr:0.000564
Ep: 3/10	It: 3601/4459	batch_loss: 3.5160	batch_accuracy: 36.40%	lr:0.000566
Ep: 3/10	It: 3651/4459	batch_loss: 3.3765	batch_accuracy: 38.03%	lr:0.000568
Ep: 3/10	It: 3701/4459	batch_loss: 3.5988	batch_accuracy: 36.29%	lr:0.000570
Ep: 3/10	It: 3751/4459	batch_loss: 3.5993	batch_accuracy: 35.91%	lr:0.000573
Ep: 3/10	It: 3801/4459	batch_loss: 3.4543	batch_accuracy: 37.76%	lr:0.000575
Ep: 3/10	It: 3851/4459	batch_loss: 3.4939	batch_accuracy: 37.21%	lr:0.000577
Ep: 3/10	It: 3901/4459	batch_loss: 3.4200	batch_accuracy: 37.92%	lr:0.000579
Ep: 3/10	It: 3951/4459	batch_loss: 3.5485	batch_accuracy: 36.51%	lr:0.000581
Ep: 3/10	It: 4001/4459	batch_loss: 3.5297	batch_accuracy: 36.68%	lr:0.000584
Ep: 3/10	It: 4051/4459	batch_loss: 3.5628	batch_accuracy: 36.10%	lr:0.000586
Ep: 3/10	It: 4101/4459	batch_loss: 3.4828	batch_accuracy: 37.13%	lr:0.000588
Ep: 3/10	It: 4151/4459	batch_loss: 3.6137	batch_accuracy: 35.42%	lr:0.000590
Ep: 3/10	It: 4201/4459	batch_loss: 3.5303	batch_accuracy: 36.38%	lr:0.000593
Ep: 3/10	It: 4251/4459	batch_loss: 3.4742	batch_accuracy: 37.03%	lr:0.000595
Ep: 3/10	It: 4301/4459	batch_loss: 3.5760	batch_accuracy: 35.66%	lr:0.000597
Ep: 3/10	It: 4351/4459	batch_loss: 3.6168	batch_accuracy: 35.17%	lr:0.000599
Ep: 3/10	It: 4401/4459	batch_loss: 3.5191	batch_accuracy: 36.97%	lr:0.000601
Ep: 3/10	It: 4451/4459	batch_loss: 3.5516	batch_accuracy: 36.71%	lr:0.000604
Ep: 3/10	It: 4459/4459	batch_loss: 3.5146	batch_accuracy: 37.48%	lr:0.000604


Generated text for input text "You" is:
You know, we are going to talk about that time, but there's actually a question that can be helpful. The concept of "finding" is to be simple and useful to describe the relationships between the variables and the data.

In the case of "the two-dimensional space" and "no-dimensional space", we will first need to understand how to do this using the example provided in the extract.

The idea of "functions" is used to describe how it is used in different situations where people are placed in a space. In our case, we want to show that "functions are the basis" of all things that can be used in a given way.

Let's say you have a bunch of blocks of blocks. One of them is "functions." When you multiply numbers by two points, you get a "group." A "functions" can be represented by a sequence of numbers that are similar to a group of different numbers. In other words, the number of blocks is simply the same size as the number of blocks.

In fact, if you have a bunch of blocks, you will follow the rules of each block and you want to make sure that you can use a magical place to make a


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 4/10	It: 1/4459	batch_loss: 3.4647	batch_accuracy: 37.17%	lr:0.000604
Ep: 4/10	It: 51/4459	batch_loss: 3.4561	batch_accuracy: 37.20%	lr:0.000606
Ep: 4/10	It: 101/4459	batch_loss: 3.5040	batch_accuracy: 36.52%	lr:0.000608
Ep: 4/10	It: 151/4459	batch_loss: 3.4487	batch_accuracy: 37.71%	lr:0.000611
Ep: 4/10	It: 201/4459	batch_loss: 3.5599	batch_accuracy: 36.02%	lr:0.000613
Ep: 4/10	It: 251/4459	batch_loss: 3.5826	batch_accuracy: 35.73%	lr:0.000615
Ep: 4/10	It: 301/4459	batch_loss: 3.5154	batch_accuracy: 37.23%	lr:0.000617
Ep: 4/10	It: 351/4459	batch_loss: 3.5321	batch_accuracy: 36.50%	lr:0.000620
Ep: 4/10	It: 401/4459	batch_loss: 3.4365	batch_accuracy: 37.76%	lr:0.000622
Ep: 4/10	It: 451/4459	batch_loss: 3.5533	batch_accuracy: 36.51%	lr:0.000624
Ep: 4/10	It: 501/4459	batch_loss: 3.5253	batch_accuracy: 36.06%	lr:0.000626
Ep: 4/10	It: 551/4459	batch_loss: 3.4398	batch_accuracy: 37.42%	lr:0.000628
Ep: 4/10	It: 601/4459	batch_loss: 3.6289	batch_accuracy: 35.25%	lr:0.000631
Ep: 4/10	It: 651/4459	batch_loss: 3.4618	batch_accuracy: 37.59%	lr:0.000633
Ep: 4/10	It: 701/4459	batch_loss: 3.4880	batch_accuracy: 36.81%	lr:0.000635
Ep: 4/10	It: 751/4459	batch_loss: 3.5249	batch_accuracy: 36.02%	lr:0.000637
Ep: 4/10	It: 801/4459	batch_loss: 3.5974	batch_accuracy: 35.82%	lr:0.000640
Ep: 4/10	It: 851/4459	batch_loss: 3.4443	batch_accuracy: 38.01%	lr:0.000642
Ep: 4/10	It: 901/4459	batch_loss: 3.5381	batch_accuracy: 36.16%	lr:0.000644
Ep: 4/10	It: 951/4459	batch_loss: 3.4593	batch_accuracy: 37.66%	lr:0.000646
Ep: 4/10	It: 1001/4459	batch_loss: 3.5030	batch_accuracy: 36.82%	lr:0.000648
Ep: 4/10	It: 1051/4459	batch_loss: 3.3797	batch_accuracy: 37.90%	lr:0.000651
Ep: 4/10	It: 1101/4459	batch_loss: 3.4341	batch_accuracy: 37.18%	lr:0.000653
Ep: 4/10	It: 1151/4459	batch_loss: 3.4068	batch_accuracy: 37.75%	lr:0.000655
Ep: 4/10	It: 1201/4459	batch_loss: 3.3564	batch_accuracy: 38.14%	lr:0.000657
Ep: 4/10	It: 1251/4459	batch_loss: 3.5052	batch_accuracy: 36.46%	lr:0.000660
Ep: 4/10	It: 1301/4459	batch_loss: 3.4025	batch_accuracy: 37.63%	lr:0.000662
Ep: 4/10	It: 1351/4459	batch_loss: 3.4897	batch_accuracy: 36.96%	lr:0.000664
Ep: 4/10	It: 1401/4459	batch_loss: 3.4810	batch_accuracy: 36.93%	lr:0.000666
Ep: 4/10	It: 1451/4459	batch_loss: 3.4793	batch_accuracy: 36.64%	lr:0.000668
Ep: 4/10	It: 1501/4459	batch_loss: 3.4840	batch_accuracy: 37.66%	lr:0.000671
Ep: 4/10	It: 1551/4459	batch_loss: 3.5165	batch_accuracy: 36.51%	lr:0.000673
Ep: 4/10	It: 1601/4459	batch_loss: 3.3568	batch_accuracy: 38.33%	lr:0.000675
Ep: 4/10	It: 1651/4459	batch_loss: 3.3665	batch_accuracy: 38.28%	lr:0.000677
Ep: 4/10	It: 1701/4459	batch_loss: 3.4120	batch_accuracy: 37.35%	lr:0.000680
Ep: 4/10	It: 1751/4459	batch_loss: 3.5692	batch_accuracy: 36.08%	lr:0.000682
Ep: 4/10	It: 1801/4459	batch_loss: 3.4286	batch_accuracy: 36.69%	lr:0.000684
Ep: 4/10	It: 1851/4459	batch_loss: 3.3297	batch_accuracy: 38.84%	lr:0.000686
Ep: 4/10	It: 1901/4459	batch_loss: 3.4162	batch_accuracy: 37.35%	lr:0.000688
Ep: 4/10	It: 1951/4459	batch_loss: 3.5441	batch_accuracy: 36.27%	lr:0.000691
Ep: 4/10	It: 2001/4459	batch_loss: 3.4580	batch_accuracy: 36.94%	lr:0.000693
Ep: 4/10	It: 2051/4459	batch_loss: 3.3628	batch_accuracy: 38.51%	lr:0.000695
Ep: 4/10	It: 2101/4459	batch_loss: 3.4681	batch_accuracy: 37.23%	lr:0.000697
Ep: 4/10	It: 2151/4459	batch_loss: 3.4296	batch_accuracy: 37.18%	lr:0.000700
Ep: 4/10	It: 2201/4459	batch_loss: 3.4499	batch_accuracy: 37.08%	lr:0.000702
Ep: 4/10	It: 2251/4459	batch_loss: 3.4792	batch_accuracy: 37.04%	lr:0.000704
Ep: 4/10	It: 2301/4459	batch_loss: 3.3545	batch_accuracy: 38.95%	lr:0.000706
Ep: 4/10	It: 2351/4459	batch_loss: 3.4647	batch_accuracy: 37.44%	lr:0.000708
Ep: 4/10	It: 2401/4459	batch_loss: 3.3174	batch_accuracy: 39.18%	lr:0.000711
Ep: 4/10	It: 2451/4459	batch_loss: 3.3142	batch_accuracy: 39.34%	lr:0.000713
Ep: 4/10	It: 2501/4459	batch_loss: 3.4008	batch_accuracy: 37.56%	lr:0.000715
Ep: 4/10	It: 2551/4459	batch_loss: 3.3189	batch_accuracy: 39.34%	lr:0.000717
Ep: 4/10	It: 2601/4459	batch_loss: 3.3562	batch_accuracy: 37.94%	lr:0.000719
Ep: 4/10	It: 2651/4459	batch_loss: 3.4921	batch_accuracy: 36.82%	lr:0.000722
Ep: 4/10	It: 2701/4459	batch_loss: 3.3689	batch_accuracy: 37.87%	lr:0.000724
Ep: 4/10	It: 2751/4459	batch_loss: 3.3904	batch_accuracy: 37.34%	lr:0.000726
Ep: 4/10	It: 2801/4459	batch_loss: 3.3821	batch_accuracy: 37.62%	lr:0.000728
Ep: 4/10	It: 2851/4459	batch_loss: 3.5166	batch_accuracy: 36.35%	lr:0.000731
Ep: 4/10	It: 2901/4459	batch_loss: 3.5252	batch_accuracy: 36.48%	lr:0.000733
Ep: 4/10	It: 2951/4459	batch_loss: 3.3158	batch_accuracy: 38.49%	lr:0.000735
Ep: 4/10	It: 3001/4459	batch_loss: 3.3891	batch_accuracy: 37.50%	lr:0.000737
Ep: 4/10	It: 3051/4459	batch_loss: 3.4662	batch_accuracy: 37.30%	lr:0.000739
Ep: 4/10	It: 3101/4459	batch_loss: 3.4993	batch_accuracy: 36.88%	lr:0.000742
Ep: 4/10	It: 3151/4459	batch_loss: 3.4197	batch_accuracy: 37.70%	lr:0.000744
Ep: 4/10	It: 3201/4459	batch_loss: 3.4536	batch_accuracy: 36.78%	lr:0.000746
Ep: 4/10	It: 3251/4459	batch_loss: 3.3433	batch_accuracy: 38.71%	lr:0.000748
Ep: 4/10	It: 3301/4459	batch_loss: 3.4334	batch_accuracy: 36.93%	lr:0.000751
Ep: 4/10	It: 3351/4459	batch_loss: 3.3485	batch_accuracy: 37.94%	lr:0.000753
Ep: 4/10	It: 3401/4459	batch_loss: 3.4419	batch_accuracy: 36.84%	lr:0.000755
Ep: 4/10	It: 3451/4459	batch_loss: 3.4735	batch_accuracy: 37.34%	lr:0.000757
Ep: 4/10	It: 3501/4459	batch_loss: 3.4387	batch_accuracy: 37.11%	lr:0.000759
Ep: 4/10	It: 3551/4459	batch_loss: 3.3058	batch_accuracy: 39.21%	lr:0.000762
Ep: 4/10	It: 3601/4459	batch_loss: 3.3763	batch_accuracy: 37.87%	lr:0.000764
Ep: 4/10	It: 3651/4459	batch_loss: 3.4872	batch_accuracy: 37.12%	lr:0.000766
Ep: 4/10	It: 3701/4459	batch_loss: 3.4732	batch_accuracy: 37.69%	lr:0.000768
Ep: 4/10	It: 3751/4459	batch_loss: 3.4632	batch_accuracy: 37.43%	lr:0.000771
Ep: 4/10	It: 3801/4459	batch_loss: 3.4668	batch_accuracy: 37.78%	lr:0.000773
Ep: 4/10	It: 3851/4459	batch_loss: 3.4394	batch_accuracy: 37.02%	lr:0.000775
Ep: 4/10	It: 3901/4459	batch_loss: 3.3899	batch_accuracy: 37.74%	lr:0.000777
Ep: 4/10	It: 3951/4459	batch_loss: 3.3147	batch_accuracy: 38.59%	lr:0.000779
Ep: 4/10	It: 4001/4459	batch_loss: 3.3621	batch_accuracy: 38.49%	lr:0.000782
Ep: 4/10	It: 4051/4459	batch_loss: 3.4201	batch_accuracy: 37.59%	lr:0.000784
Ep: 4/10	It: 4101/4459	batch_loss: 3.3658	batch_accuracy: 38.70%	lr:0.000786
Ep: 4/10	It: 4151/4459	batch_loss: 3.3980	batch_accuracy: 38.35%	lr:0.000788
Ep: 4/10	It: 4201/4459	batch_loss: 3.3314	batch_accuracy: 38.50%	lr:0.000791
Ep: 4/10	It: 4251/4459	batch_loss: 3.3918	batch_accuracy: 37.46%	lr:0.000793
Ep: 4/10	It: 4301/4459	batch_loss: 3.3993	batch_accuracy: 38.13%	lr:0.000795
Ep: 4/10	It: 4351/4459	batch_loss: 3.3694	batch_accuracy: 38.14%	lr:0.000797
Ep: 4/10	It: 4401/4459	batch_loss: 3.3740	batch_accuracy: 38.11%	lr:0.000799
Ep: 4/10	It: 4451/4459	batch_loss: 3.4935	batch_accuracy: 36.69%	lr:0.000802
Ep: 4/10	It: 4459/4459	batch_loss: 3.4345	batch_accuracy: 37.01%	lr:0.000802


Generated text for input text "You" is:
You might find yourself feeling overwhelmed or frustrated, I might have been stuck with my new routine.

As I sat down, scrolling through Reddit, I stumbled upon a thread discussing "The Art of Music: From Pure: A New Era of Musical Creativity. My excitement grew as I read further, and I decided to share my own experience with others.

As I sat down to see how much I had come from my favorite songs, I noticed something peculiar - an unusual scene from a band that caught my eye. They were all about the band, looking at the sounds of waves, and feeling a sense of camaraderie over the same passion.

You see, my first stop was the "Cotton" sound that I had ever heard. You see, my friends and I was part of a local musician, who was going to be a guitarist and then took in a musical career to a large-format music studio.

As I walked around the entrance, I noticed something unusual. There was a small indie band called "Save: The Tall of Pure," which was a hit single 'Cotton' band with a small group of musicians who loved creating something unique and engaging. They were excited to learn how to create music, learn


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 5/10	It: 1/4459	batch_loss: 3.3883	batch_accuracy: 38.33%	lr:0.000802
Ep: 5/10	It: 51/4459	batch_loss: 3.3637	batch_accuracy: 38.40%	lr:0.000804
Ep: 5/10	It: 101/4459	batch_loss: 3.3370	batch_accuracy: 38.95%	lr:0.000806
Ep: 5/10	It: 151/4459	batch_loss: 3.4288	batch_accuracy: 37.98%	lr:0.000809
Ep: 5/10	It: 201/4459	batch_loss: 3.3648	batch_accuracy: 38.27%	lr:0.000811
Ep: 5/10	It: 251/4459	batch_loss: 3.4309	batch_accuracy: 37.76%	lr:0.000813
Ep: 5/10	It: 301/4459	batch_loss: 3.4200	batch_accuracy: 37.77%	lr:0.000815
Ep: 5/10	It: 351/4459	batch_loss: 3.4649	batch_accuracy: 37.28%	lr:0.000818
Ep: 5/10	It: 401/4459	batch_loss: 3.4006	batch_accuracy: 37.33%	lr:0.000820
Ep: 5/10	It: 451/4459	batch_loss: 3.3077	batch_accuracy: 38.80%	lr:0.000822
Ep: 5/10	It: 501/4459	batch_loss: 3.3509	batch_accuracy: 38.11%	lr:0.000824
Ep: 5/10	It: 551/4459	batch_loss: 3.2207	batch_accuracy: 39.77%	lr:0.000826
Ep: 5/10	It: 601/4459	batch_loss: 3.3011	batch_accuracy: 38.76%	lr:0.000829
Ep: 5/10	It: 651/4459	batch_loss: 3.3329	batch_accuracy: 38.37%	lr:0.000831
Ep: 5/10	It: 701/4459	batch_loss: 3.2844	batch_accuracy: 39.29%	lr:0.000833
Ep: 5/10	It: 751/4459	batch_loss: 3.3474	batch_accuracy: 38.52%	lr:0.000835
Ep: 5/10	It: 801/4459	batch_loss: 3.3630	batch_accuracy: 38.55%	lr:0.000838
Ep: 5/10	It: 851/4459	batch_loss: 3.4601	batch_accuracy: 37.50%	lr:0.000840
Ep: 5/10	It: 901/4459	batch_loss: 3.4431	batch_accuracy: 37.42%	lr:0.000842
Ep: 5/10	It: 951/4459	batch_loss: 3.3583	batch_accuracy: 38.17%	lr:0.000844
Ep: 5/10	It: 1001/4459	batch_loss: 3.3196	batch_accuracy: 38.66%	lr:0.000846
Ep: 5/10	It: 1051/4459	batch_loss: 3.4908	batch_accuracy: 37.77%	lr:0.000849
Ep: 5/10	It: 1101/4459	batch_loss: 3.2172	batch_accuracy: 39.29%	lr:0.000851
Ep: 5/10	It: 1151/4459	batch_loss: 3.3905	batch_accuracy: 37.82%	lr:0.000853
Ep: 5/10	It: 1201/4459	batch_loss: 3.3979	batch_accuracy: 37.56%	lr:0.000855
Ep: 5/10	It: 1251/4459	batch_loss: 3.3559	batch_accuracy: 37.90%	lr:0.000858
Ep: 5/10	It: 1301/4459	batch_loss: 3.3021	batch_accuracy: 39.70%	lr:0.000860
Ep: 5/10	It: 1351/4459	batch_loss: 3.3076	batch_accuracy: 39.12%	lr:0.000862
Ep: 5/10	It: 1401/4459	batch_loss: 3.3380	batch_accuracy: 38.40%	lr:0.000864
Ep: 5/10	It: 1451/4459	batch_loss: 3.3893	batch_accuracy: 37.94%	lr:0.000866
Ep: 5/10	It: 1501/4459	batch_loss: 3.3627	batch_accuracy: 38.43%	lr:0.000869
Ep: 5/10	It: 1551/4459	batch_loss: 3.3764	batch_accuracy: 39.01%	lr:0.000871
Ep: 5/10	It: 1601/4459	batch_loss: 3.3631	batch_accuracy: 38.01%	lr:0.000873
Ep: 5/10	It: 1651/4459	batch_loss: 3.2660	batch_accuracy: 38.57%	lr:0.000875
Ep: 5/10	It: 1701/4459	batch_loss: 3.3379	batch_accuracy: 38.69%	lr:0.000878
Ep: 5/10	It: 1751/4459	batch_loss: 3.4196	batch_accuracy: 37.29%	lr:0.000880
Ep: 5/10	It: 1801/4459	batch_loss: 3.3560	batch_accuracy: 38.50%	lr:0.000882
Ep: 5/10	It: 1851/4459	batch_loss: 3.3712	batch_accuracy: 38.50%	lr:0.000884
Ep: 5/10	It: 1901/4459	batch_loss: 3.3206	batch_accuracy: 38.51%	lr:0.000886
Ep: 5/10	It: 1951/4459	batch_loss: 3.2803	batch_accuracy: 39.38%	lr:0.000889
Ep: 5/10	It: 2001/4459	batch_loss: 3.3566	batch_accuracy: 38.13%	lr:0.000891
Ep: 5/10	It: 2051/4459	batch_loss: 3.3739	batch_accuracy: 38.59%	lr:0.000893
Ep: 5/10	It: 2101/4459	batch_loss: 3.3726	batch_accuracy: 37.95%	lr:0.000895
Ep: 5/10	It: 2151/4459	batch_loss: 3.3777	batch_accuracy: 38.26%	lr:0.000898
Ep: 5/10	It: 2201/4459	batch_loss: 3.1989	batch_accuracy: 40.41%	lr:0.000900
Ep: 5/10	It: 2251/4459	batch_loss: 3.4132	batch_accuracy: 37.60%	lr:0.000902
Ep: 5/10	It: 2301/4459	batch_loss: 3.3244	batch_accuracy: 39.07%	lr:0.000904
Ep: 5/10	It: 2351/4459	batch_loss: 3.3270	batch_accuracy: 38.52%	lr:0.000906
Ep: 5/10	It: 2401/4459	batch_loss: 3.3515	batch_accuracy: 37.89%	lr:0.000909
Ep: 5/10	It: 2451/4459	batch_loss: 3.3324	batch_accuracy: 38.54%	lr:0.000911
Ep: 5/10	It: 2501/4459	batch_loss: 3.3680	batch_accuracy: 38.68%	lr:0.000913
Ep: 5/10	It: 2551/4459	batch_loss: 3.2605	batch_accuracy: 38.88%	lr:0.000915
Ep: 5/10	It: 2601/4459	batch_loss: 3.3772	batch_accuracy: 38.17%	lr:0.000917
Ep: 5/10	It: 2651/4459	batch_loss: 3.3769	batch_accuracy: 37.84%	lr:0.000920
Ep: 5/10	It: 2701/4459	batch_loss: 3.3659	batch_accuracy: 38.78%	lr:0.000922
Ep: 5/10	It: 2751/4459	batch_loss: 3.3585	batch_accuracy: 38.37%	lr:0.000924
Ep: 5/10	It: 2801/4459	batch_loss: 3.3398	batch_accuracy: 38.09%	lr:0.000926
Ep: 5/10	It: 2851/4459	batch_loss: 3.4327	batch_accuracy: 37.52%	lr:0.000929
Ep: 5/10	It: 2901/4459	batch_loss: 3.3504	batch_accuracy: 38.02%	lr:0.000931
Ep: 5/10	It: 2951/4459	batch_loss: 3.3239	batch_accuracy: 38.59%	lr:0.000933
Ep: 5/10	It: 3001/4459	batch_loss: 3.2825	batch_accuracy: 39.41%	lr:0.000935
Ep: 5/10	It: 3051/4459	batch_loss: 3.3076	batch_accuracy: 38.53%	lr:0.000937
Ep: 5/10	It: 3101/4459	batch_loss: 3.3231	batch_accuracy: 38.23%	lr:0.000940
Ep: 5/10	It: 3151/4459	batch_loss: 3.3312	batch_accuracy: 38.24%	lr:0.000942
Ep: 5/10	It: 3201/4459	batch_loss: 3.3897	batch_accuracy: 37.79%	lr:0.000944
Ep: 5/10	It: 3251/4459	batch_loss: 3.2040	batch_accuracy: 40.01%	lr:0.000946
Ep: 5/10	It: 3301/4459	batch_loss: 3.2244	batch_accuracy: 39.20%	lr:0.000949
Ep: 5/10	It: 3351/4459	batch_loss: 3.2352	batch_accuracy: 39.51%	lr:0.000951
Ep: 5/10	It: 3401/4459	batch_loss: 3.3771	batch_accuracy: 37.84%	lr:0.000953
Ep: 5/10	It: 3451/4459	batch_loss: 3.2804	batch_accuracy: 38.95%	lr:0.000955
Ep: 5/10	It: 3501/4459	batch_loss: 3.1819	batch_accuracy: 39.78%	lr:0.000957
Ep: 5/10	It: 3551/4459	batch_loss: 3.2886	batch_accuracy: 39.17%	lr:0.000960
Ep: 5/10	It: 3601/4459	batch_loss: 3.3801	batch_accuracy: 37.79%	lr:0.000962
Ep: 5/10	It: 3651/4459	batch_loss: 3.2872	batch_accuracy: 38.84%	lr:0.000964
Ep: 5/10	It: 3701/4459	batch_loss: 3.2810	batch_accuracy: 39.17%	lr:0.000966
Ep: 5/10	It: 3751/4459	batch_loss: 3.3406	batch_accuracy: 38.46%	lr:0.000969
Ep: 5/10	It: 3801/4459	batch_loss: 3.3322	batch_accuracy: 38.29%	lr:0.000971
Ep: 5/10	It: 3851/4459	batch_loss: 3.4275	batch_accuracy: 37.63%	lr:0.000973
Ep: 5/10	It: 3901/4459	batch_loss: 3.2794	batch_accuracy: 39.51%	lr:0.000975
Ep: 5/10	It: 3951/4459	batch_loss: 3.3853	batch_accuracy: 38.57%	lr:0.000977
Ep: 5/10	It: 4001/4459	batch_loss: 3.3226	batch_accuracy: 38.27%	lr:0.000980
Ep: 5/10	It: 4051/4459	batch_loss: 3.2724	batch_accuracy: 39.24%	lr:0.000982
Ep: 5/10	It: 4101/4459	batch_loss: 3.2627	batch_accuracy: 39.65%	lr:0.000984
Ep: 5/10	It: 4151/4459	batch_loss: 3.3241	batch_accuracy: 38.32%	lr:0.000986
Ep: 5/10	It: 4201/4459	batch_loss: 3.3014	batch_accuracy: 39.06%	lr:0.000989
Ep: 5/10	It: 4251/4459	batch_loss: 3.2805	batch_accuracy: 39.28%	lr:0.000991
Ep: 5/10	It: 4301/4459	batch_loss: 3.3241	batch_accuracy: 38.48%	lr:0.000993
Ep: 5/10	It: 4351/4459	batch_loss: 3.2240	batch_accuracy: 39.95%	lr:0.000995
Ep: 5/10	It: 4401/4459	batch_loss: 3.2860	batch_accuracy: 38.60%	lr:0.000997
Ep: 5/10	It: 4451/4459	batch_loss: 3.3258	batch_accuracy: 38.60%	lr:0.001000
Ep: 5/10	It: 4459/4459	batch_loss: 3.1605	batch_accuracy: 40.41%	lr:0.001000


Generated text for input text "You" is:
You may be surprised at the story of a woman, who happened to be born in the United States.

This is precisely what happened to the story of a woman who had been diagnosed with a severe prostate cancer. While her husband's condition might seem alarming, it highlights the importance of addressing the root causes of this condition. By taking proactive steps to protect her, she helps her cope with both her condition and the devastating consequences of her addiction.

The extract also highlights the importance of taking a proactive approach to managing her chronic condition. As mentioned in the extract, her condition is caused by the presence of a new one-year-old daughter. By adopting a healthy lifestyle, the victim can take care of her own health and make her more vulnerable. This approach offers several benefits, including improved communication, reduced stress, and greater awareness of the root causes of her condition.

One key factor contributing to her addiction is her ability to manage chronic conditions. Chronic stress is a chronic condition that affects millions of people worldwide. It's common to find relief in low-income households with disabilities, and it's important to take action to address these triggers. Additionally, chronic stress can contribute to chronic conditions such as diabetes, diabetes, and certain forms of cancer.

In


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 6/10	It: 1/4459	batch_loss: 3.2666	batch_accuracy: 39.34%	lr:0.001000
Ep: 6/10	It: 51/4459	batch_loss: 3.2503	batch_accuracy: 39.11%	lr:0.001000
Ep: 6/10	It: 101/4459	batch_loss: 3.3092	batch_accuracy: 39.36%	lr:0.001000
Ep: 6/10	It: 151/4459	batch_loss: 3.3467	batch_accuracy: 38.23%	lr:0.001000
Ep: 6/10	It: 201/4459	batch_loss: 3.2413	batch_accuracy: 39.43%	lr:0.001000
Ep: 6/10	It: 251/4459	batch_loss: 3.2960	batch_accuracy: 39.20%	lr:0.001000
Ep: 6/10	It: 301/4459	batch_loss: 3.2497	batch_accuracy: 39.65%	lr:0.001000
Ep: 6/10	It: 351/4459	batch_loss: 3.2717	batch_accuracy: 38.84%	lr:0.000999
Ep: 6/10	It: 401/4459	batch_loss: 3.3581	batch_accuracy: 37.87%	lr:0.000999
Ep: 6/10	It: 451/4459	batch_loss: 3.2212	batch_accuracy: 39.16%	lr:0.000999
Ep: 6/10	It: 501/4459	batch_loss: 3.3629	batch_accuracy: 38.27%	lr:0.000999
Ep: 6/10	It: 551/4459	batch_loss: 3.2627	batch_accuracy: 39.54%	lr:0.000999
Ep: 6/10	It: 601/4459	batch_loss: 3.2772	batch_accuracy: 39.11%	lr:0.000998
Ep: 6/10	It: 651/4459	batch_loss: 3.3250	batch_accuracy: 39.12%	lr:0.000998
Ep: 6/10	It: 701/4459	batch_loss: 3.2751	batch_accuracy: 39.28%	lr:0.000998
Ep: 6/10	It: 751/4459	batch_loss: 3.2249	batch_accuracy: 39.75%	lr:0.000997
Ep: 6/10	It: 801/4459	batch_loss: 3.3217	batch_accuracy: 38.60%	lr:0.000997
Ep: 6/10	It: 851/4459	batch_loss: 3.3976	batch_accuracy: 38.04%	lr:0.000996
Ep: 6/10	It: 901/4459	batch_loss: 3.3304	batch_accuracy: 38.92%	lr:0.000996
Ep: 6/10	It: 951/4459	batch_loss: 3.1994	batch_accuracy: 39.78%	lr:0.000996
Ep: 6/10	It: 1001/4459	batch_loss: 3.3591	batch_accuracy: 37.80%	lr:0.000995
Ep: 6/10	It: 1051/4459	batch_loss: 3.4413	batch_accuracy: 37.23%	lr:0.000995
Ep: 6/10	It: 1101/4459	batch_loss: 3.2154	batch_accuracy: 39.43%	lr:0.000994
Ep: 6/10	It: 1151/4459	batch_loss: 3.3020	batch_accuracy: 38.66%	lr:0.000994
Ep: 6/10	It: 1201/4459	batch_loss: 3.2299	batch_accuracy: 39.31%	lr:0.000993
Ep: 6/10	It: 1251/4459	batch_loss: 3.1807	batch_accuracy: 39.78%	lr:0.000992
Ep: 6/10	It: 1301/4459	batch_loss: 3.3311	batch_accuracy: 38.87%	lr:0.000992
Ep: 6/10	It: 1351/4459	batch_loss: 3.3289	batch_accuracy: 38.88%	lr:0.000991
Ep: 6/10	It: 1401/4459	batch_loss: 3.2145	batch_accuracy: 39.72%	lr:0.000990
Ep: 6/10	It: 1451/4459	batch_loss: 3.2766	batch_accuracy: 39.00%	lr:0.000990
Ep: 6/10	It: 1501/4459	batch_loss: 3.2517	batch_accuracy: 39.48%	lr:0.000989
Ep: 6/10	It: 1551/4459	batch_loss: 3.2390	batch_accuracy: 39.75%	lr:0.000988
Ep: 6/10	It: 1601/4459	batch_loss: 3.2737	batch_accuracy: 38.98%	lr:0.000987
Ep: 6/10	It: 1651/4459	batch_loss: 3.1602	batch_accuracy: 41.05%	lr:0.000987
Ep: 6/10	It: 1701/4459	batch_loss: 3.2684	batch_accuracy: 39.57%	lr:0.000986
Ep: 6/10	It: 1751/4459	batch_loss: 3.2941	batch_accuracy: 38.31%	lr:0.000985
Ep: 6/10	It: 1801/4459	batch_loss: 3.3582	batch_accuracy: 38.57%	lr:0.000984
Ep: 6/10	It: 1851/4459	batch_loss: 3.3246	batch_accuracy: 38.85%	lr:0.000983
Ep: 6/10	It: 1901/4459	batch_loss: 3.2335	batch_accuracy: 39.98%	lr:0.000982
Ep: 6/10	It: 1951/4459	batch_loss: 3.2971	batch_accuracy: 38.68%	lr:0.000981
Ep: 6/10	It: 2001/4459	batch_loss: 3.3311	batch_accuracy: 38.36%	lr:0.000980
Ep: 6/10	It: 2051/4459	batch_loss: 3.3660	batch_accuracy: 37.53%	lr:0.000979
Ep: 6/10	It: 2101/4459	batch_loss: 3.2172	batch_accuracy: 39.51%	lr:0.000978
Ep: 6/10	It: 2151/4459	batch_loss: 3.2086	batch_accuracy: 39.31%	lr:0.000977
Ep: 6/10	It: 2201/4459	batch_loss: 3.1694	batch_accuracy: 39.90%	lr:0.000976
Ep: 6/10	It: 2251/4459	batch_loss: 3.1578	batch_accuracy: 40.37%	lr:0.000975
Ep: 6/10	It: 2301/4459	batch_loss: 3.2478	batch_accuracy: 39.51%	lr:0.000974
Ep: 6/10	It: 2351/4459	batch_loss: 3.2098	batch_accuracy: 39.75%	lr:0.000973
Ep: 6/10	It: 2401/4459	batch_loss: 3.1591	batch_accuracy: 40.25%	lr:0.000972
Ep: 6/10	It: 2451/4459	batch_loss: 3.3124	batch_accuracy: 38.67%	lr:0.000971
Ep: 6/10	It: 2501/4459	batch_loss: 3.3029	batch_accuracy: 38.95%	lr:0.000970
Ep: 6/10	It: 2551/4459	batch_loss: 3.2391	batch_accuracy: 39.65%	lr:0.000968
Ep: 6/10	It: 2601/4459	batch_loss: 3.2818	batch_accuracy: 38.31%	lr:0.000967
Ep: 6/10	It: 2651/4459	batch_loss: 3.2303	batch_accuracy: 39.40%	lr:0.000966
Ep: 6/10	It: 2701/4459	batch_loss: 3.1865	batch_accuracy: 40.43%	lr:0.000965
Ep: 6/10	It: 2751/4459	batch_loss: 3.1229	batch_accuracy: 40.66%	lr:0.000963
Ep: 6/10	It: 2801/4459	batch_loss: 3.2649	batch_accuracy: 39.18%	lr:0.000962
Ep: 6/10	It: 2851/4459	batch_loss: 3.1868	batch_accuracy: 40.06%	lr:0.000961
Ep: 6/10	It: 2901/4459	batch_loss: 3.2805	batch_accuracy: 39.29%	lr:0.000959
Ep: 6/10	It: 2951/4459	batch_loss: 3.2107	batch_accuracy: 39.70%	lr:0.000958
Ep: 6/10	It: 3001/4459	batch_loss: 3.1994	batch_accuracy: 40.16%	lr:0.000956
Ep: 6/10	It: 3051/4459	batch_loss: 3.3549	batch_accuracy: 38.28%	lr:0.000955
Ep: 6/10	It: 3101/4459	batch_loss: 3.2646	batch_accuracy: 39.08%	lr:0.000953
Ep: 6/10	It: 3151/4459	batch_loss: 3.1467	batch_accuracy: 40.16%	lr:0.000952
Ep: 6/10	It: 3201/4459	batch_loss: 3.2393	batch_accuracy: 39.29%	lr:0.000950
Ep: 6/10	It: 3251/4459	batch_loss: 3.2520	batch_accuracy: 39.73%	lr:0.000949
Ep: 6/10	It: 3301/4459	batch_loss: 3.1226	batch_accuracy: 40.56%	lr:0.000947
Ep: 6/10	It: 3351/4459	batch_loss: 3.0825	batch_accuracy: 41.11%	lr:0.000946
Ep: 6/10	It: 3401/4459	batch_loss: 3.2241	batch_accuracy: 39.69%	lr:0.000944
Ep: 6/10	It: 3451/4459	batch_loss: 3.1562	batch_accuracy: 40.84%	lr:0.000943
Ep: 6/10	It: 3501/4459	batch_loss: 3.2661	batch_accuracy: 38.72%	lr:0.000941
Ep: 6/10	It: 3551/4459	batch_loss: 3.1956	batch_accuracy: 39.81%	lr:0.000939
Ep: 6/10	It: 3601/4459	batch_loss: 3.2511	batch_accuracy: 39.15%	lr:0.000938
Ep: 6/10	It: 3651/4459	batch_loss: 3.1804	batch_accuracy: 40.46%	lr:0.000936
Ep: 6/10	It: 3701/4459	batch_loss: 3.2263	batch_accuracy: 39.70%	lr:0.000934
Ep: 6/10	It: 3751/4459	batch_loss: 3.2265	batch_accuracy: 39.50%	lr:0.000932
Ep: 6/10	It: 3801/4459	batch_loss: 3.2562	batch_accuracy: 39.82%	lr:0.000931
Ep: 6/10	It: 3851/4459	batch_loss: 3.0813	batch_accuracy: 41.34%	lr:0.000929
Ep: 6/10	It: 3901/4459	batch_loss: 3.1355	batch_accuracy: 40.61%	lr:0.000927
Ep: 6/10	It: 3951/4459	batch_loss: 3.3214	batch_accuracy: 38.56%	lr:0.000925
Ep: 6/10	It: 4001/4459	batch_loss: 3.1509	batch_accuracy: 40.33%	lr:0.000923
Ep: 6/10	It: 4051/4459	batch_loss: 3.2689	batch_accuracy: 38.63%	lr:0.000922
Ep: 6/10	It: 4101/4459	batch_loss: 3.1667	batch_accuracy: 40.06%	lr:0.000920
Ep: 6/10	It: 4151/4459	batch_loss: 3.1802	batch_accuracy: 40.31%	lr:0.000918
Ep: 6/10	It: 4201/4459	batch_loss: 3.2494	batch_accuracy: 39.54%	lr:0.000916
Ep: 6/10	It: 4251/4459	batch_loss: 3.1760	batch_accuracy: 40.41%	lr:0.000914
Ep: 6/10	It: 4301/4459	batch_loss: 3.0983	batch_accuracy: 40.89%	lr:0.000912
Ep: 6/10	It: 4351/4459	batch_loss: 3.2139	batch_accuracy: 40.23%	lr:0.000910
Ep: 6/10	It: 4401/4459	batch_loss: 3.2925	batch_accuracy: 39.19%	lr:0.000908
Ep: 6/10	It: 4451/4459	batch_loss: 3.1362	batch_accuracy: 40.78%	lr:0.000906
Ep: 6/10	It: 4459/4459	batch_loss: 3.0966	batch_accuracy: 41.14%	lr:0.000905


Generated text for input text "You" is:
You see, the I-3578. This year, I had recently been working on my own business - a new way to expand my online presence. But what does it really mean for I-358? And how can I ensure that my online presence is up to a positive and engaging environment? Let's dive in and explore some ways I can help you build a successful business and achieve your goals.

First, let's talk about the importance of social media. As the name suggests, Twitter has taken the world by offering an email list to various people who are interested in your area. With Twitter's official website, Twitter has become an essential tool for businesses to reach out to potential customers. By doing so, you are able to tap into a diverse pool of potential customers, increase their chances of discovering your products, and increase your chances of being found.

But how does this relate to your marketing and business strategies? One key aspect of effective social media marketing is its focus on creating value for your audience. As stated in the extract, Instagram's "move your followers to your followers" makes it more likely to engage with your followers and drive sales. By focusing on what truly matters, you'll not only convert prospects into loyal customers but also foster


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 7/10	It: 1/4459	batch_loss: 3.1230	batch_accuracy: 40.42%	lr:0.000905
Ep: 7/10	It: 51/4459	batch_loss: 3.1747	batch_accuracy: 40.70%	lr:0.000903
Ep: 7/10	It: 101/4459	batch_loss: 3.1711	batch_accuracy: 39.72%	lr:0.000901
Ep: 7/10	It: 151/4459	batch_loss: 3.1150	batch_accuracy: 41.04%	lr:0.000899
Ep: 7/10	It: 201/4459	batch_loss: 3.1378	batch_accuracy: 40.61%	lr:0.000897
Ep: 7/10	It: 251/4459	batch_loss: 3.2093	batch_accuracy: 39.62%	lr:0.000895
Ep: 7/10	It: 301/4459	batch_loss: 3.1924	batch_accuracy: 40.72%	lr:0.000893
Ep: 7/10	It: 351/4459	batch_loss: 3.1877	batch_accuracy: 40.41%	lr:0.000891
Ep: 7/10	It: 401/4459	batch_loss: 3.1925	batch_accuracy: 39.63%	lr:0.000888
Ep: 7/10	It: 451/4459	batch_loss: 3.1577	batch_accuracy: 41.08%	lr:0.000886
Ep: 7/10	It: 501/4459	batch_loss: 3.2159	batch_accuracy: 39.85%	lr:0.000884
Ep: 7/10	It: 551/4459	batch_loss: 3.2595	batch_accuracy: 38.89%	lr:0.000882
Ep: 7/10	It: 601/4459	batch_loss: 3.2083	batch_accuracy: 39.92%	lr:0.000879
Ep: 7/10	It: 651/4459	batch_loss: 3.1630	batch_accuracy: 40.45%	lr:0.000877
Ep: 7/10	It: 701/4459	batch_loss: 3.1846	batch_accuracy: 40.20%	lr:0.000875
Ep: 7/10	It: 751/4459	batch_loss: 3.1548	batch_accuracy: 40.19%	lr:0.000872
Ep: 7/10	It: 801/4459	batch_loss: 3.1701	batch_accuracy: 40.16%	lr:0.000870
Ep: 7/10	It: 851/4459	batch_loss: 3.2721	batch_accuracy: 39.44%	lr:0.000868
Ep: 7/10	It: 901/4459	batch_loss: 3.2051	batch_accuracy: 40.23%	lr:0.000865
Ep: 7/10	It: 951/4459	batch_loss: 3.1516	batch_accuracy: 40.47%	lr:0.000863
Ep: 7/10	It: 1001/4459	batch_loss: 3.1954	batch_accuracy: 40.33%	lr:0.000861
Ep: 7/10	It: 1051/4459	batch_loss: 3.1422	batch_accuracy: 40.34%	lr:0.000858
Ep: 7/10	It: 1101/4459	batch_loss: 3.2199	batch_accuracy: 40.07%	lr:0.000856
Ep: 7/10	It: 1151/4459	batch_loss: 3.1679	batch_accuracy: 40.62%	lr:0.000853
Ep: 7/10	It: 1201/4459	batch_loss: 3.1398	batch_accuracy: 40.69%	lr:0.000851
Ep: 7/10	It: 1251/4459	batch_loss: 3.1252	batch_accuracy: 40.18%	lr:0.000848
Ep: 7/10	It: 1301/4459	batch_loss: 3.0954	batch_accuracy: 41.56%	lr:0.000846
Ep: 7/10	It: 1351/4459	batch_loss: 3.1426	batch_accuracy: 40.88%	lr:0.000843
Ep: 7/10	It: 1401/4459	batch_loss: 3.1171	batch_accuracy: 40.66%	lr:0.000841
Ep: 7/10	It: 1451/4459	batch_loss: 3.1678	batch_accuracy: 40.25%	lr:0.000838
Ep: 7/10	It: 1501/4459	batch_loss: 3.1782	batch_accuracy: 40.01%	lr:0.000835
Ep: 7/10	It: 1551/4459	batch_loss: 3.2126	batch_accuracy: 39.98%	lr:0.000833
Ep: 7/10	It: 1601/4459	batch_loss: 3.0832	batch_accuracy: 41.35%	lr:0.000830
Ep: 7/10	It: 1651/4459	batch_loss: 3.1072	batch_accuracy: 41.20%	lr:0.000828
Ep: 7/10	It: 1701/4459	batch_loss: 3.1651	batch_accuracy: 40.46%	lr:0.000825
Ep: 7/10	It: 1751/4459	batch_loss: 3.2025	batch_accuracy: 39.67%	lr:0.000822
Ep: 7/10	It: 1801/4459	batch_loss: 3.1129	batch_accuracy: 41.12%	lr:0.000820
Ep: 7/10	It: 1851/4459	batch_loss: 3.2111	batch_accuracy: 39.65%	lr:0.000817
Ep: 7/10	It: 1901/4459	batch_loss: 3.1096	batch_accuracy: 40.84%	lr:0.000814
Ep: 7/10	It: 1951/4459	batch_loss: 3.1992	batch_accuracy: 39.88%	lr:0.000811
Ep: 7/10	It: 2001/4459	batch_loss: 3.1614	batch_accuracy: 40.22%	lr:0.000809
Ep: 7/10	It: 2051/4459	batch_loss: 3.1260	batch_accuracy: 40.35%	lr:0.000806
Ep: 7/10	It: 2101/4459	batch_loss: 3.0916	batch_accuracy: 40.93%	lr:0.000803
Ep: 7/10	It: 2151/4459	batch_loss: 3.1264	batch_accuracy: 39.54%	lr:0.000800
Ep: 7/10	It: 2201/4459	batch_loss: 3.1199	batch_accuracy: 40.43%	lr:0.000798
Ep: 7/10	It: 2251/4459	batch_loss: 3.1327	batch_accuracy: 40.96%	lr:0.000795
Ep: 7/10	It: 2301/4459	batch_loss: 3.1564	batch_accuracy: 40.48%	lr:0.000792
Ep: 7/10	It: 2351/4459	batch_loss: 3.0760	batch_accuracy: 41.05%	lr:0.000789
Ep: 7/10	It: 2401/4459	batch_loss: 3.1928	batch_accuracy: 40.25%	lr:0.000786
Ep: 7/10	It: 2451/4459	batch_loss: 3.0294	batch_accuracy: 42.30%	lr:0.000783
Ep: 7/10	It: 2501/4459	batch_loss: 3.0921	batch_accuracy: 41.27%	lr:0.000780
Ep: 7/10	It: 2551/4459	batch_loss: 3.0622	batch_accuracy: 41.36%	lr:0.000778
Ep: 7/10	It: 2601/4459	batch_loss: 3.2231	batch_accuracy: 39.93%	lr:0.000775
Ep: 7/10	It: 2651/4459	batch_loss: 3.1943	batch_accuracy: 39.78%	lr:0.000772
Ep: 7/10	It: 2701/4459	batch_loss: 3.1416	batch_accuracy: 40.17%	lr:0.000769
Ep: 7/10	It: 2751/4459	batch_loss: 3.1085	batch_accuracy: 41.66%	lr:0.000766
Ep: 7/10	It: 2801/4459	batch_loss: 3.0877	batch_accuracy: 41.35%	lr:0.000763
Ep: 7/10	It: 2851/4459	batch_loss: 3.0922	batch_accuracy: 41.00%	lr:0.000760
Ep: 7/10	It: 2901/4459	batch_loss: 3.1502	batch_accuracy: 40.36%	lr:0.000757
Ep: 7/10	It: 2951/4459	batch_loss: 3.1296	batch_accuracy: 40.48%	lr:0.000754
Ep: 7/10	It: 3001/4459	batch_loss: 3.2782	batch_accuracy: 39.25%	lr:0.000751
Ep: 7/10	It: 3051/4459	batch_loss: 3.0393	batch_accuracy: 41.77%	lr:0.000748
Ep: 7/10	It: 3101/4459	batch_loss: 3.0891	batch_accuracy: 40.73%	lr:0.000745
Ep: 7/10	It: 3151/4459	batch_loss: 3.0614	batch_accuracy: 41.08%	lr:0.000742
Ep: 7/10	It: 3201/4459	batch_loss: 3.1144	batch_accuracy: 40.92%	lr:0.000739
Ep: 7/10	It: 3251/4459	batch_loss: 3.1025	batch_accuracy: 40.78%	lr:0.000736
Ep: 7/10	It: 3301/4459	batch_loss: 3.0589	batch_accuracy: 40.73%	lr:0.000732
Ep: 7/10	It: 3351/4459	batch_loss: 3.0574	batch_accuracy: 41.86%	lr:0.000729
Ep: 7/10	It: 3401/4459	batch_loss: 3.1695	batch_accuracy: 40.09%	lr:0.000726
Ep: 7/10	It: 3451/4459	batch_loss: 3.2127	batch_accuracy: 39.48%	lr:0.000723
Ep: 7/10	It: 3501/4459	batch_loss: 3.0683	batch_accuracy: 40.99%	lr:0.000720
Ep: 7/10	It: 3551/4459	batch_loss: 3.0961	batch_accuracy: 41.20%	lr:0.000717
Ep: 7/10	It: 3601/4459	batch_loss: 3.1469	batch_accuracy: 40.81%	lr:0.000714
Ep: 7/10	It: 3651/4459	batch_loss: 3.0114	batch_accuracy: 42.20%	lr:0.000710
Ep: 7/10	It: 3701/4459	batch_loss: 3.0366	batch_accuracy: 41.77%	lr:0.000707
Ep: 7/10	It: 3751/4459	batch_loss: 3.0840	batch_accuracy: 41.14%	lr:0.000704
Ep: 7/10	It: 3801/4459	batch_loss: 3.0928	batch_accuracy: 41.66%	lr:0.000701
Ep: 7/10	It: 3851/4459	batch_loss: 3.1114	batch_accuracy: 41.11%	lr:0.000698
Ep: 7/10	It: 3901/4459	batch_loss: 3.0874	batch_accuracy: 41.01%	lr:0.000694
Ep: 7/10	It: 3951/4459	batch_loss: 3.1410	batch_accuracy: 40.84%	lr:0.000691
Ep: 7/10	It: 4001/4459	batch_loss: 2.9795	batch_accuracy: 42.34%	lr:0.000688
Ep: 7/10	It: 4051/4459	batch_loss: 3.0676	batch_accuracy: 41.74%	lr:0.000685
Ep: 7/10	It: 4101/4459	batch_loss: 3.0407	batch_accuracy: 42.43%	lr:0.000682
Ep: 7/10	It: 4151/4459	batch_loss: 3.0843	batch_accuracy: 40.91%	lr:0.000678
Ep: 7/10	It: 4201/4459	batch_loss: 3.0757	batch_accuracy: 40.73%	lr:0.000675
Ep: 7/10	It: 4251/4459	batch_loss: 3.1489	batch_accuracy: 40.58%	lr:0.000672
Ep: 7/10	It: 4301/4459	batch_loss: 3.1892	batch_accuracy: 40.32%	lr:0.000668
Ep: 7/10	It: 4351/4459	batch_loss: 2.9715	batch_accuracy: 42.57%	lr:0.000665
Ep: 7/10	It: 4401/4459	batch_loss: 2.9873	batch_accuracy: 41.97%	lr:0.000662
Ep: 7/10	It: 4451/4459	batch_loss: 3.1924	batch_accuracy: 40.05%	lr:0.000658
Ep: 7/10	It: 4459/4459	batch_loss: 3.1205	batch_accuracy: 40.70%	lr:0.000658


Generated text for input text "You" is:
You can't get back to school? Instead, let's find out that sometimes, our most critical issues may not yield positive results, but we can also find solutions to problems.

Imagine you have a bag full of different colored marbles - red, green, green, green, green, and so on. The next time you decide to add marbles to your bag, you'll see how many different ways you could make it look different from your friend's bag. This way, you can still choose your bag and pick some marbles from the bag.

Now, imagine having a bag of marbles from all of your bag, and that one bag is picked up in a bag containing exactly one color. To make sure you have enough color, you decide to write a collection of marbles and represent them as "number of colors." This will give you a good idea of what it means for your bag to make sure that you always choose it.

But why is this so important? Well, it helps us figure out what we need to do when we want to organize our bags. Let's say we want to add up all the colors of the bag. We could create a special box called the "Special Category." In our example, we write a special rule


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 8/10	It: 1/4459	batch_loss: 3.0889	batch_accuracy: 41.19%	lr:0.000658
Ep: 8/10	It: 51/4459	batch_loss: 3.0455	batch_accuracy: 41.53%	lr:0.000655
Ep: 8/10	It: 101/4459	batch_loss: 3.0825	batch_accuracy: 41.69%	lr:0.000651
Ep: 8/10	It: 151/4459	batch_loss: 3.0882	batch_accuracy: 40.63%	lr:0.000648
Ep: 8/10	It: 201/4459	batch_loss: 3.1303	batch_accuracy: 40.75%	lr:0.000645
Ep: 8/10	It: 251/4459	batch_loss: 3.0729	batch_accuracy: 41.01%	lr:0.000641
Ep: 8/10	It: 301/4459	batch_loss: 3.0663	batch_accuracy: 41.82%	lr:0.000638
Ep: 8/10	It: 351/4459	batch_loss: 2.9516	batch_accuracy: 42.66%	lr:0.000635
Ep: 8/10	It: 401/4459	batch_loss: 3.2335	batch_accuracy: 38.94%	lr:0.000631
Ep: 8/10	It: 451/4459	batch_loss: 3.1967	batch_accuracy: 39.78%	lr:0.000628
Ep: 8/10	It: 501/4459	batch_loss: 3.0447	batch_accuracy: 41.25%	lr:0.000624
Ep: 8/10	It: 551/4459	batch_loss: 3.1059	batch_accuracy: 41.14%	lr:0.000621
Ep: 8/10	It: 601/4459	batch_loss: 3.1215	batch_accuracy: 41.17%	lr:0.000618
Ep: 8/10	It: 651/4459	batch_loss: 3.1319	batch_accuracy: 40.33%	lr:0.000614
Ep: 8/10	It: 701/4459	batch_loss: 3.0732	batch_accuracy: 41.66%	lr:0.000611
Ep: 8/10	It: 751/4459	batch_loss: 3.1207	batch_accuracy: 40.26%	lr:0.000607
Ep: 8/10	It: 801/4459	batch_loss: 3.0483	batch_accuracy: 41.64%	lr:0.000604
Ep: 8/10	It: 851/4459	batch_loss: 3.1703	batch_accuracy: 39.86%	lr:0.000601
Ep: 8/10	It: 901/4459	batch_loss: 3.1023	batch_accuracy: 41.52%	lr:0.000597
Ep: 8/10	It: 951/4459	batch_loss: 3.1817	batch_accuracy: 40.39%	lr:0.000594
Ep: 8/10	It: 1001/4459	batch_loss: 3.0640	batch_accuracy: 42.00%	lr:0.000590
Ep: 8/10	It: 1051/4459	batch_loss: 3.0648	batch_accuracy: 41.82%	lr:0.000587
Ep: 8/10	It: 1101/4459	batch_loss: 3.1286	batch_accuracy: 40.59%	lr:0.000583
Ep: 8/10	It: 1151/4459	batch_loss: 3.2190	batch_accuracy: 39.47%	lr:0.000580
Ep: 8/10	It: 1201/4459	batch_loss: 3.0337	batch_accuracy: 41.84%	lr:0.000576
Ep: 8/10	It: 1251/4459	batch_loss: 3.0884	batch_accuracy: 40.73%	lr:0.000573
Ep: 8/10	It: 1301/4459	batch_loss: 3.0463	batch_accuracy: 42.12%	lr:0.000570
Ep: 8/10	It: 1351/4459	batch_loss: 3.0046	batch_accuracy: 42.21%	lr:0.000566
Ep: 8/10	It: 1401/4459	batch_loss: 3.1052	batch_accuracy: 40.86%	lr:0.000563
Ep: 8/10	It: 1451/4459	batch_loss: 3.0916	batch_accuracy: 41.63%	lr:0.000559
Ep: 8/10	It: 1501/4459	batch_loss: 3.0565	batch_accuracy: 41.96%	lr:0.000556
Ep: 8/10	It: 1551/4459	batch_loss: 2.9971	batch_accuracy: 42.00%	lr:0.000552
Ep: 8/10	It: 1601/4459	batch_loss: 3.0175	batch_accuracy: 42.01%	lr:0.000549
Ep: 8/10	It: 1651/4459	batch_loss: 3.0610	batch_accuracy: 41.73%	lr:0.000545
Ep: 8/10	It: 1701/4459	batch_loss: 3.1224	batch_accuracy: 40.91%	lr:0.000542
Ep: 8/10	It: 1751/4459	batch_loss: 2.9848	batch_accuracy: 42.50%	lr:0.000538
Ep: 8/10	It: 1801/4459	batch_loss: 3.0088	batch_accuracy: 41.83%	lr:0.000535
Ep: 8/10	It: 1851/4459	batch_loss: 3.1198	batch_accuracy: 40.60%	lr:0.000531
Ep: 8/10	It: 1901/4459	batch_loss: 3.0455	batch_accuracy: 41.46%	lr:0.000528
Ep: 8/10	It: 1951/4459	batch_loss: 2.9574	batch_accuracy: 43.36%	lr:0.000524
Ep: 8/10	It: 2001/4459	batch_loss: 3.0460	batch_accuracy: 41.97%	lr:0.000521
Ep: 8/10	It: 2051/4459	batch_loss: 3.1026	batch_accuracy: 40.98%	lr:0.000517
Ep: 8/10	It: 2101/4459	batch_loss: 2.9875	batch_accuracy: 42.68%	lr:0.000514
Ep: 8/10	It: 2151/4459	batch_loss: 2.9901	batch_accuracy: 43.30%	lr:0.000510
Ep: 8/10	It: 2201/4459	batch_loss: 3.0105	batch_accuracy: 41.67%	lr:0.000507
Ep: 8/10	It: 2251/4459	batch_loss: 3.0008	batch_accuracy: 42.55%	lr:0.000504
Ep: 8/10	It: 2301/4459	batch_loss: 3.0405	batch_accuracy: 41.82%	lr:0.000500
Ep: 8/10	It: 2351/4459	batch_loss: 2.9704	batch_accuracy: 42.96%	lr:0.000497
Ep: 8/10	It: 2401/4459	batch_loss: 3.0554	batch_accuracy: 42.35%	lr:0.000493
Ep: 8/10	It: 2451/4459	batch_loss: 3.0812	batch_accuracy: 40.65%	lr:0.000490
Ep: 8/10	It: 2501/4459	batch_loss: 3.0781	batch_accuracy: 41.78%	lr:0.000486
Ep: 8/10	It: 2551/4459	batch_loss: 2.9774	batch_accuracy: 42.30%	lr:0.000483
Ep: 8/10	It: 2601/4459	batch_loss: 3.0671	batch_accuracy: 41.56%	lr:0.000479
Ep: 8/10	It: 2651/4459	batch_loss: 3.1093	batch_accuracy: 40.80%	lr:0.000476
Ep: 8/10	It: 2701/4459	batch_loss: 3.0462	batch_accuracy: 41.51%	lr:0.000472
Ep: 8/10	It: 2751/4459	batch_loss: 3.0728	batch_accuracy: 41.09%	lr:0.000469
Ep: 8/10	It: 2801/4459	batch_loss: 3.0400	batch_accuracy: 41.95%	lr:0.000465
Ep: 8/10	It: 2851/4459	batch_loss: 3.1741	batch_accuracy: 39.95%	lr:0.000462
Ep: 8/10	It: 2901/4459	batch_loss: 2.9340	batch_accuracy: 42.76%	lr:0.000458
Ep: 8/10	It: 2951/4459	batch_loss: 3.0290	batch_accuracy: 41.96%	lr:0.000455
Ep: 8/10	It: 3001/4459	batch_loss: 3.0553	batch_accuracy: 41.80%	lr:0.000451
Ep: 8/10	It: 3051/4459	batch_loss: 2.9691	batch_accuracy: 42.39%	lr:0.000448
Ep: 8/10	It: 3101/4459	batch_loss: 3.0334	batch_accuracy: 42.10%	lr:0.000444
Ep: 8/10	It: 3151/4459	batch_loss: 2.9679	batch_accuracy: 42.71%	lr:0.000441
Ep: 8/10	It: 3201/4459	batch_loss: 3.0699	batch_accuracy: 41.34%	lr:0.000437
Ep: 8/10	It: 3251/4459	batch_loss: 3.1250	batch_accuracy: 40.95%	lr:0.000434
Ep: 8/10	It: 3301/4459	batch_loss: 2.9482	batch_accuracy: 42.95%	lr:0.000431
Ep: 8/10	It: 3351/4459	batch_loss: 3.0609	batch_accuracy: 41.50%	lr:0.000427
Ep: 8/10	It: 3401/4459	batch_loss: 3.0167	batch_accuracy: 41.75%	lr:0.000424
Ep: 8/10	It: 3451/4459	batch_loss: 3.0144	batch_accuracy: 42.08%	lr:0.000420
Ep: 8/10	It: 3501/4459	batch_loss: 3.1208	batch_accuracy: 40.84%	lr:0.000417
Ep: 8/10	It: 3551/4459	batch_loss: 2.9284	batch_accuracy: 43.04%	lr:0.000413
Ep: 8/10	It: 3601/4459	batch_loss: 3.0145	batch_accuracy: 41.95%	lr:0.000410
Ep: 8/10	It: 3651/4459	batch_loss: 2.9585	batch_accuracy: 42.46%	lr:0.000407
Ep: 8/10	It: 3701/4459	batch_loss: 3.0363	batch_accuracy: 41.57%	lr:0.000403
Ep: 8/10	It: 3751/4459	batch_loss: 3.0137	batch_accuracy: 42.18%	lr:0.000400
Ep: 8/10	It: 3801/4459	batch_loss: 3.1740	batch_accuracy: 39.94%	lr:0.000396
Ep: 8/10	It: 3851/4459	batch_loss: 3.0509	batch_accuracy: 41.28%	lr:0.000393
Ep: 8/10	It: 3901/4459	batch_loss: 3.0365	batch_accuracy: 41.53%	lr:0.000389
Ep: 8/10	It: 3951/4459	batch_loss: 3.0003	batch_accuracy: 42.52%	lr:0.000386
Ep: 8/10	It: 4001/4459	batch_loss: 3.0737	batch_accuracy: 40.78%	lr:0.000383
Ep: 8/10	It: 4051/4459	batch_loss: 3.0734	batch_accuracy: 41.31%	lr:0.000379
Ep: 8/10	It: 4101/4459	batch_loss: 3.0660	batch_accuracy: 41.32%	lr:0.000376
Ep: 8/10	It: 4151/4459	batch_loss: 3.0667	batch_accuracy: 42.14%	lr:0.000373
Ep: 8/10	It: 4201/4459	batch_loss: 2.9915	batch_accuracy: 42.11%	lr:0.000369
Ep: 8/10	It: 4251/4459	batch_loss: 2.9789	batch_accuracy: 42.30%	lr:0.000366
Ep: 8/10	It: 4301/4459	batch_loss: 2.9130	batch_accuracy: 43.19%	lr:0.000363
Ep: 8/10	It: 4351/4459	batch_loss: 3.0078	batch_accuracy: 41.84%	lr:0.000359
Ep: 8/10	It: 4401/4459	batch_loss: 3.0689	batch_accuracy: 41.49%	lr:0.000356
Ep: 8/10	It: 4451/4459	batch_loss: 2.9975	batch_accuracy: 42.71%	lr:0.000353
Ep: 8/10	It: 4459/4459	batch_loss: 3.0127	batch_accuracy: 42.22%	lr:0.000352


Generated text for input text "You" is:
You have to do with you, remember that every day brings you closer to making a difference in your lives. So, if you find yourself stuck in your journey, don't worry; we'll be here for you!
<eot>
<sot>
 Title: The Art of Handcrafted Jewelry: A Deep Dive into Jewelry Design and Manufacturing

Introduction

When it comes to jewelry design and manufacturing, there's no shortage of options. From traditional pieces to modern ones, these creations have evolved beyond traditional necklaces and bracelets. Today, we will take a closer look at one such piece - the silver and its components – and explore how they intersect to create truly exceptional pieces.

The Art of Handmade Jewelry

Handmade jewelry often goes hand-in-hand with cutting and shaping techniques. But did you know that this type of jewelry has a unique shape and pattern? For instance, the silver and gold are created using various materials such as silver, platinum, or gold alloys. These processes can be used to create stunning pieces that reflect the spirit of the piece.

Materials Used in Handmade Jewelry

One of the most significant aspects of handcrafting jewelry is the choice of materials. Gold, gold, and platinum often have unique properties


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 9/10	It: 1/4459	batch_loss: 2.9855	batch_accuracy: 42.56%	lr:0.000352
Ep: 9/10	It: 51/4459	batch_loss: 2.9291	batch_accuracy: 42.76%	lr:0.000349
Ep: 9/10	It: 101/4459	batch_loss: 3.0708	batch_accuracy: 41.49%	lr:0.000345
Ep: 9/10	It: 151/4459	batch_loss: 2.9487	batch_accuracy: 42.97%	lr:0.000342
Ep: 9/10	It: 201/4459	batch_loss: 2.8649	batch_accuracy: 43.65%	lr:0.000339
Ep: 9/10	It: 251/4459	batch_loss: 3.1082	batch_accuracy: 41.39%	lr:0.000335
Ep: 9/10	It: 301/4459	batch_loss: 3.0579	batch_accuracy: 41.75%	lr:0.000332
Ep: 9/10	It: 351/4459	batch_loss: 3.0947	batch_accuracy: 41.23%	lr:0.000329
Ep: 9/10	It: 401/4459	batch_loss: 2.8580	batch_accuracy: 44.27%	lr:0.000326
Ep: 9/10	It: 451/4459	batch_loss: 3.1224	batch_accuracy: 40.78%	lr:0.000322
Ep: 9/10	It: 501/4459	batch_loss: 2.9984	batch_accuracy: 41.90%	lr:0.000319
Ep: 9/10	It: 551/4459	batch_loss: 3.0130	batch_accuracy: 42.34%	lr:0.000316
Ep: 9/10	It: 601/4459	batch_loss: 2.9696	batch_accuracy: 42.74%	lr:0.000313
Ep: 9/10	It: 651/4459	batch_loss: 3.0867	batch_accuracy: 41.33%	lr:0.000310
Ep: 9/10	It: 701/4459	batch_loss: 2.9691	batch_accuracy: 42.72%	lr:0.000306
Ep: 9/10	It: 751/4459	batch_loss: 3.0514	batch_accuracy: 41.28%	lr:0.000303
Ep: 9/10	It: 801/4459	batch_loss: 3.0116	batch_accuracy: 42.24%	lr:0.000300
Ep: 9/10	It: 851/4459	batch_loss: 2.9994	batch_accuracy: 42.27%	lr:0.000297
Ep: 9/10	It: 901/4459	batch_loss: 2.9670	batch_accuracy: 42.72%	lr:0.000294
Ep: 9/10	It: 951/4459	batch_loss: 2.9964	batch_accuracy: 42.27%	lr:0.000291
Ep: 9/10	It: 1001/4459	batch_loss: 3.0124	batch_accuracy: 41.94%	lr:0.000287
Ep: 9/10	It: 1051/4459	batch_loss: 2.9463	batch_accuracy: 42.73%	lr:0.000284
Ep: 9/10	It: 1101/4459	batch_loss: 2.9779	batch_accuracy: 42.75%	lr:0.000281
Ep: 9/10	It: 1151/4459	batch_loss: 2.8343	batch_accuracy: 44.18%	lr:0.000278
Ep: 9/10	It: 1201/4459	batch_loss: 2.9571	batch_accuracy: 42.65%	lr:0.000275
Ep: 9/10	It: 1251/4459	batch_loss: 3.1296	batch_accuracy: 40.91%	lr:0.000272
Ep: 9/10	It: 1301/4459	batch_loss: 3.0299	batch_accuracy: 41.69%	lr:0.000269
Ep: 9/10	It: 1351/4459	batch_loss: 2.9957	batch_accuracy: 41.46%	lr:0.000266
Ep: 9/10	It: 1401/4459	batch_loss: 3.0166	batch_accuracy: 42.36%	lr:0.000263
Ep: 9/10	It: 1451/4459	batch_loss: 2.9595	batch_accuracy: 42.61%	lr:0.000260
Ep: 9/10	It: 1501/4459	batch_loss: 2.8749	batch_accuracy: 43.71%	lr:0.000257
Ep: 9/10	It: 1551/4459	batch_loss: 3.0529	batch_accuracy: 41.75%	lr:0.000254
Ep: 9/10	It: 1601/4459	batch_loss: 3.0324	batch_accuracy: 41.71%	lr:0.000251
Ep: 9/10	It: 1651/4459	batch_loss: 3.0524	batch_accuracy: 42.16%	lr:0.000248
Ep: 9/10	It: 1701/4459	batch_loss: 2.9098	batch_accuracy: 43.54%	lr:0.000245
Ep: 9/10	It: 1751/4459	batch_loss: 2.9677	batch_accuracy: 43.25%	lr:0.000242
Ep: 9/10	It: 1801/4459	batch_loss: 3.0191	batch_accuracy: 41.76%	lr:0.000239
Ep: 9/10	It: 1851/4459	batch_loss: 2.9213	batch_accuracy: 42.94%	lr:0.000236
Ep: 9/10	It: 1901/4459	batch_loss: 2.9588	batch_accuracy: 42.39%	lr:0.000233
Ep: 9/10	It: 1951/4459	batch_loss: 3.0515	batch_accuracy: 41.42%	lr:0.000230
Ep: 9/10	It: 2001/4459	batch_loss: 3.0242	batch_accuracy: 42.72%	lr:0.000227
Ep: 9/10	It: 2051/4459	batch_loss: 2.9625	batch_accuracy: 42.28%	lr:0.000224
Ep: 9/10	It: 2101/4459	batch_loss: 2.8633	batch_accuracy: 43.95%	lr:0.000221
Ep: 9/10	It: 2151/4459	batch_loss: 2.9962	batch_accuracy: 42.42%	lr:0.000218
Ep: 9/10	It: 2201/4459	batch_loss: 2.9571	batch_accuracy: 42.82%	lr:0.000216
Ep: 9/10	It: 2251/4459	batch_loss: 2.9747	batch_accuracy: 42.57%	lr:0.000213
Ep: 9/10	It: 2301/4459	batch_loss: 3.0141	batch_accuracy: 42.25%	lr:0.000210
Ep: 9/10	It: 2351/4459	batch_loss: 2.9252	batch_accuracy: 42.47%	lr:0.000207
Ep: 9/10	It: 2401/4459	batch_loss: 2.9244	batch_accuracy: 42.44%	lr:0.000204
Ep: 9/10	It: 2451/4459	batch_loss: 2.8778	batch_accuracy: 43.82%	lr:0.000202
Ep: 9/10	It: 2501/4459	batch_loss: 2.9465	batch_accuracy: 42.74%	lr:0.000199
Ep: 9/10	It: 2551/4459	batch_loss: 2.9000	batch_accuracy: 43.50%	lr:0.000196
Ep: 9/10	It: 2601/4459	batch_loss: 2.9170	batch_accuracy: 43.02%	lr:0.000193
Ep: 9/10	It: 2651/4459	batch_loss: 3.0288	batch_accuracy: 42.29%	lr:0.000191
Ep: 9/10	It: 2701/4459	batch_loss: 2.9554	batch_accuracy: 42.31%	lr:0.000188
Ep: 9/10	It: 2751/4459	batch_loss: 3.1207	batch_accuracy: 41.31%	lr:0.000185
Ep: 9/10	It: 2801/4459	batch_loss: 2.9781	batch_accuracy: 42.82%	lr:0.000183
Ep: 9/10	It: 2851/4459	batch_loss: 2.9487	batch_accuracy: 42.52%	lr:0.000180
Ep: 9/10	It: 2901/4459	batch_loss: 2.9624	batch_accuracy: 42.24%	lr:0.000178
Ep: 9/10	It: 2951/4459	batch_loss: 2.9830	batch_accuracy: 42.19%	lr:0.000175
Ep: 9/10	It: 3001/4459	batch_loss: 2.9736	batch_accuracy: 42.18%	lr:0.000172
Ep: 9/10	It: 3051/4459	batch_loss: 2.9241	batch_accuracy: 42.99%	lr:0.000170
Ep: 9/10	It: 3101/4459	batch_loss: 3.0235	batch_accuracy: 41.80%	lr:0.000167
Ep: 9/10	It: 3151/4459	batch_loss: 2.9262	batch_accuracy: 43.26%	lr:0.000165
Ep: 9/10	It: 3201/4459	batch_loss: 2.9012	batch_accuracy: 43.43%	lr:0.000162
Ep: 9/10	It: 3251/4459	batch_loss: 2.9714	batch_accuracy: 42.13%	lr:0.000160
Ep: 9/10	It: 3301/4459	batch_loss: 2.9514	batch_accuracy: 42.99%	lr:0.000157
Ep: 9/10	It: 3351/4459	batch_loss: 3.0129	batch_accuracy: 41.74%	lr:0.000155
Ep: 9/10	It: 3401/4459	batch_loss: 2.9091	batch_accuracy: 43.89%	lr:0.000152
Ep: 9/10	It: 3451/4459	batch_loss: 2.9463	batch_accuracy: 43.32%	lr:0.000150
Ep: 9/10	It: 3501/4459	batch_loss: 2.9839	batch_accuracy: 42.25%	lr:0.000147
Ep: 9/10	It: 3551/4459	batch_loss: 2.9994	batch_accuracy: 42.41%	lr:0.000145
Ep: 9/10	It: 3601/4459	batch_loss: 2.8791	batch_accuracy: 43.46%	lr:0.000143
Ep: 9/10	It: 3651/4459	batch_loss: 3.0211	batch_accuracy: 42.28%	lr:0.000140
Ep: 9/10	It: 3701/4459	batch_loss: 2.9799	batch_accuracy: 41.93%	lr:0.000138
Ep: 9/10	It: 3751/4459	batch_loss: 2.9360	batch_accuracy: 43.27%	lr:0.000136
Ep: 9/10	It: 3801/4459	batch_loss: 3.0289	batch_accuracy: 41.91%	lr:0.000133
Ep: 9/10	It: 3851/4459	batch_loss: 2.9112	batch_accuracy: 43.07%	lr:0.000131
Ep: 9/10	It: 3901/4459	batch_loss: 2.9103	batch_accuracy: 43.50%	lr:0.000129
Ep: 9/10	It: 3951/4459	batch_loss: 2.8783	batch_accuracy: 43.74%	lr:0.000126
Ep: 9/10	It: 4001/4459	batch_loss: 2.9166	batch_accuracy: 43.33%	lr:0.000124
Ep: 9/10	It: 4051/4459	batch_loss: 2.9435	batch_accuracy: 42.52%	lr:0.000122
Ep: 9/10	It: 4101/4459	batch_loss: 3.0702	batch_accuracy: 41.42%	lr:0.000120
Ep: 9/10	It: 4151/4459	batch_loss: 3.0204	batch_accuracy: 41.84%	lr:0.000118
Ep: 9/10	It: 4201/4459	batch_loss: 3.0079	batch_accuracy: 42.32%	lr:0.000115
Ep: 9/10	It: 4251/4459	batch_loss: 2.9249	batch_accuracy: 43.07%	lr:0.000113
Ep: 9/10	It: 4301/4459	batch_loss: 2.9718	batch_accuracy: 42.37%	lr:0.000111
Ep: 9/10	It: 4351/4459	batch_loss: 2.9120	batch_accuracy: 43.60%	lr:0.000109
Ep: 9/10	It: 4401/4459	batch_loss: 2.9728	batch_accuracy: 42.88%	lr:0.000107
Ep: 9/10	It: 4451/4459	batch_loss: 2.9438	batch_accuracy: 42.71%	lr:0.000105
Ep: 9/10	It: 4459/4459	batch_loss: 2.9726	batch_accuracy: 42.16%	lr:0.000105


Generated text for input text "You" is:
You know, it's like our secret weapon. But instead of giving us an idea, we're talking about the world around us.

Imagine you have a big box full of colorful toys, but every time you put a block, it goes up to each other. When you put them together, you put them together and get a new toy. This is similar to what happens when you flip a coin and see how many times it goes up.

Now, let's say you want to find out how many blocks you need to add up to the big box. To do this, you could write down the total number of blocks in the box. But if you keep looking, you could also use something called the "mean inequality."

For example, let's say we have a bunch of blocks that we want to make a block. In our example, the block "n" is 1, and the block "n" is 1. So, if we want to figure out how many blocks are in the box, we can divide the blocks by 2.

In this case, the block "n" is the number of blocks that are the block "n" on the box. In this case, we would say that the block "n


huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Ep: 10/10	It: 1/4459	batch_loss: 2.8264	batch_accuracy: 44.47%	lr:0.000104
Ep: 10/10	It: 51/4459	batch_loss: 2.9837	batch_accuracy: 42.43%	lr:0.000102
Ep: 10/10	It: 101/4459	batch_loss: 2.9240	batch_accuracy: 43.32%	lr:0.000100
Ep: 10/10	It: 151/4459	batch_loss: 2.9102	batch_accuracy: 43.70%	lr:0.000098
Ep: 10/10	It: 201/4459	batch_loss: 2.8912	batch_accuracy: 43.76%	lr:0.000096
Ep: 10/10	It: 251/4459	batch_loss: 2.9916	batch_accuracy: 41.83%	lr:0.000094
Ep: 10/10	It: 301/4459	batch_loss: 2.8768	batch_accuracy: 43.43%	lr:0.000093
Ep: 10/10	It: 351/4459	batch_loss: 3.0941	batch_accuracy: 41.55%	lr:0.000091
Ep: 10/10	It: 401/4459	batch_loss: 3.0469	batch_accuracy: 42.21%	lr:0.000089
Ep: 10/10	It: 451/4459	batch_loss: 3.0218	batch_accuracy: 41.35%	lr:0.000087
Ep: 10/10	It: 501/4459	batch_loss: 2.9092	batch_accuracy: 43.66%	lr:0.000085
Ep: 10/10	It: 551/4459	batch_loss: 3.0592	batch_accuracy: 42.16%	lr:0.000083
Ep: 10/10	It: 601/4459	batch_loss: 3.0151	batch_accuracy: 41.62%	lr:0.000081
Ep: 10/10	It: 651/4459	batch_loss: 2.9311	batch_accuracy: 42.89%	lr:0.000080
Ep: 10/10	It: 701/4459	batch_loss: 2.9694	batch_accuracy: 41.94%	lr:0.000078
Ep: 10/10	It: 751/4459	batch_loss: 3.0170	batch_accuracy: 42.18%	lr:0.000076
Ep: 10/10	It: 801/4459	batch_loss: 2.9786	batch_accuracy: 42.16%	lr:0.000074
Ep: 10/10	It: 851/4459	batch_loss: 2.8745	batch_accuracy: 43.85%	lr:0.000073
Ep: 10/10	It: 901/4459	batch_loss: 2.9337	batch_accuracy: 42.65%	lr:0.000071
Ep: 10/10	It: 951/4459	batch_loss: 2.9864	batch_accuracy: 42.47%	lr:0.000069
Ep: 10/10	It: 1001/4459	batch_loss: 2.9530	batch_accuracy: 43.05%	lr:0.000068
Ep: 10/10	It: 1051/4459	batch_loss: 2.9124	batch_accuracy: 43.42%	lr:0.000066
Ep: 10/10	It: 1101/4459	batch_loss: 3.0109	batch_accuracy: 42.40%	lr:0.000064
Ep: 10/10	It: 1151/4459	batch_loss: 2.8248	batch_accuracy: 44.48%	lr:0.000063
Ep: 10/10	It: 1201/4459	batch_loss: 2.9029	batch_accuracy: 43.68%	lr:0.000061
Ep: 10/10	It: 1251/4459	batch_loss: 2.9689	batch_accuracy: 42.52%	lr:0.000060
Ep: 10/10	It: 1301/4459	batch_loss: 2.9522	batch_accuracy: 42.27%	lr:0.000058
Ep: 10/10	It: 1351/4459	batch_loss: 2.9942	batch_accuracy: 42.27%	lr:0.000057
Ep: 10/10	It: 1401/4459	batch_loss: 2.9923	batch_accuracy: 42.54%	lr:0.000055
Ep: 10/10	It: 1451/4459	batch_loss: 2.8905	batch_accuracy: 42.98%	lr:0.000054
Ep: 10/10	It: 1501/4459	batch_loss: 2.9791	batch_accuracy: 42.22%	lr:0.000052
Ep: 10/10	It: 1551/4459	batch_loss: 2.9505	batch_accuracy: 43.17%	lr:0.000051
Ep: 10/10	It: 1601/4459	batch_loss: 2.9274	batch_accuracy: 43.06%	lr:0.000050
Ep: 10/10	It: 1651/4459	batch_loss: 2.9137	batch_accuracy: 42.82%	lr:0.000048
Ep: 10/10	It: 1701/4459	batch_loss: 2.8668	batch_accuracy: 43.41%	lr:0.000047
Ep: 10/10	It: 1751/4459	batch_loss: 2.9221	batch_accuracy: 43.09%	lr:0.000046
Ep: 10/10	It: 1801/4459	batch_loss: 2.8818	batch_accuracy: 43.47%	lr:0.000044
Ep: 10/10	It: 1851/4459	batch_loss: 2.9004	batch_accuracy: 43.37%	lr:0.000043
Ep: 10/10	It: 1901/4459	batch_loss: 2.9827	batch_accuracy: 42.23%	lr:0.000042
Ep: 10/10	It: 1951/4459	batch_loss: 2.9052	batch_accuracy: 44.11%	lr:0.000041
Ep: 10/10	It: 2001/4459	batch_loss: 2.8702	batch_accuracy: 44.36%	lr:0.000039
Ep: 10/10	It: 2051/4459	batch_loss: 3.0187	batch_accuracy: 41.53%	lr:0.000038
Ep: 10/10	It: 2101/4459	batch_loss: 2.8071	batch_accuracy: 44.26%	lr:0.000037
Ep: 10/10	It: 2151/4459	batch_loss: 3.0223	batch_accuracy: 42.12%	lr:0.000036
Ep: 10/10	It: 2201/4459	batch_loss: 2.9951	batch_accuracy: 42.35%	lr:0.000035
Ep: 10/10	It: 2251/4459	batch_loss: 2.9556	batch_accuracy: 42.65%	lr:0.000034
Ep: 10/10	It: 2301/4459	batch_loss: 2.9189	batch_accuracy: 42.83%	lr:0.000033
Ep: 10/10	It: 2351/4459	batch_loss: 2.8621	batch_accuracy: 44.48%	lr:0.000032
Ep: 10/10	It: 2401/4459	batch_loss: 2.8670	batch_accuracy: 43.80%	lr:0.000031
Ep: 10/10	It: 2451/4459	batch_loss: 2.9011	batch_accuracy: 43.38%	lr:0.000030
Ep: 10/10	It: 2501/4459	batch_loss: 2.8859	batch_accuracy: 44.15%	lr:0.000029
Ep: 10/10	It: 2551/4459	batch_loss: 2.8762	batch_accuracy: 43.71%	lr:0.000028
Ep: 10/10	It: 2601/4459	batch_loss: 3.0315	batch_accuracy: 41.90%	lr:0.000027
Ep: 10/10	It: 2651/4459	batch_loss: 2.9436	batch_accuracy: 43.31%	lr:0.000026
Ep: 10/10	It: 2701/4459	batch_loss: 2.9466	batch_accuracy: 42.60%	lr:0.000025
Ep: 10/10	It: 2751/4459	batch_loss: 3.0385	batch_accuracy: 42.11%	lr:0.000024
Ep: 10/10	It: 2801/4459	batch_loss: 2.9676	batch_accuracy: 42.68%	lr:0.000023
Ep: 10/10	It: 2851/4459	batch_loss: 3.0215	batch_accuracy: 42.55%	lr:0.000023
Ep: 10/10	It: 2901/4459	batch_loss: 3.0215	batch_accuracy: 41.68%	lr:0.000022
Ep: 10/10	It: 2951/4459	batch_loss: 2.8910	batch_accuracy: 43.32%	lr:0.000021
Ep: 10/10	It: 3001/4459	batch_loss: 2.9676	batch_accuracy: 43.02%	lr:0.000020
Ep: 10/10	It: 3051/4459	batch_loss: 2.9567	batch_accuracy: 43.23%	lr:0.000020
Ep: 10/10	It: 3101/4459	batch_loss: 2.8524	batch_accuracy: 43.83%	lr:0.000019
Ep: 10/10	It: 3151/4459	batch_loss: 2.9005	batch_accuracy: 43.33%	lr:0.000018
Ep: 10/10	It: 3201/4459	batch_loss: 2.9788	batch_accuracy: 42.77%	lr:0.000018
Ep: 10/10	It: 3251/4459	batch_loss: 2.9098	batch_accuracy: 42.61%	lr:0.000017
Ep: 10/10	It: 3301/4459	batch_loss: 2.8824	batch_accuracy: 43.77%	lr:0.000017
Ep: 10/10	It: 3351/4459	batch_loss: 2.8447	batch_accuracy: 44.29%	lr:0.000016
Ep: 10/10	It: 3401/4459	batch_loss: 2.9115	batch_accuracy: 43.48%	lr:0.000015
Ep: 10/10	It: 3451/4459	batch_loss: 2.8521	batch_accuracy: 44.08%	lr:0.000015
Ep: 10/10	It: 3501/4459	batch_loss: 2.9771	batch_accuracy: 42.25%	lr:0.000015
Ep: 10/10	It: 3551/4459	batch_loss: 2.9611	batch_accuracy: 43.10%	lr:0.000014
Ep: 10/10	It: 3601/4459	batch_loss: 3.0169	batch_accuracy: 41.95%	lr:0.000014
Ep: 10/10	It: 3651/4459	batch_loss: 2.9575	batch_accuracy: 42.76%	lr:0.000013
Ep: 10/10	It: 3701/4459	batch_loss: 2.8389	batch_accuracy: 44.27%	lr:0.000013
Ep: 10/10	It: 3751/4459	batch_loss: 2.9283	batch_accuracy: 42.98%	lr:0.000012
Ep: 10/10	It: 3801/4459	batch_loss: 2.9519	batch_accuracy: 42.97%	lr:0.000012
Ep: 10/10	It: 3851/4459	batch_loss: 2.8898	batch_accuracy: 43.59%	lr:0.000012
Ep: 10/10	It: 3901/4459	batch_loss: 3.0448	batch_accuracy: 41.84%	lr:0.000012
Ep: 10/10	It: 3951/4459	batch_loss: 3.0073	batch_accuracy: 41.69%	lr:0.000011
Ep: 10/10	It: 4001/4459	batch_loss: 2.9457	batch_accuracy: 42.35%	lr:0.000011
Ep: 10/10	It: 4051/4459	batch_loss: 2.9597	batch_accuracy: 42.36%	lr:0.000011
Ep: 10/10	It: 4101/4459	batch_loss: 2.9137	batch_accuracy: 43.59%	lr:0.000011
Ep: 10/10	It: 4151/4459	batch_loss: 2.9483	batch_accuracy: 42.57%	lr:0.000010
Ep: 10/10	It: 4201/4459	batch_loss: 2.9110	batch_accuracy: 43.74%	lr:0.000010
Ep: 10/10	It: 4251/4459	batch_loss: 2.9708	batch_accuracy: 42.74%	lr:0.000010
Ep: 10/10	It: 4301/4459	batch_loss: 2.8156	batch_accuracy: 44.70%	lr:0.000010
Ep: 10/10	It: 4351/4459	batch_loss: 2.8610	batch_accuracy: 43.38%	lr:0.000010
Ep: 10/10	It: 4401/4459	batch_loss: 3.0612	batch_accuracy: 41.69%	lr:0.000010
Ep: 10/10	It: 4451/4459	batch_loss: 2.9161	batch_accuracy: 42.95%	lr:0.000010
Ep: 10/10	It: 4459/4459	batch_loss: 3.0402	batch_accuracy: 41.94%	lr:0.000010


Generated text for input text "You" is:
You might have heard that you're talking about something called "camera." Have you ever heard of a new kind of machine? Well, it's just a fancy name for a way to sort and manipulate things.

Let's say we have a special machine that gives us a new toy in the toy box. Our machine could use something called a "triangle" to describe how the machine worked. It's like a machine that uses different kinds of data to create a new toy box. When we do this, we mean the machine and machine is changing the way things move.

Now, imagine you want to find out how many toys you can use when you add them together. That's where the rule of addition comes in handy. It helps us see how many toys you could create, but it's just a fancy way of changing them.

Let's say we want to know how many toys you can take out. To do this, we need to calculate the total number of toys in a different box. This means that if you have just one toy box, then the number of toys you have is the same size, and the number of toys you can get in different ways.

To find the total number of toys in a way


Ended at 2025-04-21 00:26:30
Duration: 4:16:05.396745
Job finished at Mon Apr 21 00:27:02 CDT 2025
